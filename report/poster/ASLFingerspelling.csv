Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body
77UCAIXE,conferencePaper,2019,"Shi, Bowen; Rio, Aurora Martinez Del; Keane, Jonathan; Brentari, Diane; Shakhnarovich, Greg; Livescu, Karen",Fingerspelling Recognition in the Wild With Iterative Visual Attention,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00550,https://ieeexplore.ieee.org/document/9010036/,"Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of ﬁngerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It outperforms prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of ﬁngerspelling in the wild, and show that performance can be further improved with this additional data set.",2019-10,2023-11-29 00:10:41,2023-11-29 00:11:50,2023-11-29 00:10:41,5399-5408,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\VNP3293B\Shi et al. - 2019 - Fingerspelling Recognition in the Wild With Iterat.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
VP64XSS4,conferencePaper,2018,"S Kumar, Sujay; Wangyal, Tenzin; Saboo, Varun; Srinath, Ramamoorthy",Time Series Neural Networks for Real Time Sign Language Translation,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),978-1-5386-6805-4,,10.1109/ICMLA.2018.00043,https://ieeexplore.ieee.org/document/8614068/,"Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.",2018-12,2023-11-29 00:13:27,2023-11-29 00:13:27,2023-11-29 00:13:27,243-248,,,,,,,,,,,IEEE,"Orlando, FL",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\JPEHN7HA\S Kumar et al. - 2018 - Time Series Neural Networks for Real Time Sign Lan.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),,,,,,,,,,,,,,,
7RC5VFKY,conferencePaper,2019,"Nguyen, Huy B.D; Do, Hung Ngoc",Deep Learning for American Sign Language Fingerspelling Recognition System,2019 26th International Conference on Telecommunications (ICT),978-1-72810-273-3,,10.1109/ICT.2019.8798856,https://ieeexplore.ieee.org/document/8798856/,"Sign language has always been a major tool for communication among people with disabilities. In this paper, a sign language ﬁngerspelling alphabet identiﬁcation system would be developed by using image processing technique, supervised machine learning and deep learning. In particular, 24 alphabetical symbols are presented by several combinations of static gestures (excluding 2 motion gestures J and Z). Histogram of Oriented Gradients (HOG) and Local Binary Pattern (LBP) features of each gesture will be extracted from training images. Then Multiclass Support Vector Machines (SVMs) will be applied to train these extracted data. Also, an end-to-end Convolutional Neural Network (CNN) architecture will be applied to the training dataset for comparison. After that, a further combination of CNN as feature descriptor and SVM produces an acceptable result. The Massey Dataset is implemented in the training and testing phases of the whole system.",2019-04,2023-11-29 00:15:30,2023-11-29 00:15:30,2023-11-29 00:15:30,314-318,,,,,,,,,,,IEEE,"Hanoi, Vietnam",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\ZCNRKPQK\Nguyen and Do - 2019 - Deep Learning for American Sign Language Fingerspe.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 26th International Conference on Telecommunications (ICT),,,,,,,,,,,,,,,
62JYKNKK,journalArticle,2018,"Chong, Teak-Wei; Lee, Boon-Giin",American Sign Language Recognition Using Leap Motion Controller with Machine Learning Approach,Sensors,,1424-8220,10.3390/s18103554,https://www.mdpi.com/1424-8220/18/10/3554,"Sign language is intentionally designed to allow deaf and dumb communities to convey messages and to connect with society. Unfortunately, learning and practicing sign language is not common among society; hence, this study developed a sign language recognition prototype using the Leap Motion Controller (LMC). Many existing studies have proposed methods for incomplete sign language recognition, whereas this study aimed for full American Sign Language (ASL) recognition, which consists of 26 letters and 10 digits. Most of the ASL letters are static (no movement), but certain ASL letters are dynamic (they require certain movements). Thus, this study also aimed to extract features from finger and hand motions to differentiate between the static and dynamic gestures. The experimental results revealed that the sign language recognition rates for the 26 letters using a support vector machine (SVM) and a deep neural network (DNN) are 80.30% and 93.81%, respectively. Meanwhile, the recognition rates for a combination of 26 letters and 10 digits are slightly lower, approximately 72.79% for the SVM and 88.79% for the DNN. As a result, the sign language recognition system has great potential for reducing the gap between deaf and dumb communities and others. The proposed prototype could also serve as an interpreter for the deaf and dumb in everyday life in service sectors, such as at the bank or post office.",2018-10,2023-11-29 00:16:34,2023-11-29 00:16:34,2023-11-29 00:16:34,3554,,10,18,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,Number: 10 Publisher: Multidisciplinary Digital Publishing Institute,,C:\Users\test\Zotero\storage\QGWQXCYL\Chong and Lee - 2018 - American Sign Language Recognition Using Leap Moti.pdf,,,American Sign Language; deep neural network; human-computer interaction; Leap Motion Controller; machine learning; multi-class classification; sign language recognition; support vector machine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3CV77PDE,journalArticle,2021,"Adeyanju, I.A.; Bello, O.O.; Adegboye, M.A.",Machine learning methods for sign language recognition: A critical review and analysis,Intelligent Systems with Applications,,26673053,10.1016/j.iswa.2021.200056,https://linkinghub.elsevier.com/retrieve/pii/S2667305321000454,,2021-11,2023-11-29 00:17:57,2023-11-29 00:17:57,2023-11-29 00:17:57,200056,,,12,,Intelligent Systems with Applications,Machine learning methods for sign language recognition,,,,,,,en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\WDP9WTXW\Adeyanju et al. - 2021 - Machine learning methods for sign language recogni.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L2J33D2S,journalArticle,2021,"Rastgoo, Razieh; Kiani, Kourosh; Escalera, Sergio",Sign Language Recognition: A Deep Survey,Expert Systems with Applications,,09574174,10.1016/j.eswa.2020.113794,https://linkinghub.elsevier.com/retrieve/pii/S095741742030614X,"Sign language, as a different form of the communication language, is important to large groups of people in society. There are different signs in each sign language with variability in hand shape, motion profile, and position of the hand, face, and body parts contributing to each sign. So, visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the visionbased proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved. We present a taxonomy to categorize the proposed models for isolated and continuous sign language recognition, discussing applications, datasets, hybrid models, complexity, and future lines of research in the field.",2021-02,2023-11-29 00:18:32,2023-11-29 00:18:32,2023-11-29 00:18:32,113794,,,164,,Expert Systems with Applications,Sign Language Recognition,,,,,,,en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\NXZMWM3W\Rastgoo et al. - 2021 - Sign Language Recognition A Deep Survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRZEL8DL,conferencePaper,2020,"Cihan Camgoz, Necati; Koller, Oscar; Hadfield, Simon; Bowden, Richard",Sign Language Transformers: Joint End-to-End Sign Language Recognition and Translation,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72817-168-5,,10.1109/CVPR42600.2020.01004,https://ieeexplore.ieee.org/document/9156773/,"Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-theart in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classiﬁcation (CTC) loss to bind the recognition and translation problems into a single uniﬁed architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-tosequence learning problems and leads to signiﬁcant performance gains.",2020-06,2023-11-29 00:18:54,2023-11-29 00:18:54,2023-11-29 00:18:54,10020-10030,,,,,,Sign Language Transformers,,,,,IEEE,"Seattle, WA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\MGNGYH32\Cihan Camgoz et al. - 2020 - Sign Language Transformers Joint End-to-End Sign .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
JAP5RPTG,conferencePaper,2021,"Shi, Bowen; Brentari, Diane; Shakhnarovich, Greg; Livescu, Karen",Fingerspelling Detection in American Sign Language,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-66544-509-2,,10.1109/CVPR46437.2021.00415,https://ieeexplore.ieee.org/document/9578297/,"Fingerspelling, in which words are signed letter by letter, is an important component of American Sign Language. Most previous work on automatic ﬁngerspelling recognition has assumed that the boundaries of ﬁngerspelling regions in signing videos are known beforehand. In this paper, we consider the task of ﬁngerspelling detection in raw, untrimmed sign language videos. This is an important step towards building real-world ﬁngerspelling recognition systems. We propose a benchmark and a suite of evaluation metrics, some of which reﬂect the effect of detection on the downstream ﬁngerspelling recognition task. In addition, we propose a new model that learns to detect ﬁngerspelling via multi-task training, incorporating pose estimation and ﬁngerspelling recognition (transcription) along with detection, and compare this model to several alternatives. The model outperforms all alternative approaches across all metrics, establishing a state of the art on the benchmark.",2021-06,2023-11-29 00:20:14,2023-11-29 00:20:14,2023-11-29 00:20:14,4164-4173,,,,,,,,,,,IEEE,"Nashville, TN, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\3DXLW7AJ\Shi et al. - 2021 - Fingerspelling Detection in American Sign Language.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
YXD3QATL,conferencePaper,2020,"Abiyev, Rahib; Idoko, John Bush; Arslan, Murat",Reconstruction of Convolutional Neural Network for Sign Language Recognition,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",978-1-72817-116-6,,10.1109/ICECCE49384.2020.9179356,https://ieeexplore.ieee.org/document/9179356/,"This paper presents a Sign Language translation model using Convolutional Neural Networks (CNN). A sign language is a language which allows mute and hearingimpaired people to communicate. It is a visually oriented, nonverbal communication which facilitates communication through body/facial postures, expressions and a collection of gestures. To contribute to the wellbeing of the affected population, we are motivated to implement a vision-based system to avert their day to day challenges. Our propose model constitutes object detection and classification phases. The first module is made up of single shot multi-box detector (SSD) used for hand detection. The second module constitutes convolutional neural network plus a fully connected network utilized to constructively translate the detected signs into text. The propose model is implemented using American sign language fingerspelling dataset. The propose system outperformed other published results in the comparative analysis, hence recommended for further exploitation in sign language recognition problems.",2020-06,2023-11-29 00:20:20,2023-11-29 00:20:20,2023-11-29 00:20:20,1-5,,,,,,,,,,,IEEE,"Istanbul, Turkey",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\UFTNKESY\Abiyev et al. - 2020 - Reconstruction of Convolutional Neural Network for.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",,,,,,,,,,,,,,,
4ZKWVACK,conferencePaper,2022,"Weerasooriya, Amal A.; Ambegoda, Thanuja D.",Sinhala Fingerspelling Sign Language Recognition with Computer Vision,2022 Moratuwa Engineering Research Conference (MERCon),978-1-66548-786-3,,10.1109/MERCon55799.2022.9906281,https://ieeexplore.ieee.org/document/9906281/,"Computer vision based sign language translation is usually based on using thousands of images or video sequences for model training. This is not an issue in the case of widely used languages such as American Sign Language. However, in case of languages with low resources such as Sinhala Sign Language, it’s challenging to use similar methods for developing translators since there are no known data sets available for such studies.",2022-07-27,2023-11-29 00:20:53,2023-11-29 00:20:53,2023-11-29 00:20:53,1-6,,,,,,,,,,,IEEE,"Moratuwa, Sri Lanka",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\J4VM2MWH\Weerasooriya and Ambegoda - 2022 - Sinhala Fingerspelling Sign Language Recognition w.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022 Moratuwa Engineering Research Conference (MERCon),,,,,,,,,,,,,,,
T2D8ZNTH,conferencePaper,2018,"Bantupalli, Kshitij; Xie, Ying",American Sign Language Recognition using Deep Learning and Computer Vision,2018 IEEE International Conference on Big Data (Big Data),,,10.1109/BigData.2018.8622141,https://ieeexplore.ieee.org/document/8622141,"Speech impairment is a disability which affects an individuals ability to communicate using speech and hearing. People who are affected by this use other media of communication such as sign language. Although sign language is ubiquitous in recent times, there remains a challenge for non-sign language speakers to communicate with sign language speakers or signers. With recent advances in deep learning and computer vision there has been promising progress in the fields of motion and gesture recognition using deep learning and computer vision based techniques. The focus of this work is to create a visionbased application which offers sign language translation to text thus aiding communication between signers and non-signers. The proposed model takes video sequences and extracts temporal and spatial features from them. We then use Inception, a CNN (Convolutional Neural Network) for recognizing spatial features. We then use a RNN (Recurrent Neural Network) to train on temporal features. The dataset used is the American Sign Language Dataset.",2018-12,2023-11-29 19:39:49,2023-11-29 19:39:49,2023-11-29 19:39:49,4896-4899,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,C:\Users\test\Zotero\storage\SHY89S3Q\8622141.html; C:\Users\test\Zotero\storage\DEKECKH3\Bantupalli and Xie - 2018 - American Sign Language Recognition using Deep Lear.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE International Conference on Big Data (Big Data),,,,,,,,,,,,,,,
6MZJ8CPY,conferencePaper,2018,"Shi, Bowen; Del Rio, Aurora Martinez; Keane, Jonathan; Michaux, Jonathan; Brentari, Diane; Shakhnarovich, Greg; Livescu, Karen",American Sign Language Fingerspelling Recognition in the Wild,2018 IEEE Spoken Language Technology Workshop (SLT),,,10.1109/SLT.2018.8639639,https://ieeexplore.ieee.org/abstract/document/8639639,"We address the problem of American Sign Language fingerspelling recognition “in the wild”, using videos collected from websites. We introduce the largest data set available so far for the problem of fingerspelling recognition, and the first using naturally occurring video data. Using this data set, we present the first attempt to recognize fingerspelling sequences in this challenging setting. Unlike prior work, our video data is extremely challenging due to low frame rates and visual variability. To tackle the visual challenges, we train a special-purpose signing hand detector using a small subset of our data. Given the hand detector output, a sequence model decodes the hypothesized fingerspelled letter sequence. For the sequence model, we explore attention-based recurrent encoder-decoders and CTC-based approaches. As the first attempt at fingerspelling recognition in the wild, this work is intended to serve as a baseline for future work on sign language recognition in realistic conditions. We find that, as expected, letter error rates are much higher than in previous work on more controlled data, and we analyze the sources of error and effects of model variants.",2018-12,2023-11-29 19:41:37,2023-11-29 19:41:37,2023-11-29 19:41:37,145-152,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,C:\Users\test\Zotero\storage\CE65XA5S\8639639.html; C:\Users\test\Zotero\storage\M9RBGXG2\Shi et al. - 2018 - American Sign Language Fingerspelling Recognition .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE Spoken Language Technology Workshop (SLT),,,,,,,,,,,,,,,
5FAGHSRQ,conferencePaper,2023,"Kabade, Amruta E; Desai, Padmashree; C, Sujatha; G, Shankar",American Sign Language Fingerspelling Recognition using Attention Model,2023 IEEE 8th International Conference for Convergence in Technology (I2CT),,,10.1109/I2CT57861.2023.10126277,https://ieeexplore.ieee.org/document/10126277,"Sign Language Recognition(SLR) is a complex gesture recognition problem because of the quick and highly coarticulated motion involved in gestures. This research work focuses on Fingerspelling recognition task, which constitutes 35% of the American Sign Language (ASL). Fingerspelling identifies the word letter by letter. Fingerspelling is used for signing the words which do not have designated ASL signs such as technical terms, content words and proper nouns. In our proposed work for ASL Fingerspelling recognition, we consider ChicagoFSWild dataset which consists of occlusions and images captured in varying illuminations, lighting conditions (in the wild environments). The optical flow is obtained from Lucas-Kanade algorithm, prior is generated, images are resized and cropped with face-roi technique to get the region of interest (ROI). The visual attention mechanism attends to the ROI iteratively. ResNet, pretrained on Imagenet is used for the extraction of spatial features. The Bi-LSTM network with Connectionist Temporal Classification (CTC) is used to predict the sign. It provides the accuracy of 57% on ChicagoFSWild dataset for Fingerspelling recognition task.",2023-04,2023-11-29 19:42:10,2023-11-29 19:42:10,2023-11-29 19:42:10,1-6,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,C:\Users\test\Zotero\storage\E8II75TX\Kabade et al. - 2023 - American Sign Language Fingerspelling Recognition .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2023 IEEE 8th International Conference for Convergence in Technology (I2CT),,,,,,,,,,,,,,,
PM5QSNKJ,preprint,2022,"Shi, Bowen; Brentari, Diane; Shakhnarovich, Greg; Livescu, Karen",Searching for fingerspelled content in American Sign Language,,,,,http://arxiv.org/abs/2203.13291,"Natural language processing for sign language video - including tasks like recognition, translation, and search - is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years. In this paper, we address the problem of searching for fingerspelled key-words or key phrases in raw sign language videos. This is an important task since significant content in sign language is often conveyed via fingerspelling, and to our knowledge the task has not been studied before. We propose an end-to-end model for this task, FSS-Net, that jointly detects fingerspelling and matches it to a text sequence. Our experiments, done on a large public dataset of ASL fingerspelling in the wild, show the importance of fingerspelling detection as a component of a search and retrieval model. Our model significantly outperforms baseline methods adapted from prior work on related tasks",2022-03-24,2023-12-13 01:12:07,2023-12-13 01:12:07,2023-12-13 01:12:07,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2203.13291 [cs],,C:\Users\test\Zotero\storage\D4BVWXQD\2203.html; C:\Users\test\Zotero\storage\7EF3XGGJ\Shi et al. - 2022 - Searching for fingerspelled content in American Si.pdf,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,arXiv:2203.13291,,,,,,,,,,,,,,,,,,,,,,,,,,,
YR2GK3Y4,conferencePaper,2021,"Gajurel, Kamala; Zhong, Cuncong; Wang, Guanghui",A Fine-Grained Visual Attention Approach for Fingerspelling Recognition in the Wild,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",978-1-66544-207-7,,10.1109/SMC52423.2021.9658982,https://ieeexplore.ieee.org/document/9658982/,"Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of ﬁngerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in ﬁngerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in ﬁngerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated ﬁngerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a ﬁne-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The ﬁne-grained attention is achieved by utilizing the change in motion of the video frames (optical ﬂow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classiﬁcation (CTC) loss and the maximum-entropy loss. The proposed approach can capture better ﬁne-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.",2021-10-17,2023-12-13 01:20:11,2023-12-13 01:20:11,2023-12-13 01:20:11,3266-3271,,,,,,,,,,,IEEE,"Melbourne, Australia",en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\TSAAPSAB\Gajurel et al. - 2021 - A Fine-Grained Visual Attention Approach for Finge.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,,,,,,,,,,,,,,
V5PSAD5X,journalArticle,2022,"Saeed, Zinah Raad; Zainol, Zurinahni Binti; Zaidan, B. B.; Alamoodi, A. H.",A Systematic Review on Systems-Based Sensory Gloves for Sign Language Pattern Recognition: An Update From 2017 to 2022,IEEE Access,,2169-3536,10.1109/ACCESS.2022.3219430,https://ieeexplore.ieee.org/document/9938436/,"Sign language is the predominant mode of communication for the Hearing impaired community. For the millions of people who suffer from hearing loss around the world, interaction with people who have the ability to hear and do not suffer from hearing impairment or loss is considered as complicated. In line with this issue, technology is perceived as a crucial factor in being an enabler of providing solutions to enhance the quality of life of the hearing impairment by increasing accessibility. This research aims to review and analyze articles related to sign language recognition based on the sensor- based glove system, in order to identify academic motivations, challenges, and recommendations related to this ﬁeld. The search for the relevant review materials and articles was performed on four major databases ranging from 2017 to 2022: Science Direct, Web of Science, IEEE Xplore, and Scopus. The articles were chosen based on our inclusion and exclusion criteria. The literature ﬁndings of this paper indicate the dataset size to be open issues and challenges for hand gesture recognition. Furthermore, the majority of research on sign language recognition based on data glove was performed on static, single hand, and isolated gestures. Moreover, recognition accuracy typically achieved results higher than 90%. However, most experiments were carried out with a limited number of gestures. Overall, it is hoped that this study will serve as a roadmap for future research and raise awareness among researchers in the ﬁeld of sign language recognition.",2022,2023-12-15 15:16:26,2023-12-15 15:16:26,2023-12-15 15:16:26,123358-123377,,,10,,IEEE Access,A Systematic Review on Systems-Based Sensory Gloves for Sign Language Pattern Recognition,,,,,,,en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\4JVIXX4C\Saeed et al. - 2022 - A Systematic Review on Systems-Based Sensory Glove.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MGYKQYEI,journalArticle,1998,"Starner, T.; Weaver, J.; Pentland, A.",Real-time American sign language recognition using desk and wearable computer based video,IEEE Transactions on Pattern Analysis and Machine Intelligence,,1939-3539,10.1109/34.735811,https://ieeexplore.ieee.org/document/735811,We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.,1998-12,2023-12-15 15:39:44,2023-12-15 15:39:44,2023-12-15 15:39:44,1371-1375,,12,20,,,,,,,,,,,,,,,IEEE Xplore,,Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence,,C:\Users\test\Zotero\storage\W6XC44TD\735811.html; C:\Users\test\Zotero\storage\9ITYQU23\Starner et al. - 1998 - Real-time American sign language recognition using.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J2YE4HRH,journalArticle,2008,"von Agris, Ulrich; Zieren, Jörg; Canzler, Ulrich; Bauer, Britta; Kraiss, Karl-Friedrich",Recent developments in visual sign language recognition,Universal Access in the Information Society,,1615-5297,10.1007/s10209-007-0104-x,https://doi.org/10.1007/s10209-007-0104-x,"Research in the field of sign language recognition has made significant advances in recent years. The present achievements provide the basis for future applications with the objective of supporting the integration of deaf people into the hearing society. Translation systems, for example, could facilitate communication between deaf and hearing people in public situations. Further applications, such as user interfaces and automatic indexing of signed videos, become feasible. The current state in sign language recognition is roughly 30 years behind speech recognition, which corresponds to the gradual transition from isolated to continuous recognition for small vocabulary tasks. Research efforts were mainly focused on robust feature extraction or statistical modeling of signs. However, current recognition systems are still designed for signer-dependent operation under laboratory conditions. This paper describes a comprehensive concept for robust visual sign language recognition, which represents the recent developments in this field. The proposed recognition system aims for signer-independent operation and utilizes a single video camera for data acquisition to ensure user-friendliness. Since sign languages make use of manual and facial means of expression, both channels are employed for recognition. For mobile operation in uncontrolled environments, sophisticated algorithms were developed that robustly extract manual and facial features. The extraction of manual features relies on a multiple hypotheses tracking approach to resolve ambiguities of hand positions. For facial feature extraction, an active appearance model is applied which allows identification of areas of interest such as the eyes and mouth region. In the next processing step, a numerical description of the facial expression, head pose, line of sight, and lip outline is computed. The system employs a resolution strategy for dealing with mutual overlapping of the signer’s hands and face. Classification is based on hidden Markov models which are able to compensate time and amplitude variances in the articulation of a sign. The classification stage is designed for recognition of isolated signs, as well as of continuous sign language. In the latter case, a stochastic language model can be utilized, which considers uni- and bigram probabilities of single and successive signs. For statistical modeling of reference models each sign is represented either as a whole or as a composition of smaller subunits—similar to phonemes in spoken languages. While recognition based on word models is limited to rather small vocabularies, subunit models open the door to large vocabularies. Achieving signer-independence constitutes a challenging problem, as the articulation of a sign is subject to high interpersonal variance. This problem cannot be solved by simple feature normalization and must be addressed at the classification level. Therefore, dedicated adaptation methods known from speech recognition were implemented and modified to consider the specifics of sign languages. For rapid adaptation to unknown signers the proposed recognition system employs a combined approach of maximum likelihood linear regression and maximum a posteriori estimation.",2008-02-01,2023-12-15 16:02:52,2023-12-15 16:02:52,2023-12-15 16:02:52,323-362,,4,6,,Univ Access Inf Soc,,,,,,,,en,,,,,Springer Link,,,,,,,Computer vision; Hidden Markov models; Human–computer interaction; Sign language recognition; Signer adaptation; Statistical pattern recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C4W8YECB,conferencePaper,1999,"Vogler, C.; Metaxas, D.",Parallel hidden Markov models for American sign language recognition,Proceedings of the Seventh IEEE International Conference on Computer Vision,,,10.1109/ICCV.1999.791206,https://ieeexplore.ieee.org/document/791206,"The major challenge that faces American Sign Language (ASL) recognition now is to develop methods that will scale well with increasing vocabulary size. Unlike in spoken languages, phonemes can occur simultaneously in ASL. The number of possible combinations of phonemes after enforcing linguistic constraints is approximately 5.5/spl times/10/sup 8/. Gesture recognition, which is less constrained than ASL recognition, suffers from the same problem. Thus, it is not feasible to train conventional hidden Markov models (HMMs) for large-scab ASL applications. Factorial HMMs and coupled HMMs are two extensions to HMMs that explicitly attempt to model several processes occuring in parallel. Unfortunately, they still require consideration of the combinations at training time. In this paper we present a novel approach to ASL recognition that aspires to being a solution to the scalability problems. It is based on parallel HMMs (PaHMMs), which model the parallel processes independently. Thus, they can also be trained independently, and do not require consideration of the different combinations at training time. We develop the recognition algorithm for PaHMMs and show that it runs in time polynomial in the number of states, and in time linear in the number of parallel processes. We run several experiments with a 22 sign vocabulary and demonstrate that PaHMMs can improve the robustness of HMM-based recognition even on a small scale. Thus, PaHMMs are a very promising general recognition scheme with applications in both gesture and ASL recognition.",1999-09,2023-12-15 16:32:55,2023-12-15 16:32:55,2023-12-15 16:32:55,116-122 vol.1,,,1,,,,,,,,,,,,,,,IEEE Xplore,,,,C:\Users\test\Zotero\storage\K3R8CHBQ\791206.html; C:\Users\test\Zotero\storage\JBDWMENN\Vogler and Metaxas - 1999 - Parallel hidden Markov models for American sign la.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the Seventh IEEE International Conference on Computer Vision,,,,,,,,,,,,,,,
CPRDLJNC,journalArticle,2011,"Oz, Cemil; Leu, Ming C.",American Sign Language word recognition with a sensory glove using artificial neural networks,Engineering Applications of Artificial Intelligence,,0952-1976,10.1016/j.engappai.2011.06.015,https://www.sciencedirect.com/science/article/pii/S0952197611001230,"An American Sign Language (ASL) recognition system is being developed using artificial neural networks (ANNs) to translate ASL words into English. The system uses a sensory glove called the Cyberglove™ and a Flock of Birds® 3-D motion tracker to extract the gesture features. The data regarding finger joint angles obtained from strain gauges in the sensory glove define the hand shape, while the data from the tracker describe the trajectory of hand movements. The data from these devices are processed by a velocity network with noise reduction and feature extraction and by a word recognition network. Some global and local features are extracted for each ASL word. A neural network is used as a classifier of this feature vector. Our goal is to continuously recognize ASL signs using these devices in real time. We trained and tested the ANN model for 50 ASL words with a different number of samples for every word. The test results show that our feature vector extraction method and neural networks can be used successfully for isolated word recognition. This system is flexible and open for future extension.",2011-10-01,2023-12-15 16:52:23,2023-12-15 16:52:23,2023-12-15 16:52:23,1204-1213,,7,24,,Engineering Applications of Artificial Intelligence,,Infrastructures and Tools for Multiagent Systems,,,,,,,,,,,ScienceDirect,,,,C:\Users\test\Zotero\storage\35EI6IGW\S0952197611001230.html,,,American Sign Language (ASL); Artificial Neural Networks (ANNs); ASL recognition; Finger spelling recognition; Hand-shape recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EYIMKUSL,journalArticle,2007,"Munib, Qutaishat; Habeeb, Moussa; Takruri, Bayan; Al-Malik, Hiba Abed",American sign language (ASL) recognition based on Hough transform and neural networks,Expert Systems with Applications,,09574174,10.1016/j.eswa.2005.11.018,https://linkinghub.elsevier.com/retrieve/pii/S0957417405003040,"The work presented in this paper aims to develop a system for automatic translation of static gestures of alphabets and signs in American sign language. In doing so, we have used Hough transform and neural networks which is trained to recognize signs. Our system does not rely on using any gloves or visual markings to achieve the recognition task. Instead, it deals with images of bare hands, which allows the user to interact with the system in a natural way. An image is processed and converted to a feature vector that will be compared with the feature vectors of a training set of signs. The extracted features are not aﬀected by the rotation, scaling or translation of the gesture within the image, which makes the system more ﬂexible.",2007-01,2023-12-15 16:52:58,2023-12-15 16:52:58,2023-12-15 16:52:58,24-37,,1,32,,Expert Systems with Applications,,,,,,,,en,,,,,DOI.org (Crossref),,,,C:\Users\test\Zotero\storage\HQR6UEYU\Munib et al. - 2007 - American sign language (ASL) recognition based on .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NGUY4347,journalArticle,1998,"Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P.",Gradient-based learning applied to document recognition,Proceedings of the IEEE,,1558-2256,10.1109/5.726791,https://ieeexplore.ieee.org/document/726791,"Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",1998-11,2023-12-15 19:59:47,2023-12-15 19:59:47,2023-12-15 19:59:47,2278-2324,,11,86,,,,,,,,,,,,,,,IEEE Xplore,,Conference Name: Proceedings of the IEEE,,C:\Users\test\Zotero\storage\P7VRXTPH\726791.html; C:\Users\test\Zotero\storage\UN65WJMB\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2V3JX736,conferencePaper,2012,"Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E",ImageNet Classification with Deep Convolutional Neural Networks,Advances in Neural Information Processing Systems,,,,https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,"We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.",2012,2023-12-15 20:11:16,2023-12-16 21:12:18,2023-12-15 20:11:16,1097-1105,,,25,,,,,,,,"Curran Associates, Inc.",,,,,,,Neural Information Processing Systems,,,,C:\Users\test\Zotero\storage\FKTSXN22\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QN63PNJQ,preprint,2016,"Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua",Neural Machine Translation by Jointly Learning to Align and Translate,,,,10.48550/arXiv.1409.0473,http://arxiv.org/abs/1409.0473,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",2016-05-19,2023-12-15 21:19:46,2023-12-15 21:19:46,2023-12-15 21:19:46,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,"arXiv:1409.0473 [cs, stat]",,C:\Users\test\Zotero\storage\8AI9Y35A\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf; C:\Users\test\Zotero\storage\IX7ZCCJJ\1409.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:1409.0473,,,,,,,,,,,,,,,,,,,,,,,,,,,
829E4WPA,preprint,2023,"Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia",Attention Is All You Need,,,,10.48550/arXiv.1706.03762,http://arxiv.org/abs/1706.03762,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",2023-08-01,2023-12-15 21:22:28,2023-12-15 21:22:28,2023-12-15 21:22:28,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1706.03762 [cs],,C:\Users\test\Zotero\storage\DY65ESM9\Vaswani et al. - 2023 - Attention Is All You Need.pdf; C:\Users\test\Zotero\storage\4VPZQG2G\1706.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:1706.03762,,,,,,,,,,,,,,,,,,,,,,,,,,,
VAJRXVNY,journalArticle,2020,"Sherstinsky, Alex",Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network,Physica D: Nonlinear Phenomena,,01672789,10.1016/j.physd.2019.132306,http://arxiv.org/abs/1808.03314,"Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of ""unrolling"" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the ""Vanilla LSTM"" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.",2020-03,2023-12-16 15:55:07,2023-12-16 15:55:07,2023-12-16 15:55:07,132306,,,404,,Physica D: Nonlinear Phenomena,,,,,,,,,,,,,arXiv.org,,"arXiv:1808.03314 [cs, stat]",,C:\Users\test\Zotero\storage\AMLAA4BQ\Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and.pdf; C:\Users\test\Zotero\storage\DDEIX5SN\1808.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NY56KXEW,conferencePaper,2006,"Graves, Alex; Fernández, Santiago; Gomez, Faustino; Schmidhuber, Jürgen",Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,Proceedings of the 23rd international conference on Machine learning,978-1-59593-383-6,,10.1145/1143844.1143891,https://dl.acm.org/doi/10.1145/1143844.1143891,"Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.",2006-06-25,2023-12-16 21:14:00,2023-12-16 21:14:00,2023-12-16,369–376,,,,,,Connectionist temporal classification,ICML '06,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,C:\Users\test\Zotero\storage\FS9L227Z\Graves et al. - 2006 - Connectionist temporal classification labelling u.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2DUWR4KK,preprint,2019,"Lugaresi, Camillo; Tang, Jiuqiang; Nash, Hadon; McClanahan, Chris; Uboweja, Esha; Hays, Michael; Zhang, Fan; Chang, Chuo-Ling; Yong, Ming Guang; Lee, Juhyun; Chang, Wan-Teh; Hua, Wei; Georg, Manfred; Grundmann, Matthias",MediaPipe: A Framework for Building Perception Pipelines,,,,10.48550/arXiv.1906.08172,http://arxiv.org/abs/1906.08172,"Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.",2019-06-14,2023-12-18 16:15:40,2023-12-18 16:15:40,2023-12-18 16:15:40,,,,,,,MediaPipe,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1906.08172 [cs],,C:\Users\test\Zotero\storage\TL3SCWJQ\Lugaresi et al. - 2019 - MediaPipe A Framework for Building Perception Pip.pdf; C:\Users\test\Zotero\storage\PKF8C8CU\1906.html,,,"Computer Science - Distributed, Parallel, and Cluster Computing",,,,,,,,,,,,,,,,,,,arXiv:1906.08172,,,,,,,,,,,,,,,,,,,,,,,,,,,
VUG6HXEQ,journalArticle,,"Culjak, Ivan; Abram, David; Pribanic, Tomislav; Dzapo, Hrvoje; Cifrek, Mario",A brief introduction to OpenCV,,,,,,"The purpose of this paper is to introduce and quickly make a reader familiar with OpenCV (Open Source Computer Vision) basics without having to go through the lengthy reference manuals and books. OpenCV is an open source library for image and video analysis, originally introduced more than decade ago by Intel. Since then, a number of programmers have contributed to the most recent library developments. The latest major change took place in 2009 (OpenCV 2) which includes main changes to the C++ interface. Nowadays the library has >2500 optimized algorithms. It is extensively used around the world, having >2.5M downloads and >40K people in the user group. Regardless of whether one is a novice C++ programmer or a professional software developer, unaware of OpenCV, the main library content should be interesting for the graduate students and researchers in image processing and computer vision areas. To master every library element it is necessary to consult many books available on the topic of OpenCV. However, reading such more comprehensive material should be easier after comprehending some basics about OpenCV from this paper.",,2023-12-18 16:31:10,2023-12-18 16:31:10,,,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\test\Zotero\storage\S8CRTPLE\Culjak et al. - A brief introduction to OpenCV.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48Z9CJGB,conferencePaper,2023,"Srinivasan, R; Kavita, R; Kavitha, M; Mallikarjuna, Basetty; Bhatia, Sandeep; Agarwal, Bhawna; Ahlawat, Vinay; Goel, Amit",Python And Opencv For Sign Language Recognition,"2023 International Conference on Device Intelligence, Computing and Communication Technologies, (DICCT)",,,10.1109/DICCT56244.2023.10110225,https://ieeexplore.ieee.org/document/10110225,"Hearing-impaired people cannot communicate with normal people easily. Most people are not aware of sign language recognition. To support this, machine learning and CV can be used to create an impact on the impaired. This can be improved into automatic editors, in which the person can easily understand the sign language of the impaired people by just using hand sign recognition. In non-verbal communication, hand gesture always being an important mode of communication and it plays a vital role to bridge gap between deaf and dumb people. Several sign language recognition systems have been developed but the systems are not flexible and cost-effective. Physically challenged people can express their emotions and feelings through sign language. In this paper, we develop a sign detector that can detect signs and numbers and also other signs that are used in sign language with the help of OpenCV and Keras modules in python. By using this technology, we can understand what they want to convey through sign language which is not a common language to communicate with people. OpenCV and Keras of python are the modules used to achieve our work and the proposed work proved to be a user-friendly approach to communication by using Python language to recognize sign languages for hearing-impaired persons.",2023-03,2023-12-18 16:34:42,2023-12-18 16:34:42,2023-12-18 16:34:42,1-5,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,C:\Users\test\Zotero\storage\UKFDHB7G\10110225.html; C:\Users\test\Zotero\storage\XTHMKVND\Srinivasan et al. - 2023 - Python And Opencv For Sign Language Recognition.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2023 International Conference on Device Intelligence, Computing and Communication Technologies, (DICCT)",,,,,,,,,,,,,,,
I9I5UUFI,preprint,2016,"Abadi, Martín; Barham, Paul; Chen, Jianmin; Chen, Zhifeng; Davis, Andy; Dean, Jeffrey; Devin, Matthieu; Ghemawat, Sanjay; Irving, Geoffrey; Isard, Michael; Kudlur, Manjunath; Levenberg, Josh; Monga, Rajat; Moore, Sherry; Murray, Derek G.; Steiner, Benoit; Tucker, Paul; Vasudevan, Vijay; Warden, Pete; Wicke, Martin; Yu, Yuan; Zheng, Xiaoqiang",TensorFlow: A system for large-scale machine learning,,,,10.48550/arXiv.1605.08695,http://arxiv.org/abs/1605.08695,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",2016-05-31,2023-12-18 16:51:02,2023-12-18 16:51:02,2023-12-18 16:51:02,,,,,,,TensorFlow,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1605.08695 [cs],,C:\Users\test\Zotero\storage\3ZQEDSI8\Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf; C:\Users\test\Zotero\storage\CVZGU8KX\1605.html,,,"Computer Science - Distributed, Parallel, and Cluster Computing; Computer Science - Artificial Intelligence",,,,,,,,,,,,,,,,,,,arXiv:1605.08695,,,,,,,,,,,,,,,,,,,,,,,,,,,
ECR2UXHJ,preprint,2019,"Paszke, Adam; Gross, Sam; Massa, Francisco; Lerer, Adam; Bradbury, James; Chanan, Gregory; Killeen, Trevor; Lin, Zeming; Gimelshein, Natalia; Antiga, Luca; Desmaison, Alban; Köpf, Andreas; Yang, Edward; DeVito, Zach; Raison, Martin; Tejani, Alykhan; Chilamkurthy, Sasank; Steiner, Benoit; Fang, Lu; Bai, Junjie; Chintala, Soumith","PyTorch: An Imperative Style, High-Performance Deep Learning Library",,,,10.48550/arXiv.1912.01703,http://arxiv.org/abs/1912.01703,"Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",2019-12-03,2023-12-18 16:51:18,2023-12-18 16:51:18,2023-12-18 16:51:18,,,,,,,PyTorch,,,,,arXiv,,,,,,,arXiv.org,,"arXiv:1912.01703 [cs, stat]",,"C:\Users\test\Zotero\storage\KTP3SUJU\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf; C:\Users\test\Zotero\storage\94839W9V\1912.html",,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Mathematical Software,,,,,,,,,,,,,,,,,,,arXiv:1912.01703,,,,,,,,,,,,,,,,,,,,,,,,,,,
THRKQAF9,preprint,2020,"Gulati, Anmol; Qin, James; Chiu, Chung-Cheng; Parmar, Niki; Zhang, Yu; Yu, Jiahui; Han, Wei; Wang, Shibo; Zhang, Zhengdong; Wu, Yonghui; Pang, Ruoming",Conformer: Convolution-augmented Transformer for Speech Recognition,,,,10.48550/arXiv.2005.08100,http://arxiv.org/abs/2005.08100,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",2020-05-16,2023-12-18 21:12:23,2023-12-18 21:12:23,2023-12-18 21:12:23,,,,,,,Conformer,,,,,arXiv,,,,,,,arXiv.org,,"arXiv:2005.08100 [cs, eess]",,C:\Users\test\Zotero\storage\FU6XEWW8\Gulati et al. - 2020 - Conformer Convolution-augmented Transformer for S.pdf; C:\Users\test\Zotero\storage\6CR52HYI\2005.html,,,Computer Science - Machine Learning; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing,,,,,,,,,,,,,,,,,,,arXiv:2005.08100,,,,,,,,,,,,,,,,,,,,,,,,,,,
I83JW5CV,preprint,2022,"Kim, Sehoon; Gholami, Amir; Shaw, Albert; Lee, Nicholas; Mangalam, Karttikeya; Malik, Jitendra; Mahoney, Michael W.; Keutzer, Kurt",Squeezeformer: An Efficient Transformer for Automatic Speech Recognition,,,,10.48550/arXiv.2206.00888,http://arxiv.org/abs/2206.00888,"The recently proposed Conformer model has become the de facto backbone model for various downstream speech tasks based on its hybrid attention-convolution architecture that captures both local and global features. However, through a series of systematic studies, we find that the Conformer architecture's design choices are not optimal. After re-examining the design choices for both the macro and micro-architecture of Conformer, we propose Squeezeformer which consistently outperforms the state-of-the-art ASR models under the same training schemes. In particular, for the macro-architecture, Squeezeformer incorporates (i) the Temporal U-Net structure which reduces the cost of the multi-head attention modules on long sequences, and (ii) a simpler block structure of multi-head attention or convolution modules followed up by feed-forward module instead of the Macaron structure proposed in Conformer. Furthermore, for the micro-architecture, Squeezeformer (i) simplifies the activations in the convolutional block, (ii) removes redundant Layer Normalization operations, and (iii) incorporates an efficient depthwise down-sampling layer to efficiently sub-sample the input signal. Squeezeformer achieves state-of-the-art results of 7.5%, 6.5%, and 6.0% word-error-rate (WER) on LibriSpeech test-other without external language models, which are 3.1%, 1.4%, and 0.6% better than Conformer-CTC with the same number of FLOPs. Our code is open-sourced and available online.",2022-10-15,2023-12-18 21:14:53,2023-12-18 21:14:53,2023-12-18 21:14:53,,,,,,,Squeezeformer,,,,,arXiv,,,,,,,arXiv.org,,"arXiv:2206.00888 [cs, eess]",,C:\Users\test\Zotero\storage\UXYTA3FJ\Kim et al. - 2022 - Squeezeformer An Efficient Transformer for Automa.pdf; C:\Users\test\Zotero\storage\TTAC3JU9\2206.html,,,Computer Science - Computation and Language; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing,,,,,,,,,,,,,,,,,,,arXiv:2206.00888,,,,,,,,,,,,,,,,,,,,,,,,,,,