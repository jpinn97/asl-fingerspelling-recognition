@inproceedings{abiyevReconstructionConvolutionalNeural2020,
  title = {Reconstruction of {{Convolutional Neural Network}} for {{Sign Language Recognition}}},
  booktitle = {2020 {{International Conference}} on {{Electrical}}, {{Communication}}, and {{Computer Engineering}} ({{ICECCE}})},
  author = {Abiyev, Rahib and Idoko, John Bush and Arslan, Murat},
  year = {2020},
  month = jun,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Istanbul, Turkey}},
  doi = {10.1109/ICECCE49384.2020.9179356},
  url = {https://ieeexplore.ieee.org/document/9179356/},
  urldate = {2023-11-29},
  abstract = {This paper presents a Sign Language translation model using Convolutional Neural Networks (CNN). A sign language is a language which allows mute and hearingimpaired people to communicate. It is a visually oriented, nonverbal communication which facilitates communication through body/facial postures, expressions and a collection of gestures. To contribute to the wellbeing of the affected population, we are motivated to implement a vision-based system to avert their day to day challenges. Our propose model constitutes object detection and classification phases. The first module is made up of single shot multi-box detector (SSD) used for hand detection. The second module constitutes convolutional neural network plus a fully connected network utilized to constructively translate the detected signs into text. The propose model is implemented using American sign language fingerspelling dataset. The propose system outperformed other published results in the comparative analysis, hence recommended for further exploitation in sign language recognition problems.},
  isbn = {978-1-72817-116-6},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\UFTNKESY\Abiyev et al. - 2020 - Reconstruction of Convolutional Neural Network for.pdf}
}

@article{adeyanjuMachineLearningMethods2021,
  title = {Machine Learning Methods for Sign Language Recognition: {{A}} Critical Review and Analysis},
  shorttitle = {Machine Learning Methods for Sign Language Recognition},
  author = {Adeyanju, I.A. and Bello, O.O. and Adegboye, M.A.},
  year = {2021},
  month = nov,
  journal = {Intelligent Systems with Applications},
  volume = {12},
  pages = {200056},
  issn = {26673053},
  doi = {10.1016/j.iswa.2021.200056},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2667305321000454},
  urldate = {2023-11-29},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\WDP9WTXW\Adeyanju et al. - 2021 - Machine learning methods for sign language recogni.pdf}
}

@inproceedings{bantupalliAmericanSignLanguage2018,
  title = {American {{Sign Language Recognition}} Using {{Deep Learning}} and {{Computer Vision}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Bantupalli, Kshitij and Xie, Ying},
  year = {2018},
  month = dec,
  pages = {4896--4899},
  doi = {10.1109/BigData.2018.8622141},
  url = {https://ieeexplore.ieee.org/document/8622141},
  urldate = {2023-11-29},
  abstract = {Speech impairment is a disability which affects an individuals ability to communicate using speech and hearing. People who are affected by this use other media of communication such as sign language. Although sign language is ubiquitous in recent times, there remains a challenge for non-sign language speakers to communicate with sign language speakers or signers. With recent advances in deep learning and computer vision there has been promising progress in the fields of motion and gesture recognition using deep learning and computer vision based techniques. The focus of this work is to create a visionbased application which offers sign language translation to text thus aiding communication between signers and non-signers. The proposed model takes video sequences and extracts temporal and spatial features from them. We then use Inception, a CNN (Convolutional Neural Network) for recognizing spatial features. We then use a RNN (Recurrent Neural Network) to train on temporal features. The dataset used is the American Sign Language Dataset.},
  file = {C\:\\Users\\test\\Zotero\\storage\\DEKECKH3\\Bantupalli and Xie - 2018 - American Sign Language Recognition using Deep Lear.pdf;C\:\\Users\\test\\Zotero\\storage\\SHY89S3Q\\8622141.html}
}

@article{chongAmericanSignLanguage2018,
  title = {American {{Sign Language Recognition Using Leap Motion Controller}} with {{Machine Learning Approach}}},
  author = {Chong, Teak-Wei and Lee, Boon-Giin},
  year = {2018},
  month = oct,
  journal = {Sensors},
  volume = {18},
  number = {10},
  pages = {3554},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s18103554},
  url = {https://www.mdpi.com/1424-8220/18/10/3554},
  urldate = {2023-11-29},
  abstract = {Sign language is intentionally designed to allow deaf and dumb communities to convey messages and to connect with society. Unfortunately, learning and practicing sign language is not common among society; hence, this study developed a sign language recognition prototype using the Leap Motion Controller (LMC). Many existing studies have proposed methods for incomplete sign language recognition, whereas this study aimed for full American Sign Language (ASL) recognition, which consists of 26 letters and 10 digits. Most of the ASL letters are static (no movement), but certain ASL letters are dynamic (they require certain movements). Thus, this study also aimed to extract features from finger and hand motions to differentiate between the static and dynamic gestures. The experimental results revealed that the sign language recognition rates for the 26 letters using a support vector machine (SVM) and a deep neural network (DNN) are 80.30\% and 93.81\%, respectively. Meanwhile, the recognition rates for a combination of 26 letters and 10 digits are slightly lower, approximately 72.79\% for the SVM and 88.79\% for the DNN. As a result, the sign language recognition system has great potential for reducing the gap between deaf and dumb communities and others. The proposed prototype could also serve as an interpreter for the deaf and dumb in everyday life in service sectors, such as at the bank or post office.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {American Sign Language,deep neural network,human-computer interaction,Leap Motion Controller,machine learning,multi-class classification,sign language recognition,support vector machine},
  file = {C:\Users\test\Zotero\storage\QGWQXCYL\Chong and Lee - 2018 - American Sign Language Recognition Using Leap Moti.pdf}
}

@inproceedings{cihancamgozSignLanguageTransformers2020,
  title = {Sign {{Language Transformers}}: {{Joint End-to-End Sign Language Recognition}} and {{Translation}}},
  shorttitle = {Sign {{Language Transformers}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cihan Camgoz, Necati and Koller, Oscar and Hadfield, Simon and Bowden, Richard},
  year = {2020},
  month = jun,
  pages = {10020--10030},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01004},
  url = {https://ieeexplore.ieee.org/document/9156773/},
  urldate = {2023-11-29},
  abstract = {Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-theart in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-tosequence learning problems and leads to significant performance gains.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\MGNGYH32\Cihan Camgoz et al. - 2020 - Sign Language Transformers Joint End-to-End Sign .pdf}
}

@inproceedings{kabadeAmericanSignLanguage2023,
  title = {American {{Sign Language Fingerspelling Recognition}} Using {{Attention Model}}},
  booktitle = {2023 {{IEEE}} 8th {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
  author = {Kabade, Amruta E and Desai, Padmashree and C, Sujatha and G, Shankar},
  year = {2023},
  month = apr,
  pages = {1--6},
  doi = {10.1109/I2CT57861.2023.10126277},
  url = {https://ieeexplore.ieee.org/document/10126277},
  urldate = {2023-11-29},
  abstract = {Sign Language Recognition(SLR) is a complex gesture recognition problem because of the quick and highly coarticulated motion involved in gestures. This research work focuses on Fingerspelling recognition task, which constitutes 35\% of the American Sign Language (ASL). Fingerspelling identifies the word letter by letter. Fingerspelling is used for signing the words which do not have designated ASL signs such as technical terms, content words and proper nouns. In our proposed work for ASL Fingerspelling recognition, we consider ChicagoFSWild dataset which consists of occlusions and images captured in varying illuminations, lighting conditions (in the wild environments). The optical flow is obtained from Lucas-Kanade algorithm, prior is generated, images are resized and cropped with face-roi technique to get the region of interest (ROI). The visual attention mechanism attends to the ROI iteratively. ResNet, pretrained on Imagenet is used for the extraction of spatial features. The Bi-LSTM network with Connectionist Temporal Classification (CTC) is used to predict the sign. It provides the accuracy of 57\% on ChicagoFSWild dataset for Fingerspelling recognition task.},
  file = {C:\Users\test\Zotero\storage\E8II75TX\Kabade et al. - 2023 - American Sign Language Fingerspelling Recognition .pdf}
}

@article{mitchellHowManyPeople2006,
  title = {How {{Many People Use ASL}} in the {{United States}}?: {{Why Estimates Need Updating}}},
  shorttitle = {How {{Many People Use ASL}} in the {{United States}}?},
  author = {Mitchell, Ross E. and Young, Travas A. and Bachelda, Bellamie and Karchmer, Michael A.},
  year = {2006},
  journal = {Sign Language Studies},
  volume = {6},
  number = {3},
  eprint = {26190621},
  eprinttype = {jstor},
  pages = {306--335},
  publisher = {{Gallaudet University Press}},
  issn = {0302-1475},
  url = {https://www.jstor.org/stable/26190621},
  urldate = {2023-11-05},
  abstract = {This article traces the sources of the estimates of the number of American Sign Language users in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all of the data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates nor any specific study of ASL use. The article concludes with a call to action to rectify this problem.},
  file = {C:\Users\test\Zotero\storage\RC447MA5\Mitchell et al. - 2006 - How Many People Use ASL in the United States Why.pdf}
}

@inproceedings{nguyenDeepLearningAmerican2019,
  title = {Deep {{Learning}} for {{American Sign Language Fingerspelling Recognition System}}},
  booktitle = {2019 26th {{International Conference}} on {{Telecommunications}} ({{ICT}})},
  author = {Nguyen, Huy B.D and Do, Hung Ngoc},
  year = {2019},
  month = apr,
  pages = {314--318},
  publisher = {{IEEE}},
  address = {{Hanoi, Vietnam}},
  doi = {10.1109/ICT.2019.8798856},
  url = {https://ieeexplore.ieee.org/document/8798856/},
  urldate = {2023-11-29},
  abstract = {Sign language has always been a major tool for communication among people with disabilities. In this paper, a sign language fingerspelling alphabet identification system would be developed by using image processing technique, supervised machine learning and deep learning. In particular, 24 alphabetical symbols are presented by several combinations of static gestures (excluding 2 motion gestures J and Z). Histogram of Oriented Gradients (HOG) and Local Binary Pattern (LBP) features of each gesture will be extracted from training images. Then Multiclass Support Vector Machines (SVMs) will be applied to train these extracted data. Also, an end-to-end Convolutional Neural Network (CNN) architecture will be applied to the training dataset for comparison. After that, a further combination of CNN as feature descriptor and SVM produces an acceptable result. The Massey Dataset is implemented in the training and testing phases of the whole system.},
  isbn = {978-1-72810-273-3},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\ZCNRKPQK\Nguyen and Do - 2019 - Deep Learning for American Sign Language Fingerspe.pdf}
}

@article{rastgooSignLanguageRecognition2021,
  title = {Sign {{Language Recognition}}: {{A Deep Survey}}},
  shorttitle = {Sign {{Language Recognition}}},
  author = {Rastgoo, Razieh and Kiani, Kourosh and Escalera, Sergio},
  year = {2021},
  month = feb,
  journal = {Expert Systems with Applications},
  volume = {164},
  pages = {113794},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.113794},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S095741742030614X},
  urldate = {2023-11-29},
  abstract = {Sign language, as a different form of the communication language, is important to large groups of people in society. There are different signs in each sign language with variability in hand shape, motion profile, and position of the hand, face, and body parts contributing to each sign. So, visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the visionbased proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved. We present a taxonomy to categorize the proposed models for isolated and continuous sign language recognition, discussing applications, datasets, hybrid models, complexity, and future lines of research in the field.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\NXZMWM3W\Rastgoo et al. - 2021 - Sign Language Recognition A Deep Survey.pdf}
}

@inproceedings{shiAmericanSignLanguage2018,
  title = {American {{Sign Language Fingerspelling Recognition}} in the {{Wild}}},
  booktitle = {2018 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Shi, Bowen and Del Rio, Aurora Martinez and Keane, Jonathan and Michaux, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2018},
  month = dec,
  pages = {145--152},
  doi = {10.1109/SLT.2018.8639639},
  url = {https://ieeexplore.ieee.org/abstract/document/8639639},
  urldate = {2023-11-29},
  abstract = {We address the problem of American Sign Language fingerspelling recognition ``in the wild'', using videos collected from websites. We introduce the largest data set available so far for the problem of fingerspelling recognition, and the first using naturally occurring video data. Using this data set, we present the first attempt to recognize fingerspelling sequences in this challenging setting. Unlike prior work, our video data is extremely challenging due to low frame rates and visual variability. To tackle the visual challenges, we train a special-purpose signing hand detector using a small subset of our data. Given the hand detector output, a sequence model decodes the hypothesized fingerspelled letter sequence. For the sequence model, we explore attention-based recurrent encoder-decoders and CTC-based approaches. As the first attempt at fingerspelling recognition in the wild, this work is intended to serve as a baseline for future work on sign language recognition in realistic conditions. We find that, as expected, letter error rates are much higher than in previous work on more controlled data, and we analyze the sources of error and effects of model variants.},
  file = {C\:\\Users\\test\\Zotero\\storage\\M9RBGXG2\\Shi et al. - 2018 - American Sign Language Fingerspelling Recognition .pdf;C\:\\Users\\test\\Zotero\\storage\\CE65XA5S\\8639639.html}
}

@inproceedings{shiFingerspellingDetectionAmerican2021,
  title = {Fingerspelling {{Detection}} in {{American Sign Language}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shi, Bowen and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2021},
  month = jun,
  pages = {4164--4173},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00415},
  url = {https://ieeexplore.ieee.org/document/9578297/},
  urldate = {2023-11-29},
  abstract = {Fingerspelling, in which words are signed letter by letter, is an important component of American Sign Language. Most previous work on automatic fingerspelling recognition has assumed that the boundaries of fingerspelling regions in signing videos are known beforehand. In this paper, we consider the task of fingerspelling detection in raw, untrimmed sign language videos. This is an important step towards building real-world fingerspelling recognition systems. We propose a benchmark and a suite of evaluation metrics, some of which reflect the effect of detection on the downstream fingerspelling recognition task. In addition, we propose a new model that learns to detect fingerspelling via multi-task training, incorporating pose estimation and fingerspelling recognition (transcription) along with detection, and compare this model to several alternatives. The model outperforms all alternative approaches across all metrics, establishing a state of the art on the benchmark.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\3DXLW7AJ\Shi et al. - 2021 - Fingerspelling Detection in American Sign Language.pdf}
}

@inproceedings{shiFingerspellingRecognitionWild2019,
  title = {Fingerspelling {{Recognition}} in the {{Wild With Iterative Visual Attention}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Shi, Bowen and Rio, Aurora Martinez Del and Keane, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2019},
  month = oct,
  pages = {5399--5408},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00550},
  url = {https://ieeexplore.ieee.org/document/9010036/},
  urldate = {2023-11-29},
  abstract = {Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It outperforms prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\VNP3293B\Shi et al. - 2019 - Fingerspelling Recognition in the Wild With Iterat.pdf}
}

@inproceedings{skumarTimeSeriesNeural2018,
  title = {Time {{Series Neural Networks}} for {{Real Time Sign Language Translation}}},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {S Kumar, Sujay and Wangyal, Tenzin and Saboo, Varun and Srinath, Ramamoorthy},
  year = {2018},
  month = dec,
  pages = {243--248},
  publisher = {{IEEE}},
  address = {{Orlando, FL}},
  doi = {10.1109/ICMLA.2018.00043},
  url = {https://ieeexplore.ieee.org/document/8614068/},
  urldate = {2023-11-29},
  abstract = {Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.},
  isbn = {978-1-5386-6805-4},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\JPEHN7HA\S Kumar et al. - 2018 - Time Series Neural Networks for Real Time Sign Lan.pdf}
}

@inproceedings{weerasooriyaSinhalaFingerspellingSign2022,
  title = {Sinhala {{Fingerspelling Sign Language Recognition}} with {{Computer Vision}}},
  booktitle = {2022 {{Moratuwa Engineering Research Conference}} ({{MERCon}})},
  author = {Weerasooriya, Amal A. and Ambegoda, Thanuja D.},
  year = {2022},
  month = jul,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Moratuwa, Sri Lanka}},
  doi = {10.1109/MERCon55799.2022.9906281},
  url = {https://ieeexplore.ieee.org/document/9906281/},
  urldate = {2023-11-29},
  abstract = {Computer vision based sign language translation is usually based on using thousands of images or video sequences for model training. This is not an issue in the case of widely used languages such as American Sign Language. However, in case of languages with low resources such as Sinhala Sign Language, it's challenging to use similar methods for developing translators since there are no known data sets available for such studies.},
  isbn = {978-1-66548-786-3},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\J4VM2MWH\Weerasooriya and Ambegoda - 2022 - Sinhala Fingerspelling Sign Language Recognition w.pdf}
}
