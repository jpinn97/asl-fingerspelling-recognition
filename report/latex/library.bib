@inproceedings{amdahlValiditySingleProcessor1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference on - {{AFIPS}} '67 ({{Spring}})},
  author = {Amdahl, Gene M.},
  date = {1967},
  pages = {483},
  publisher = {{ACM Press}},
  location = {{Atlantic City, New Jersey}},
  doi = {10.1145/1465482.1465560},
  url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
  urldate = {2023-11-15},
  eventtitle = {The {{April}} 18-20, 1967, Spring Joint Computer Conference},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\B4N2XF6M\Amdahl - 1967 - Validity of the single processor approach to achie.pdf}
}

@online{AmericanSignLanguage,
  title = {American {{Sign Language}} | {{Ethnologue Free}}},
  url = {https://www.ethnologue.com/language/ase/},
  urldate = {2023-11-05},
  abstract = {American Sign Language is a stable indigenous language of the United States. It is a Deaf community sign language. The language is used as a first language by all in the ethnic community. It is not known to be taught in schools.},
  langid = {english},
  organization = {{Ethnologue (Free All)}},
  file = {C:\Users\test\Zotero\storage\H57QK6YX\ase.html}
}

@artwork{benedikt.seidlEnglishThisFigure2009,
  title = {English:  {{This}} Figure Shows the Operating Systems Used on the Supercomputers Listed on the {{Top500}} List. {{Data}} for This Figure Was Compiled from {{http://top500.org/stats}}},
  shorttitle = {English},
  author = {{Benedikt.Seidl}},
  date = {2009-02-12},
  url = {https://commons.wikimedia.org/wiki/File:Operating_systems_used_on_top_500_supercomputers.svg},
  urldate = {2023-11-09},
  file = {C:\Users\test\Zotero\storage\RUE4X2BS\FileOperating_systems_used_on_top_500_supercomputers.html}
}

@inreference{BeowulfCluster2023,
  title = {Beowulf Cluster},
  booktitle = {Wikipedia},
  date = {2023-09-29T15:18:42Z},
  url = {https://en.wikipedia.org/w/index.php?title=Beowulf_cluster&oldid=1177783482},
  urldate = {2023-11-10},
  abstract = {A Beowulf cluster is a computer cluster of what are normally identical, commodity-grade computers networked into a small local area network with libraries and programs installed which allow processing to be shared among them. The result is a high-performance parallel computing cluster from inexpensive personal computer hardware. The name Beowulf originally referred to a specific computer built in 1994 by Thomas Sterling and Donald Becker at NASA. The name "Beowulf" comes from the Old English epic poem of the same name.No particular piece of software defines a cluster as a Beowulf. Typically only free and open source software is used, both to save cost and to allow customization. Most Beowulf clusters run a Unix-like operating system, such as BSD, Linux, or Solaris. Commonly used parallel processing libraries include Message Passing Interface (MPI) and Parallel Virtual Machine (PVM). Both of these permit the programmer to divide a task among a group of networked computers, and collect the results of processing. Examples of MPI software include Open MPI  or MPICH. There are additional MPI implementations available. Beowulf systems operate worldwide, chiefly in support of scientific computing. Since 2017, every system on the Top500 list of the world's fastest supercomputers has used Beowulf software methods and a Linux operating system. At this level, however, most are by no means just assemblages of commodity hardware; custom design work is often required for the nodes (often blade servers), the networking and the cooling systems.},
  langid = {english},
  annotation = {Page Version ID: 1177783482},
  file = {C:\Users\test\Zotero\storage\DKLYYTIC\Beowulf_cluster.html}
}

@online{bradyQuickNoteGPU2023,
  title = {A {{Quick Note}} on {{GPU Accuracy}} and {{Double Precision}} | {{Blog}}},
  author = {Brady, Ryan},
  date = {2023-09},
  url = {https://www.experoinc.com//expero-resources/a-quick-note-on-gpu-accuracy-and-double-precision},
  urldate = {2023-11-10},
  abstract = {Single precision vs Double precision on a CPU vs GPU in high performance computing simulation.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\QW9P5SBQ\a-quick-note-on-gpu-accuracy-and-double-precision.html}
}

@online{brianWhatInfiniBandNetwork2021,
  title = {What Is {{InfiniBand Network}} and the {{Difference}} with {{Ethernet}}?},
  author = {Brian},
  date = {2021-12-01},
  url = {https://www.fibermall.com/blog/what-is-infiniband-network-and-difference-with-ethernet.htm},
  urldate = {2023-11-10},
  abstract = {What is the InfiniBand Network? The InfiniBand architecture brings fabric consolidation to the data center Storage networking can concurrently run~with cluster},
  langid = {american},
  organization = {{fibermall.com}},
  file = {C:\Users\test\Zotero\storage\GD6LZPVU\what-is-infiniband-network-and-difference-with-ethernet.html}
}

@online{CrayClusterStorE10002023,
  title = {Cray {{ClusterStor E1000}} Storage System – {{System}} Overview},
  date = {2023},
  url = {https://www.hpe.com/psnow/doc/a50001954enw.pdf?jumpid=in_pdfviewer-psnow},
  urldate = {2023-11-15},
  abstract = {This technical white paper gives system architects in HPE, at HPE partners and at HPC customers/prospects a technical overview of the Cray ClusterStor E1000 storage system.},
  langid = {english},
  organization = {{PSNow}},
  file = {C:\Users\test\Zotero\storage\ZBIXJU5Z\a50001954enw.html}
}

@article{dennardDesignIonimplantedMOSFET1974,
  title = {Design of Ion-Implanted {{MOSFET}}'s with Very Small Physical Dimensions},
  author = {Dennard, R.H. and Gaensslen, F.H. and Yu, Hwa-Nien and Rideout, V.L. and Bassous, E. and LeBlanc, A.R.},
  date = {1974-10},
  journaltitle = {IEEE Journal of Solid-State Circuits},
  volume = {9},
  number = {5},
  pages = {256--268},
  issn = {1558-173X},
  doi = {10.1109/JSSC.1974.1050511},
  url = {https://ieeexplore.ieee.org/document/1050511},
  urldate = {2023-11-08},
  abstract = {This paper considers the design, fabrication, and characterization of very small Mosfet switching devices suitable for digital integrated circuits, using dimensions of the order of 1 /spl mu/. Scaling relationships are presented which show how a conventional MOSFET can be reduced in size. An improved small device structure is presented that uses ion implantation, to provide shallow source and drain regions and a nonuniform substrate doping profile. One-dimensional models are used to predict the substrate doping profile and the corresponding threshold voltage versus source voltage characteristic. A two-dimensional current transport model is used to predict the relative degree of short-channel effects for different device parameter combinations. Polysilicon-gate MOSFET's with channel lengths as short as 0.5 /spl mu/ were fabricated, and the device characteristics measured and compared with predicted values. The performance improvement expected from using these very small devices in highly miniaturized integrated circuits is projected.},
  eventtitle = {{{IEEE Journal}} of {{Solid-State Circuits}}},
  file = {C\:\\Users\\test\\Zotero\\storage\\34KWE724\\Dennard et al. - 1974 - Design of ion-implanted MOSFET's with very small p.pdf;C\:\\Users\\test\\Zotero\\storage\\ZRLA468P\\1050511.html}
}

@online{DevelopmentTimeTOP500,
  title = {Development over {{Time}} | {{TOP500}}},
  url = {https://www.top500.org/statistics/overtime/},
  urldate = {2023-11-08},
  file = {C:\Users\test\Zotero\storage\38M8P6JY\overtime.html}
}

@online{FileLustreFile2017,
  title = {File:{{Lustre File System Overview}} ({{DNE}}) Lowres v1.Png - {{Lustre Wiki}}},
  date = {2017-08-07},
  url = {https://wiki.lustre.org/File:Lustre_File_System_Overview_(DNE)_lowres_v1.png},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\IC5ALB8W\FileLustre_File_System_Overview_(DNE)_lowres_v1.html}
}

@article{frazelleChippingAwayMoore2020,
  title = {Chipping {{Away}} at {{Moore}}’s {{Law}}: {{Modern CPUs}} Are Just Chiplets Connected Together.},
  shorttitle = {Chipping {{Away}} at {{Moore}}’s {{Law}}},
  author = {Frazelle, Jessie},
  date = {2020-02-29},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {18},
  number = {1},
  pages = {5--15},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/3387945.3388515},
  url = {https://dl.acm.org/doi/10.1145/3387945.3388515},
  urldate = {2023-11-08},
  abstract = {Smaller transistors can do more calculations without overheating, which makes them more power efficient. It also allows for smaller die sizes, which reduce costs and can increase density, allowing more cores per chip. The silicon wafers that chips are made of vary in purity, and none are perfect, which means every chip has a chance of having imperfections that differ in effect. Manufacturers can limit the effect of imperfections by using chiplets.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\ZEDCWJ8W\Frazelle - 2020 - Chipping Away at Moore’s Law Modern CPUs are just.pdf}
}

@online{Frontier,
  title = {Frontier},
  url = {https://www.olcf.ornl.gov/frontier/#4},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\2BKYXE45\frontier.html}
}

@article{gillespieAmdahlLawGustafson2008,
  title = {Amdahl's {{Law}}, {{Gustafson}}'s {{Trend}}, and the {{Performance Limits}} of {{Parallel Applications}}},
  author = {Gillespie, Matt},
  date = {2008},
  abstract = {Parallelization is a core strategic-planning consideration for all software makers, and the amount of performance benefit available from parallelizing a given application (or part of an application) is a key aspect of setting performance goals for the parallelization process. Theoretical discussions of performance potential are necessarily the starting point for understanding the critical issues involved, before moving to practical issues associated with a given project. Amdahl's Law and its modification by Gustafson's trend give us the basic means to understand what's possible for a given application, and tools and best practices give us the means to decide how to use that information in practice.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\5IZPPURB\Gillespie - Amdahl's Law, Gustafson's Trend, and the Performan.pdf}
}

@online{guptaWhatDoublePrecisionTensor2020,
  title = {What {{Is}} a {{Double-Precision Tensor Core}}?},
  author = {Gupta, Geetika},
  date = {2020-05-14T12:59:55+00:00},
  url = {https://blogs.nvidia.com/blog/2020/05/14/double-precision-tensor-cores/},
  urldate = {2023-11-10},
  abstract = {A Double-Precision Tensor Core in the NVIDIA Ampere architecture speeds FP64 math for simulations and iterative solvers in high performance computing.},
  langid = {american},
  organization = {{NVIDIA Blog}},
  file = {C:\Users\test\Zotero\storage\JUBTVASE\double-precision-tensor-cores.html}
}

@online{hewlettpackardenterprisesHPCServiceAccelerate2023,
  title = {{{HPC}} as a Service to Accelerate Transformational Growth},
  author = {{Hewlett Packard Enterprises}},
  date = {2023},
  url = {https://www.hpe.com/psnow/doc/a50004225enw.pdf?jumpid=in_pdfviewer-psnow},
  urldate = {2023-11-15},
  abstract = {High-performance computing (HPC) offers enterprises better data analytics, simulations, and artificial intelligence. It enables them to be smarter, faster, and more competitive.},
  langid = {english},
  organization = {{PSNow}},
  file = {C:\Users\test\Zotero\storage\BSFIQQ4L\a50004225enw.html}
}

@online{HPEGreenLakeHigh,
  title = {{{HPE GreenLake}} for {{High Performance Computing}} - {{HPC}}},
  url = {https://www.hpe.com/us/en/hpe-greenlake-hpc.html},
  urldate = {2023-11-15},
  abstract = {HPE GreenLake for HPC allows you to make faster decisions \& reduce time to discovery without the burden of high-performance computing infrastructure management.},
  langid = {american},
  file = {C:\Users\test\Zotero\storage\4DHHPJ29\hpe-greenlake-hpc.html}
}

@online{IEEEXploreFullText,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7921010},
  urldate = {2023-11-14},
  file = {C:\Users\test\Zotero\storage\WBY9QAEI\stamp.html}
}

@online{leslieVirtualMachinePhysical2018,
  title = {Virtual {{Machine}} or {{Physical Server}}},
  author = {Leslie},
  date = {2018-01-08T02:02:26+00:00},
  url = {https://actusdigital.com/2018/01/08/virtual-machine-or-physical-server/},
  urldate = {2023-11-14},
  abstract = {We are more and more asked about VM (Virtual Machines). There are numerous advantages of VM over physical servers and there is a lot of buzz and many broadcasters are asking us to provide proposals based on VMs. But there are also some Myths involved.},
  langid = {american},
  organization = {{Actus Digital}}
}

@online{liOpenAIGPT3Language2020,
  title = {{{OpenAI}}'s {{GPT-3 Language Model}}: {{A Technical Overview}}},
  shorttitle = {{{OpenAI}}'s {{GPT-3 Language Model}}},
  author = {Li, Chuan},
  date = {2020-06-03},
  url = {https://lambdalabs.com/blog/demystifying-gpt-3},
  urldate = {2023-11-10},
  abstract = {Chuan Li, PhD reviews GPT-3, the new NLP model from OpenAI. The technical overview covers how GPT-3 was trained, GPT-2 vs. GPT-3, and GPT-3 performance.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\XHDH4ETX\demystifying-gpt-3.html}
}

@inproceedings{liPerformanceOverheadComparison2017,
  title = {Performance {{Overhead Comparison}} between {{Hypervisor}} and {{Container Based Virtualization}}},
  booktitle = {2017 {{IEEE}} 31st {{International Conference}} on {{Advanced Information Networking}} and {{Applications}} ({{AINA}})},
  author = {Li, Zheng and Kihl, Maria and Lu, Qinghua and Andersson, Jens A.},
  date = {2017-03},
  pages = {955--962},
  issn = {1550-445X},
  doi = {10.1109/AINA.2017.79},
  url = {https://ieeexplore.ieee.org/abstract/document/7921010?casa_token=3TsqiTCW5qUAAAAA:UDvJe9vhXgLWaJ49b2LDuIeEXNuROOnR9QM2TxKYkfeQEZJMkjhtzDG8XGy6W4SyK8EqfVX2Kbx_kA},
  urldate = {2023-11-14},
  abstract = {The current virtualization solution in the Cloud widely relies on hypervisor-based technologies. Along with the recent popularity of Docker, the container-based virtualization starts receiving more attention for being a promising alternative. Since both of the virtualization solutions are not resource-free, their performance overheads would lead to negative impacts on the quality of Cloud services. To help fundamentally understand the performance difference between these two types of virtualization solutions, we use a physical machine with “just-enough” resource as a baseline to investigate the performance overhead of a standalone Docker container against a standalone virtual machine (VM). With findings contrary to the related work, our evaluation results show that the virtualization's performance overhead could vary not only on a feature-by-feature basis but also on a job-to-job basis. Although the container-based solution is undoubtedly lightweight, the hypervisor-based technology does not come with higher performance overhead in every case. For example, Docker containers particularly exhibit lower QoS in terms of storage transaction speed.},
  eventtitle = {2017 {{IEEE}} 31st {{International Conference}} on {{Advanced Information Networking}} and {{Applications}} ({{AINA}})},
  file = {C\:\\Users\\test\\Zotero\\storage\\GVNXBX8Z\\Li et al. - 2017 - Performance Overhead Comparison between Hypervisor.pdf;C\:\\Users\\test\\Zotero\\storage\\7HYGCPI4\\7921010.html}
}

@online{Lustrea,
  title = {Lustre},
  url = {https://www.lustre.org/},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\PU6RYNGE\www.lustre.org.html}
}

@article{mitchellHowManyPeople2006,
  title = {How {{Many People Use ASL}} in the {{United States}}?: {{Why Estimates Need Updating}}},
  shorttitle = {How {{Many People Use ASL}} in the {{United States}}?},
  author = {Mitchell, Ross E. and Young, Travas A. and Bachelda, Bellamie and Karchmer, Michael A.},
  date = {2006},
  journaltitle = {Sign Language Studies},
  volume = {6},
  number = {3},
  eprint = {26190621},
  eprinttype = {jstor},
  pages = {306--335},
  publisher = {{Gallaudet University Press}},
  issn = {0302-1475},
  url = {https://www.jstor.org/stable/26190621},
  urldate = {2023-11-05},
  abstract = {This article traces the sources of the estimates of the number of American Sign Language users in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all of the data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates nor any specific study of ASL use. The article concludes with a call to action to rectify this problem.},
  file = {C:\Users\test\Zotero\storage\RC447MA5\Mitchell et al. - 2006 - How Many People Use ASL in the United States Why.pdf}
}

@article{mooreCrammingMoreComponents1965,
  title = {Cramming More Components onto Integrated Circuits},
  author = {Moore, Gordon E},
  date = {1965},
  volume = {38},
  number = {8},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\VPC7SHDD\Moore - 1965 - Cramming more components onto integrated circuits.pdf}
}

@online{moorinsightsHPEGreenLakeHPC2022,
  title = {{{HPE GreenLake}} for {{HPC}} Delivers and {{Innovative}} and {{Flexible HPC}}/{{AI Solution}}, by {{Moor Insights}}},
  author = {{Moor Insights}},
  date = {2022-06},
  url = {https://www.hpe.com/psnow/doc/a00124727enw},
  urldate = {2023-11-15},
  abstract = {The whitepaper authored by Moor Insights and funded by HPE-NVIDIA covers HPE GreenLake for HPC and examines the environment, industry trends, and types of service providers to determine the best method to run the HPC for AI workloads.},
  langid = {english},
  organization = {{PSNow}},
  file = {C:\Users\test\Zotero\storage\HZR4E86S\a00124727enw.html}
}

@online{OpenMPI2023,
  title = {Open {{MPI}}:},
  date = {2023-10},
  url = {https://www.open-mpi.org/},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\RWVVZSSN\www.open-mpi.org.html}
}

@online{opensfsLustre2023,
  title = {Lustre},
  author = {{OpenSFS} and {EOFS}},
  date = {2023-11},
  url = {https://www.lustre.org/},
  urldate = {2023-11-10},
  organization = {{https://www.lustre.org/}},
  file = {C:\Users\test\Zotero\storage\V5XI4EVC\www.lustre.org.html}
}

@article{patelEnergyEfficientStrategy2020,
  title = {Energy Efficient Strategy for Placement of Virtual Machines Selected from Underloaded Servers in Compute {{Cloud}}},
  author = {Patel, Nimisha and Patel, Hiren},
  date = {2020-07-01},
  journaltitle = {Journal of King Saud University - Computer and Information Sciences},
  shortjournal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {32},
  number = {6},
  pages = {700--708},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2017.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1319157817302288},
  urldate = {2023-11-14},
  abstract = {Workload consolidation is a phase in Cloud datacenter where tasks are allocated among the available hosts in such a way that a minimal number of hosts is used and users’ need in terms of service level agreement (SLA) is fulfilled. To achieve workload consolidation, hosts are divided among three groups based on their utilization namely overloaded hosts, underloaded host and normal hosts. Detection of over or underloaded host is a challenging issue. Most of the existing researchers propose to use threshold values for such detection. We believe that there is a scope of improvement in existing methods of deciding underloaded hosts and subsequently taking off virtual machines (VMs) from them and placing them on other hosts. In this research, we propose Host Utilization Aware (HUA) Algorithm for underloaded host detection and placing its VMs on other hosts in a dynamic Cloud environment. We compare our proposed mechanism with existing one and with empirical analysis; it is shown that our proposal results into shutting off more number of hosts without compromising user’s workload requirement which leads to an energy-efficient workload consolidation with minimal migration costs and efficient utilization of active hosts.},
  keywords = {Energy efficiency,Underloaded server,Utilization,VM placement,Workload consolidation},
  file = {C\:\\Users\\test\\Zotero\\storage\\E9DI2WK3\\Patel and Patel - 2020 - Energy efficient strategy for placement of virtual.pdf;C\:\\Users\\test\\Zotero\\storage\\XX86G9SF\\S1319157817302288.html}
}

@online{PVMParallelVirtual,
  title = {{{PVM}}: {{Parallel Virtual Machine}}},
  url = {https://www.epm.ornl.gov/pvm/pvm_home.html},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\5NPHEJE8\pvm_home.html}
}

@software{ruppMicroprocessorTrendData2023,
  title = {Microprocessor {{Trend Data}}},
  author = {Rupp, Karl},
  date = {2023-11-07T21:08:28Z},
  origdate = {2018-02-15T05:10:17Z},
  url = {https://github.com/karlrupp/microprocessor-trend-data},
  urldate = {2023-11-08},
  abstract = {Data repository for my blog series on microprocessor trend data.}
}

@inproceedings{samaniExploringImpactVirtualization2022,
  title = {Exploring the {{Impact}} of {{Virtualization}} on the {{Usability}} of {{Deep Learning Applications}}},
  author = {Samani, Davood G. and Salehi, Mohsen Amini},
  date = {2022-05-01},
  pages = {442--451},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/CCGrid54584.2022.00054},
  url = {https://www.computer.org/csdl/proceedings-article/ccgrid/2022/995600a442/1F8zeX3BAQM},
  urldate = {2023-11-14},
  abstract = {Deep Learning-based (DL) applications are becoming increasingly popular and advancing at an unprecedented pace. While many research works are being undertaken to enhance Deep Neural Networks (DNN)-the centerpiece of DL applications-practical deployment challenges of these applications in the Cloud and Edge systems, and their impact on the usability of the applications have not been sufficiently investigated. In particular, the impact of deploying different virtualization platforms, offered by the Cloud and Edge, on the usability of DL applications (in terms of the End-to-End (E2E) inference time) has remained an open question. Importantly, resource elasticity (by means of scale-up), CPU pinning, and processor type (CPU vs GPU) configurations have shown to be influential on the virtualization overhead. Accordingly, the goal of this research is to study the impact of these potentially decisive deployment options on the E2E performance, thus, usability of the DL applications. To that end, we measure the impact of four popular execution platforms (namely, bare-metal, virtual machine (VM), container, and container in VM) on the E2E inference time of four types of DL applications, upon changing processor configuration (scale-up, CPU pinning) and processor types. This study reveals a set of interesting and sometimes counter-intuitive findings that can be used as best practices by Cloud solution architects to efficiently deploy DL applications in various systems. The notable finding is that the solution architects must be aware of the DL application characteristics, particularly, their pre- and post-processing requirements, to be able to optimally choose and configure an execution platform, determine the use of GPU, and decide the efficient scale-up range.},
  eventtitle = {2022 22nd {{International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet Computing}} ({{CCGrid}})},
  isbn = {978-1-66549-956-9},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\TBHJJEYC\Samani and Salehi - 2022 - Exploring the Impact of Virtualization on the Usab.pdf}
}

@online{slurmSlurmWorkloadManager2022,
  title = {Slurm {{Workload Manager}} - {{Documentation}}},
  author = {{SLURM}},
  date = {2022-10-05},
  url = {https://slurm.schedmd.com/documentation.html},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\KIRP34RT\documentation.html}
}

@inreference{TOP5002023,
  title = {{{TOP500}}},
  booktitle = {Wikipedia},
  date = {2023-10-13T21:54:25Z},
  url = {https://en.wikipedia.org/w/index.php?title=TOP500&oldid=1180007333},
  urldate = {2023-11-09},
  abstract = {The TOP500 project ranks and details the 500 most powerful non-distributed computer systems in the world. The project was started in 1993 and publishes an updated list of the supercomputers twice a year. The first of these updates always coincides with the International Supercomputing Conference in June, and the second is presented at the ACM/IEEE Supercomputing Conference in November. The project aims to provide a reliable basis for tracking and detecting trends in high-performance computing and bases rankings on HPL benchmarks, a portable implementation of the high-performance LINPACK benchmark written in Fortran for distributed-memory computers. The 60th TOP500 was published in November 2022. Since June 2022, the United States' Frontier is the most powerful supercomputer on TOP500, reaching 1102 petaFlops (1.102 exaFlops) on the LINPACK benchmarks. The United States has by far the highest share of total computing power on the list (nearly 50\%), while China currently leads the list in number of systems with 173 supercomputers, with the U.S. not far behind in second place.  The TOP500 list is compiled by Jack Dongarra of the University of Tennessee, Knoxville, Erich Strohmaier and Horst Simon of the National Energy Research Scientific Computing Center (NERSC) and Lawrence Berkeley National Laboratory (LBNL), and, until his death in 2014, Hans Meuer of the University of Mannheim, Germany. The TOP500 project also includes lists such as Green500 (measuring energy efficiency) and HPCG (measuring I/O bandwidth).},
  langid = {english},
  annotation = {Page Version ID: 1180007333},
  file = {C:\Users\test\Zotero\storage\P7QJ9WCK\TOP500.html}
}

@online{top500ListStatisticsTOP5002023,
  title = {List {{Statistics}} | {{TOP500}} | {{Interconnect Family}}},
  author = {{TOP500}},
  date = {2023-06},
  url = {https://www.top500.org/statistics/list/},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\GFS97WPE\list.html}
}

@inproceedings{yooSLURMSimpleLinux2003,
  title = {{{SLURM}}: {{Simple Linux Utility}} for {{Resource Management}}},
  shorttitle = {{{SLURM}}},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
  editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {44--60},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/10968987_3},
  abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
  isbn = {978-3-540-39727-4},
  langid = {english},
  keywords = {Exit Status,Lawrence Livermore National Laboratory,Message Authentication Code,Remote Execution,Resource Management System},
  file = {C:\Users\test\Zotero\storage\ALL7PQ3K\Yoo et al. - 2003 - SLURM Simple Linux Utility for Resource Managemen.pdf}
}
