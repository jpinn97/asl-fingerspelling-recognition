@inproceedings{abiyevReconstructionConvolutionalNeural2020,
  title = {Reconstruction of {{Convolutional Neural Network}} for {{Sign Language Recognition}}},
  booktitle = {2020 {{International Conference}} on {{Electrical}}, {{Communication}}, and {{Computer Engineering}} ({{ICECCE}})},
  author = {Abiyev, Rahib and Idoko, John Bush and Arslan, Murat},
  date = {2020-06},
  pages = {1--5},
  publisher = {{IEEE}},
  location = {{Istanbul, Turkey}},
  doi = {10.1109/ICECCE49384.2020.9179356},
  url = {https://ieeexplore.ieee.org/document/9179356/},
  urldate = {2023-11-29},
  abstract = {This paper presents a Sign Language translation model using Convolutional Neural Networks (CNN). A sign language is a language which allows mute and hearingimpaired people to communicate. It is a visually oriented, nonverbal communication which facilitates communication through body/facial postures, expressions and a collection of gestures. To contribute to the wellbeing of the affected population, we are motivated to implement a vision-based system to avert their day to day challenges. Our propose model constitutes object detection and classification phases. The first module is made up of single shot multi-box detector (SSD) used for hand detection. The second module constitutes convolutional neural network plus a fully connected network utilized to constructively translate the detected signs into text. The propose model is implemented using American sign language fingerspelling dataset. The propose system outperformed other published results in the comparative analysis, hence recommended for further exploitation in sign language recognition problems.},
  eventtitle = {2020 {{International Conference}} on {{Electrical}}, {{Communication}}, and {{Computer Engineering}} ({{ICECCE}})},
  isbn = {978-1-72817-116-6},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\UFTNKESY\Abiyev et al. - 2020 - Reconstruction of Convolutional Neural Network for.pdf}
}

@article{adeyanjuMachineLearningMethods2021,
  title = {Machine Learning Methods for Sign Language Recognition: {{A}} Critical Review and Analysis},
  shorttitle = {Machine Learning Methods for Sign Language Recognition},
  author = {Adeyanju, I.A. and Bello, O.O. and Adegboye, M.A.},
  date = {2021-11},
  journaltitle = {Intelligent Systems with Applications},
  shortjournal = {Intelligent Systems with Applications},
  volume = {12},
  pages = {200056},
  issn = {26673053},
  doi = {10.1016/j.iswa.2021.200056},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2667305321000454},
  urldate = {2023-11-29},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\WDP9WTXW\Adeyanju et al. - 2021 - Machine learning methods for sign language recogni.pdf}
}

@inproceedings{amdahlValiditySingleProcessor1967,
  title = {Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities},
  booktitle = {Proceedings of the {{April}} 18-20, 1967, Spring Joint Computer Conference on - {{AFIPS}} '67 ({{Spring}})},
  author = {Amdahl, Gene M.},
  date = {1967},
  pages = {483},
  publisher = {{ACM Press}},
  location = {{Atlantic City, New Jersey}},
  doi = {10.1145/1465482.1465560},
  url = {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
  urldate = {2023-11-15},
  eventtitle = {The {{April}} 18-20, 1967, Spring Joint Computer Conference},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\B4N2XF6M\Amdahl - 1967 - Validity of the single processor approach to achie.pdf}
}

@inproceedings{bantupalliAmericanSignLanguage2018,
  title = {American {{Sign Language Recognition}} Using {{Deep Learning}} and {{Computer Vision}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Bantupalli, Kshitij and Xie, Ying},
  date = {2018-12},
  pages = {4896--4899},
  doi = {10.1109/BigData.2018.8622141},
  url = {https://ieeexplore.ieee.org/document/8622141},
  urldate = {2023-11-29},
  abstract = {Speech impairment is a disability which affects an individuals ability to communicate using speech and hearing. People who are affected by this use other media of communication such as sign language. Although sign language is ubiquitous in recent times, there remains a challenge for non-sign language speakers to communicate with sign language speakers or signers. With recent advances in deep learning and computer vision there has been promising progress in the fields of motion and gesture recognition using deep learning and computer vision based techniques. The focus of this work is to create a visionbased application which offers sign language translation to text thus aiding communication between signers and non-signers. The proposed model takes video sequences and extracts temporal and spatial features from them. We then use Inception, a CNN (Convolutional Neural Network) for recognizing spatial features. We then use a RNN (Recurrent Neural Network) to train on temporal features. The dataset used is the American Sign Language Dataset.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  file = {C\:\\Users\\test\\Zotero\\storage\\DEKECKH3\\Bantupalli and Xie - 2018 - American Sign Language Recognition using Deep Lear.pdf;C\:\\Users\\test\\Zotero\\storage\\SHY89S3Q\\8622141.html}
}

@artwork{benedikt.seidlEnglishThisFigure2009,
  title = {English:  {{This}} Figure Shows the Operating Systems Used on the Supercomputers Listed on the {{Top500}} List. {{Data}} for This Figure Was Compiled from {{http://top500.org/stats}}},
  shorttitle = {English},
  author = {{Benedikt.Seidl}},
  date = {2009-02-12},
  url = {https://commons.wikimedia.org/wiki/File:Operating_systems_used_on_top_500_supercomputers.svg},
  urldate = {2023-11-09},
  file = {C:\Users\test\Zotero\storage\RUE4X2BS\FileOperating_systems_used_on_top_500_supercomputers.html}
}

@inreference{BeowulfCluster2023,
  title = {Beowulf Cluster},
  booktitle = {Wikipedia},
  date = {2023-09-29T15:18:42Z},
  url = {https://en.wikipedia.org/w/index.php?title=Beowulf_cluster&oldid=1177783482},
  urldate = {2023-11-10},
  abstract = {A Beowulf cluster is a computer cluster of what are normally identical, commodity-grade computers networked into a small local area network with libraries and programs installed which allow processing to be shared among them. The result is a high-performance parallel computing cluster from inexpensive personal computer hardware. The name Beowulf originally referred to a specific computer built in 1994 by Thomas Sterling and Donald Becker at NASA. The name "Beowulf" comes from the Old English epic poem of the same name.No particular piece of software defines a cluster as a Beowulf. Typically only free and open source software is used, both to save cost and to allow customization. Most Beowulf clusters run a Unix-like operating system, such as BSD, Linux, or Solaris. Commonly used parallel processing libraries include Message Passing Interface (MPI) and Parallel Virtual Machine (PVM). Both of these permit the programmer to divide a task among a group of networked computers, and collect the results of processing. Examples of MPI software include Open MPI  or MPICH. There are additional MPI implementations available. Beowulf systems operate worldwide, chiefly in support of scientific computing. Since 2017, every system on the Top500 list of the world's fastest supercomputers has used Beowulf software methods and a Linux operating system. At this level, however, most are by no means just assemblages of commodity hardware; custom design work is often required for the nodes (often blade servers), the networking and the cooling systems.},
  langid = {english},
  annotation = {Page Version ID: 1177783482},
  file = {C:\Users\test\Zotero\storage\DKLYYTIC\Beowulf_cluster.html}
}

@online{bradyQuickNoteGPU2023,
  title = {A {{Quick Note}} on {{GPU Accuracy}} and {{Double Precision}} | {{Blog}}},
  author = {Brady, Ryan},
  date = {2023-09},
  url = {https://www.experoinc.com//expero-resources/a-quick-note-on-gpu-accuracy-and-double-precision},
  urldate = {2023-11-10},
  abstract = {Single precision vs Double precision on a CPU vs GPU in high performance computing simulation.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\QW9P5SBQ\a-quick-note-on-gpu-accuracy-and-double-precision.html}
}

@online{brianWhatInfiniBandNetwork2021,
  title = {What Is {{InfiniBand Network}} and the {{Difference}} with {{Ethernet}}?},
  author = {Brian},
  date = {2021-12-01},
  url = {https://www.fibermall.com/blog/what-is-infiniband-network-and-difference-with-ethernet.htm},
  urldate = {2023-11-10},
  abstract = {What is the InfiniBand Network? The InfiniBand architecture brings fabric consolidation to the data center Storage networking can concurrently run~with cluster},
  langid = {american},
  organization = {{fibermall.com}},
  file = {C:\Users\test\Zotero\storage\GD6LZPVU\what-is-infiniband-network-and-difference-with-ethernet.html}
}

@article{chongAmericanSignLanguage2018,
  title = {American {{Sign Language Recognition Using Leap Motion Controller}} with {{Machine Learning Approach}}},
  author = {Chong, Teak-Wei and Lee, Boon-Giin},
  date = {2018-10},
  journaltitle = {Sensors},
  volume = {18},
  number = {10},
  pages = {3554},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s18103554},
  url = {https://www.mdpi.com/1424-8220/18/10/3554},
  urldate = {2023-11-29},
  abstract = {Sign language is intentionally designed to allow deaf and dumb communities to convey messages and to connect with society. Unfortunately, learning and practicing sign language is not common among society; hence, this study developed a sign language recognition prototype using the Leap Motion Controller (LMC). Many existing studies have proposed methods for incomplete sign language recognition, whereas this study aimed for full American Sign Language (ASL) recognition, which consists of 26 letters and 10 digits. Most of the ASL letters are static (no movement), but certain ASL letters are dynamic (they require certain movements). Thus, this study also aimed to extract features from finger and hand motions to differentiate between the static and dynamic gestures. The experimental results revealed that the sign language recognition rates for the 26 letters using a support vector machine (SVM) and a deep neural network (DNN) are 80.30\% and 93.81\%, respectively. Meanwhile, the recognition rates for a combination of 26 letters and 10 digits are slightly lower, approximately 72.79\% for the SVM and 88.79\% for the DNN. As a result, the sign language recognition system has great potential for reducing the gap between deaf and dumb communities and others. The proposed prototype could also serve as an interpreter for the deaf and dumb in everyday life in service sectors, such as at the bank or post office.},
  issue = {10},
  langid = {english},
  keywords = {American Sign Language,deep neural network,human-computer interaction,Leap Motion Controller,machine learning,multi-class classification,sign language recognition,support vector machine},
  file = {C:\Users\test\Zotero\storage\QGWQXCYL\Chong and Lee - 2018 - American Sign Language Recognition Using Leap Moti.pdf}
}

@inproceedings{cihancamgozSignLanguageTransformers2020,
  title = {Sign {{Language Transformers}}: {{Joint End-to-End Sign Language Recognition}} and {{Translation}}},
  shorttitle = {Sign {{Language Transformers}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cihan Camgoz, Necati and Koller, Oscar and Hadfield, Simon and Bowden, Richard},
  date = {2020-06},
  pages = {10020--10030},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01004},
  url = {https://ieeexplore.ieee.org/document/9156773/},
  urldate = {2023-11-29},
  abstract = {Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-theart in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-tosequence learning problems and leads to significant performance gains.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\MGNGYH32\Cihan Camgoz et al. - 2020 - Sign Language Transformers Joint End-to-End Sign .pdf}
}

@online{CrayClusterStorE10002023,
  title = {Cray {{ClusterStor E1000}} Storage System – {{System}} Overview},
  date = {2023},
  url = {https://www.hpe.com/psnow/doc/a50001954enw.pdf?jumpid=in_pdfviewer-psnow},
  urldate = {2023-11-15},
  abstract = {This technical white paper gives system architects in HPE, at HPE partners and at HPC customers/prospects a technical overview of the Cray ClusterStor E1000 storage system.},
  langid = {english},
  organization = {{PSNow}},
  file = {C:\Users\test\Zotero\storage\ZBIXJU5Z\a50001954enw.html}
}

@article{dennardDesignIonimplantedMOSFET1974,
  title = {Design of Ion-Implanted {{MOSFET}}'s with Very Small Physical Dimensions},
  author = {Dennard, R.H. and Gaensslen, F.H. and Yu, Hwa-Nien and Rideout, V.L. and Bassous, E. and LeBlanc, A.R.},
  date = {1974-10},
  journaltitle = {IEEE Journal of Solid-State Circuits},
  volume = {9},
  number = {5},
  pages = {256--268},
  issn = {1558-173X},
  doi = {10.1109/JSSC.1974.1050511},
  url = {https://ieeexplore.ieee.org/document/1050511},
  urldate = {2023-11-08},
  abstract = {This paper considers the design, fabrication, and characterization of very small Mosfet switching devices suitable for digital integrated circuits, using dimensions of the order of 1 /spl mu/. Scaling relationships are presented which show how a conventional MOSFET can be reduced in size. An improved small device structure is presented that uses ion implantation, to provide shallow source and drain regions and a nonuniform substrate doping profile. One-dimensional models are used to predict the substrate doping profile and the corresponding threshold voltage versus source voltage characteristic. A two-dimensional current transport model is used to predict the relative degree of short-channel effects for different device parameter combinations. Polysilicon-gate MOSFET's with channel lengths as short as 0.5 /spl mu/ were fabricated, and the device characteristics measured and compared with predicted values. The performance improvement expected from using these very small devices in highly miniaturized integrated circuits is projected.},
  eventtitle = {{{IEEE Journal}} of {{Solid-State Circuits}}},
  file = {C\:\\Users\\test\\Zotero\\storage\\34KWE724\\Dennard et al. - 1974 - Design of ion-implanted MOSFET's with very small p.pdf;C\:\\Users\\test\\Zotero\\storage\\ZRLA468P\\1050511.html}
}

@online{DevelopmentTimeTOP500,
  title = {Development over {{Time}} | {{TOP500}}},
  url = {https://www.top500.org/statistics/overtime/},
  urldate = {2023-11-08},
  file = {C:\Users\test\Zotero\storage\38M8P6JY\overtime.html}
}

@online{ethnologueAmericanSignLanguage2023,
  title = {American {{Sign Language}} | {{Ethnologue Free}}},
  author = {{Ethnologue}},
  date = {2023-02-21},
  url = {https://www.ethnologue.com/language/ase/},
  urldate = {2023-11-05},
  abstract = {American Sign Language is a stable indigenous language of the United States. It is a Deaf community sign language. The language is used as a first language by all in the ethnic community. It is not known to be taught in schools.},
  langid = {english},
  organization = {{Ethnologue (Free All)}},
  file = {C:\Users\test\Zotero\storage\H57QK6YX\ase.html}
}

@online{FileLustreFile2017,
  title = {File:{{Lustre File System Overview}} ({{DNE}}) Lowres v1.Png - {{Lustre Wiki}}},
  date = {2017-08-07},
  url = {https://wiki.lustre.org/File:Lustre_File_System_Overview_(DNE)_lowres_v1.png},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\IC5ALB8W\FileLustre_File_System_Overview_(DNE)_lowres_v1.html}
}

@article{frazelleChippingAwayMoore2020,
  title = {Chipping {{Away}} at {{Moore}}’s {{Law}}: {{Modern CPUs}} Are Just Chiplets Connected Together.},
  shorttitle = {Chipping {{Away}} at {{Moore}}’s {{Law}}},
  author = {Frazelle, Jessie},
  date = {2020-02-29},
  journaltitle = {Queue},
  shortjournal = {Queue},
  volume = {18},
  number = {1},
  pages = {5--15},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/3387945.3388515},
  url = {https://dl.acm.org/doi/10.1145/3387945.3388515},
  urldate = {2023-11-08},
  abstract = {Smaller transistors can do more calculations without overheating, which makes them more power efficient. It also allows for smaller die sizes, which reduce costs and can increase density, allowing more cores per chip. The silicon wafers that chips are made of vary in purity, and none are perfect, which means every chip has a chance of having imperfections that differ in effect. Manufacturers can limit the effect of imperfections by using chiplets.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\ZEDCWJ8W\Frazelle - 2020 - Chipping Away at Moore’s Law Modern CPUs are just.pdf}
}

@online{Frontier,
  title = {Frontier},
  url = {https://www.olcf.ornl.gov/frontier/#4},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\2BKYXE45\frontier.html}
}

@inproceedings{gajurelFineGrainedVisualAttention2021,
  title = {A {{Fine-Grained Visual Attention Approach}} for {{Fingerspelling Recognition}} in the {{Wild}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Gajurel, Kamala and Zhong, Cuncong and Wang, Guanghui},
  date = {2021-10-17},
  pages = {3266--3271},
  publisher = {{IEEE}},
  location = {{Melbourne, Australia}},
  doi = {10.1109/SMC52423.2021.9658982},
  url = {https://ieeexplore.ieee.org/document/9658982/},
  urldate = {2023-12-13},
  abstract = {Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  isbn = {978-1-66544-207-7},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\TSAAPSAB\Gajurel et al. - 2021 - A Fine-Grained Visual Attention Approach for Finge.pdf}
}

@inproceedings{gajurelFineGrainedVisualAttention2021a,
  title = {A {{Fine-Grained Visual Attention Approach}} for {{Fingerspelling Recognition}} in the {{Wild}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Gajurel, Kamala and Zhong, Cuncong and Wang, Guanghui},
  date = {2021-10-17},
  pages = {3266--3271},
  publisher = {{IEEE}},
  location = {{Melbourne, Australia}},
  doi = {10.1109/SMC52423.2021.9658982},
  url = {https://ieeexplore.ieee.org/document/9658982/},
  urldate = {2023-12-13},
  abstract = {Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  isbn = {978-1-66544-207-7},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\2ZCEMD5G\Gajurel et al. - 2021 - A Fine-Grained Visual Attention Approach for Finge.pdf}
}

@article{gillespieAmdahlLawGustafson2008,
  title = {Amdahl's {{Law}}, {{Gustafson}}'s {{Trend}}, and the {{Performance Limits}} of {{Parallel Applications}}},
  author = {Gillespie, Matt},
  date = {2008},
  abstract = {Parallelization is a core strategic-planning consideration for all software makers, and the amount of performance benefit available from parallelizing a given application (or part of an application) is a key aspect of setting performance goals for the parallelization process. Theoretical discussions of performance potential are necessarily the starting point for understanding the critical issues involved, before moving to practical issues associated with a given project. Amdahl's Law and its modification by Gustafson's trend give us the basic means to understand what's possible for a given application, and tools and best practices give us the means to decide how to use that information in practice.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\5IZPPURB\Gillespie - Amdahl's Law, Gustafson's Trend, and the Performan.pdf}
}

@online{guptaWhatDoublePrecisionTensor2020,
  title = {What {{Is}} a {{Double-Precision Tensor Core}}?},
  author = {Gupta, Geetika},
  date = {2020-05-14T12:59:55+00:00},
  url = {https://blogs.nvidia.com/blog/2020/05/14/double-precision-tensor-cores/},
  urldate = {2023-11-10},
  abstract = {A Double-Precision Tensor Core in the NVIDIA Ampere architecture speeds FP64 math for simulations and iterative solvers in high performance computing.},
  langid = {american},
  organization = {{NVIDIA Blog}},
  file = {C:\Users\test\Zotero\storage\JUBTVASE\double-precision-tensor-cores.html}
}

@online{hewlettpackardenterprisesHPCServiceAccelerate2023,
  title = {{{HPC}} as a Service to Accelerate Transformational Growth},
  author = {{Hewlett Packard Enterprises}},
  date = {2023},
  url = {https://www.hpe.com/psnow/doc/a50004225enw.pdf?jumpid=in_pdfviewer-psnow},
  urldate = {2023-11-15},
  abstract = {High-performance computing (HPC) offers enterprises better data analytics, simulations, and artificial intelligence. It enables them to be smarter, faster, and more competitive.},
  langid = {english},
  organization = {{PSNow}},
  file = {C:\Users\test\Zotero\storage\BSFIQQ4L\a50004225enw.html}
}

@online{HPEGreenLakeHigh,
  title = {{{HPE GreenLake}} for {{High Performance Computing}} - {{HPC}}},
  url = {https://www.hpe.com/us/en/hpe-greenlake-hpc.html},
  urldate = {2023-11-15},
  abstract = {HPE GreenLake for HPC allows you to make faster decisions \& reduce time to discovery without the burden of high-performance computing infrastructure management.},
  langid = {american},
  file = {C:\Users\test\Zotero\storage\4DHHPJ29\hpe-greenlake-hpc.html}
}

@online{IEEEXploreFullText,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7921010},
  urldate = {2023-11-14},
  file = {C:\Users\test\Zotero\storage\WBY9QAEI\stamp.html}
}

@online{IEEEXploreFullTexta,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8639639},
  urldate = {2023-11-29},
  file = {C:\Users\test\Zotero\storage\WLGFR2IH\stamp.html}
}

@online{IEEEXploreFullTextb,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9726736},
  urldate = {2023-11-29},
  file = {C:\Users\test\Zotero\storage\4I9XAQVX\stamp.html}
}

@online{IEEEXploreFullTextc,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9726736},
  urldate = {2023-11-29},
  file = {C:\Users\test\Zotero\storage\ZYI3M3MF\stamp.html}
}

@inproceedings{kabadeAmericanSignLanguage2023,
  title = {American {{Sign Language Fingerspelling Recognition}} Using {{Attention Model}}},
  booktitle = {2023 {{IEEE}} 8th {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
  author = {Kabade, Amruta E and Desai, Padmashree and C, Sujatha and G, Shankar},
  date = {2023-04},
  pages = {1--6},
  doi = {10.1109/I2CT57861.2023.10126277},
  url = {https://ieeexplore.ieee.org/document/10126277},
  urldate = {2023-11-29},
  abstract = {Sign Language Recognition(SLR) is a complex gesture recognition problem because of the quick and highly coarticulated motion involved in gestures. This research work focuses on Fingerspelling recognition task, which constitutes 35\% of the American Sign Language (ASL). Fingerspelling identifies the word letter by letter. Fingerspelling is used for signing the words which do not have designated ASL signs such as technical terms, content words and proper nouns. In our proposed work for ASL Fingerspelling recognition, we consider ChicagoFSWild dataset which consists of occlusions and images captured in varying illuminations, lighting conditions (in the wild environments). The optical flow is obtained from Lucas-Kanade algorithm, prior is generated, images are resized and cropped with face-roi technique to get the region of interest (ROI). The visual attention mechanism attends to the ROI iteratively. ResNet, pretrained on Imagenet is used for the extraction of spatial features. The Bi-LSTM network with Connectionist Temporal Classification (CTC) is used to predict the sign. It provides the accuracy of 57\% on ChicagoFSWild dataset for Fingerspelling recognition task.},
  eventtitle = {2023 {{IEEE}} 8th {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
  file = {C:\Users\test\Zotero\storage\E8II75TX\Kabade et al. - 2023 - American Sign Language Fingerspelling Recognition .pdf}
}

@online{leslieVirtualMachinePhysical2018,
  title = {Virtual {{Machine}} or {{Physical Server}}},
  author = {Leslie},
  date = {2018-01-08T02:02:26+00:00},
  url = {https://actusdigital.com/2018/01/08/virtual-machine-or-physical-server/},
  urldate = {2023-11-14},
  abstract = {We are more and more asked about VM (Virtual Machines). There are numerous advantages of VM over physical servers and there is a lot of buzz and many broadcasters are asking us to provide proposals based on VMs. But there are also some Myths involved.},
  langid = {american},
  organization = {{Actus Digital}}
}

@online{liOpenAIGPT3Language2020,
  title = {{{OpenAI}}'s {{GPT-3 Language Model}}: {{A Technical Overview}}},
  shorttitle = {{{OpenAI}}'s {{GPT-3 Language Model}}},
  author = {Li, Chuan},
  date = {2020-06-03},
  url = {https://lambdalabs.com/blog/demystifying-gpt-3},
  urldate = {2023-11-10},
  abstract = {Chuan Li, PhD reviews GPT-3, the new NLP model from OpenAI. The technical overview covers how GPT-3 was trained, GPT-2 vs. GPT-3, and GPT-3 performance.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\XHDH4ETX\demystifying-gpt-3.html}
}

@inproceedings{liPerformanceOverheadComparison2017,
  title = {Performance {{Overhead Comparison}} between {{Hypervisor}} and {{Container Based Virtualization}}},
  booktitle = {2017 {{IEEE}} 31st {{International Conference}} on {{Advanced Information Networking}} and {{Applications}} ({{AINA}})},
  author = {Li, Zheng and Kihl, Maria and Lu, Qinghua and Andersson, Jens A.},
  date = {2017-03},
  pages = {955--962},
  issn = {1550-445X},
  doi = {10.1109/AINA.2017.79},
  url = {https://ieeexplore.ieee.org/abstract/document/7921010?casa_token=3TsqiTCW5qUAAAAA:UDvJe9vhXgLWaJ49b2LDuIeEXNuROOnR9QM2TxKYkfeQEZJMkjhtzDG8XGy6W4SyK8EqfVX2Kbx_kA},
  urldate = {2023-11-14},
  abstract = {The current virtualization solution in the Cloud widely relies on hypervisor-based technologies. Along with the recent popularity of Docker, the container-based virtualization starts receiving more attention for being a promising alternative. Since both of the virtualization solutions are not resource-free, their performance overheads would lead to negative impacts on the quality of Cloud services. To help fundamentally understand the performance difference between these two types of virtualization solutions, we use a physical machine with “just-enough” resource as a baseline to investigate the performance overhead of a standalone Docker container against a standalone virtual machine (VM). With findings contrary to the related work, our evaluation results show that the virtualization's performance overhead could vary not only on a feature-by-feature basis but also on a job-to-job basis. Although the container-based solution is undoubtedly lightweight, the hypervisor-based technology does not come with higher performance overhead in every case. For example, Docker containers particularly exhibit lower QoS in terms of storage transaction speed.},
  eventtitle = {2017 {{IEEE}} 31st {{International Conference}} on {{Advanced Information Networking}} and {{Applications}} ({{AINA}})},
  file = {C\:\\Users\\test\\Zotero\\storage\\GVNXBX8Z\\Li et al. - 2017 - Performance Overhead Comparison between Hypervisor.pdf;C\:\\Users\\test\\Zotero\\storage\\7HYGCPI4\\7921010.html}
}

@online{Lustrea,
  title = {Lustre},
  url = {https://www.lustre.org/},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\PU6RYNGE\www.lustre.org.html}
}

@article{mitchellHowManyPeople2006,
  title = {How {{Many People Use ASL}} in the {{United States}}?: {{Why Estimates Need Updating}}},
  shorttitle = {How {{Many People Use ASL}} in the {{United States}}?},
  author = {Mitchell, Ross E. and Young, Travas A. and Bachelda, Bellamie and Karchmer, Michael A.},
  date = {2006},
  journaltitle = {Sign Language Studies},
  volume = {6},
  number = {3},
  eprint = {26190621},
  eprinttype = {jstor},
  pages = {306--335},
  publisher = {{Gallaudet University Press}},
  issn = {0302-1475},
  url = {https://www.jstor.org/stable/26190621},
  urldate = {2023-11-05},
  abstract = {This article traces the sources of the estimates of the number of American Sign Language users in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all of the data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates nor any specific study of ASL use. The article concludes with a call to action to rectify this problem.},
  file = {C:\Users\test\Zotero\storage\RC447MA5\Mitchell et al. - 2006 - How Many People Use ASL in the United States Why.pdf}
}

@article{mooreCrammingMoreComponents1965,
  title = {Cramming More Components onto Integrated Circuits},
  author = {Moore, Gordon E},
  date = {1965},
  volume = {38},
  number = {8},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\VPC7SHDD\Moore - 1965 - Cramming more components onto integrated circuits.pdf}
}

@online{moorinsightsHPEGreenLakeHPC2022,
  title = {{{HPE GreenLake}} for {{HPC}} Delivers and {{Innovative}} and {{Flexible HPC}}/{{AI Solution}}, by {{Moor Insights}}},
  author = {{Moor Insights}},
  date = {2022-06},
  url = {https://www.hpe.com/psnow/doc/a00124727enw},
  urldate = {2023-11-15},
  abstract = {The whitepaper authored by Moor Insights and funded by HPE-NVIDIA covers HPE GreenLake for HPC and examines the environment, industry trends, and types of service providers to determine the best method to run the HPC for AI workloads.},
  langid = {english},
  organization = {{PSNow}},
  file = {C:\Users\test\Zotero\storage\HZR4E86S\a00124727enw.html}
}

@article{munibAmericanSignLanguage2007,
  title = {American Sign Language ({{ASL}}) Recognition Based on {{Hough}} Transform and Neural Networks},
  author = {Munib, Qutaishat and Habeeb, Moussa and Takruri, Bayan and Al-Malik, Hiba Abed},
  date = {2007-01-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {32},
  number = {1},
  pages = {24--37},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2005.11.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417405003040},
  urldate = {2023-12-15},
  abstract = {The work presented in this paper aims to develop a system for automatic translation of static gestures of alphabets and signs in American sign language. In doing so, we have used Hough transform and neural networks which is trained to recognize signs. Our system does not rely on using any gloves or visual markings to achieve the recognition task. Instead, it deals with images of bare hands, which allows the user to interact with the system in a natural way. An image is processed and converted to a feature vector that will be compared with the feature vectors of a training set of signs. The extracted features are not affected by the rotation, scaling or translation of the gesture within the image, which makes the system more flexible. The system was implemented and tested using a data set of 300 samples of hand sign images; 15 images for each sign. Experiments revealed that our system was able to recognize selected ASL signs with an accuracy of 92.3\%.},
  keywords = {American sign language,Canny edge detection,Feature extraction,Hough transform,Neural network,Sobel edge detection},
  file = {C:\Users\test\Zotero\storage\3E2IXIHD\S0957417405003040.html}
}

@article{munibAmericanSignLanguage2007a,
  title = {American Sign Language ({{ASL}}) Recognition Based on {{Hough}} Transform and Neural Networks},
  author = {Munib, Qutaishat and Habeeb, Moussa and Takruri, Bayan and Al-Malik, Hiba Abed},
  date = {2007-01-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {32},
  number = {1},
  pages = {24--37},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2005.11.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417405003040},
  urldate = {2023-12-15},
  abstract = {The work presented in this paper aims to develop a system for automatic translation of static gestures of alphabets and signs in American sign language. In doing so, we have used Hough transform and neural networks which is trained to recognize signs. Our system does not rely on using any gloves or visual markings to achieve the recognition task. Instead, it deals with images of bare hands, which allows the user to interact with the system in a natural way. An image is processed and converted to a feature vector that will be compared with the feature vectors of a training set of signs. The extracted features are not affected by the rotation, scaling or translation of the gesture within the image, which makes the system more flexible. The system was implemented and tested using a data set of 300 samples of hand sign images; 15 images for each sign. Experiments revealed that our system was able to recognize selected ASL signs with an accuracy of 92.3\%.},
  keywords = {American sign language,Canny edge detection,Feature extraction,Hough transform,Neural network,Sobel edge detection},
  file = {C:\Users\test\Zotero\storage\QFCQLXEU\S0957417405003040.html}
}

@article{munibAmericanSignLanguage2007b,
  title = {American Sign Language ({{ASL}}) Recognition Based on {{Hough}} Transform and Neural Networks},
  author = {Munib, Qutaishat and Habeeb, Moussa and Takruri, Bayan and Al-Malik, Hiba Abed},
  date = {2007-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {32},
  number = {1},
  pages = {24--37},
  issn = {09574174},
  doi = {10.1016/j.eswa.2005.11.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417405003040},
  urldate = {2023-12-15},
  abstract = {The work presented in this paper aims to develop a system for automatic translation of static gestures of alphabets and signs in American sign language. In doing so, we have used Hough transform and neural networks which is trained to recognize signs. Our system does not rely on using any gloves or visual markings to achieve the recognition task. Instead, it deals with images of bare hands, which allows the user to interact with the system in a natural way. An image is processed and converted to a feature vector that will be compared with the feature vectors of a training set of signs. The extracted features are not affected by the rotation, scaling or translation of the gesture within the image, which makes the system more flexible.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\HQR6UEYU\Munib et al. - 2007 - American sign language (ASL) recognition based on .pdf}
}

@inproceedings{nguyenDeepLearningAmerican2019,
  title = {Deep {{Learning}} for {{American Sign Language Fingerspelling Recognition System}}},
  booktitle = {2019 26th {{International Conference}} on {{Telecommunications}} ({{ICT}})},
  author = {Nguyen, Huy B.D and Do, Hung Ngoc},
  date = {2019-04},
  pages = {314--318},
  publisher = {{IEEE}},
  location = {{Hanoi, Vietnam}},
  doi = {10.1109/ICT.2019.8798856},
  url = {https://ieeexplore.ieee.org/document/8798856/},
  urldate = {2023-11-29},
  abstract = {Sign language has always been a major tool for communication among people with disabilities. In this paper, a sign language fingerspelling alphabet identification system would be developed by using image processing technique, supervised machine learning and deep learning. In particular, 24 alphabetical symbols are presented by several combinations of static gestures (excluding 2 motion gestures J and Z). Histogram of Oriented Gradients (HOG) and Local Binary Pattern (LBP) features of each gesture will be extracted from training images. Then Multiclass Support Vector Machines (SVMs) will be applied to train these extracted data. Also, an end-to-end Convolutional Neural Network (CNN) architecture will be applied to the training dataset for comparison. After that, a further combination of CNN as feature descriptor and SVM produces an acceptable result. The Massey Dataset is implemented in the training and testing phases of the whole system.},
  eventtitle = {2019 26th {{International Conference}} on {{Telecommunications}} ({{ICT}})},
  isbn = {978-1-72810-273-3},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\ZCNRKPQK\Nguyen and Do - 2019 - Deep Learning for American Sign Language Fingerspe.pdf}
}

@online{OpenMPI2023,
  title = {Open {{MPI}}:},
  date = {2023-10},
  url = {https://www.open-mpi.org/},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\RWVVZSSN\www.open-mpi.org.html}
}

@online{opensfsLustre2023,
  title = {Lustre},
  author = {{OpenSFS} and {EOFS}},
  date = {2023-11},
  url = {https://www.lustre.org/},
  urldate = {2023-11-10},
  organization = {{https://www.lustre.org/}},
  file = {C:\Users\test\Zotero\storage\V5XI4EVC\www.lustre.org.html}
}

@article{ozAmericanSignLanguage2011,
  title = {American {{Sign Language}} Word Recognition with a Sensory Glove Using Artificial Neural Networks},
  author = {Oz, Cemil and Leu, Ming C.},
  date = {2011-10-01},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  series = {Infrastructures and {{Tools}} for {{Multiagent Systems}}},
  volume = {24},
  number = {7},
  pages = {1204--1213},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2011.06.015},
  url = {https://www.sciencedirect.com/science/article/pii/S0952197611001230},
  urldate = {2023-12-15},
  abstract = {An American Sign Language (ASL) recognition system is being developed using artificial neural networks (ANNs) to translate ASL words into English. The system uses a sensory glove called the Cyberglove™ and a Flock of Birds® 3-D motion tracker to extract the gesture features. The data regarding finger joint angles obtained from strain gauges in the sensory glove define the hand shape, while the data from the tracker describe the trajectory of hand movements. The data from these devices are processed by a velocity network with noise reduction and feature extraction and by a word recognition network. Some global and local features are extracted for each ASL word. A neural network is used as a classifier of this feature vector. Our goal is to continuously recognize ASL signs using these devices in real time. We trained and tested the ANN model for 50 ASL words with a different number of samples for every word. The test results show that our feature vector extraction method and neural networks can be used successfully for isolated word recognition. This system is flexible and open for future extension.},
  keywords = {American Sign Language (ASL),Artificial Neural Networks (ANNs),ASL recognition,Finger spelling recognition,Hand-shape recognition},
  file = {C:\Users\test\Zotero\storage\35EI6IGW\S0952197611001230.html}
}

@article{patelEnergyEfficientStrategy2020,
  title = {Energy Efficient Strategy for Placement of Virtual Machines Selected from Underloaded Servers in Compute {{Cloud}}},
  author = {Patel, Nimisha and Patel, Hiren},
  date = {2020-07-01},
  journaltitle = {Journal of King Saud University - Computer and Information Sciences},
  shortjournal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {32},
  number = {6},
  pages = {700--708},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2017.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1319157817302288},
  urldate = {2023-11-14},
  abstract = {Workload consolidation is a phase in Cloud datacenter where tasks are allocated among the available hosts in such a way that a minimal number of hosts is used and users’ need in terms of service level agreement (SLA) is fulfilled. To achieve workload consolidation, hosts are divided among three groups based on their utilization namely overloaded hosts, underloaded host and normal hosts. Detection of over or underloaded host is a challenging issue. Most of the existing researchers propose to use threshold values for such detection. We believe that there is a scope of improvement in existing methods of deciding underloaded hosts and subsequently taking off virtual machines (VMs) from them and placing them on other hosts. In this research, we propose Host Utilization Aware (HUA) Algorithm for underloaded host detection and placing its VMs on other hosts in a dynamic Cloud environment. We compare our proposed mechanism with existing one and with empirical analysis; it is shown that our proposal results into shutting off more number of hosts without compromising user’s workload requirement which leads to an energy-efficient workload consolidation with minimal migration costs and efficient utilization of active hosts.},
  keywords = {Energy efficiency,Underloaded server,Utilization,VM placement,Workload consolidation},
  file = {C\:\\Users\\test\\Zotero\\storage\\E9DI2WK3\\Patel and Patel - 2020 - Energy efficient strategy for placement of virtual.pdf;C\:\\Users\\test\\Zotero\\storage\\XX86G9SF\\S1319157817302288.html}
}

@online{PVMParallelVirtual,
  title = {{{PVM}}: {{Parallel Virtual Machine}}},
  url = {https://www.epm.ornl.gov/pvm/pvm_home.html},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\5NPHEJE8\pvm_home.html}
}

@article{rastgooSignLanguageRecognition2021,
  title = {Sign {{Language Recognition}}: {{A Deep Survey}}},
  shorttitle = {Sign {{Language Recognition}}},
  author = {Rastgoo, Razieh and Kiani, Kourosh and Escalera, Sergio},
  date = {2021-02},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {164},
  pages = {113794},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.113794},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S095741742030614X},
  urldate = {2023-11-29},
  abstract = {Sign language, as a different form of the communication language, is important to large groups of people in society. There are different signs in each sign language with variability in hand shape, motion profile, and position of the hand, face, and body parts contributing to each sign. So, visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the visionbased proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved. We present a taxonomy to categorize the proposed models for isolated and continuous sign language recognition, discussing applications, datasets, hybrid models, complexity, and future lines of research in the field.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\NXZMWM3W\Rastgoo et al. - 2021 - Sign Language Recognition A Deep Survey.pdf}
}

@software{ruppMicroprocessorTrendData2023,
  title = {Microprocessor {{Trend Data}}},
  author = {Rupp, Karl},
  date = {2023-11-07T21:08:28Z},
  origdate = {2018-02-15T05:10:17Z},
  url = {https://github.com/karlrupp/microprocessor-trend-data},
  urldate = {2023-11-08},
  abstract = {Data repository for my blog series on microprocessor trend data.}
}

@article{saeedSystematicReviewSystemsBased2022,
  title = {A {{Systematic Review}} on {{Systems-Based Sensory Gloves}} for {{Sign Language Pattern Recognition}}: {{An Update From}} 2017 to 2022},
  shorttitle = {A {{Systematic Review}} on {{Systems-Based Sensory Gloves}} for {{Sign Language Pattern Recognition}}},
  author = {Saeed, Zinah Raad and Zainol, Zurinahni Binti and Zaidan, B. B. and Alamoodi, A. H.},
  date = {2022},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {10},
  pages = {123358--123377},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3219430},
  url = {https://ieeexplore.ieee.org/document/9938436/},
  urldate = {2023-12-15},
  abstract = {Sign language is the predominant mode of communication for the Hearing impaired community. For the millions of people who suffer from hearing loss around the world, interaction with people who have the ability to hear and do not suffer from hearing impairment or loss is considered as complicated. In line with this issue, technology is perceived as a crucial factor in being an enabler of providing solutions to enhance the quality of life of the hearing impairment by increasing accessibility. This research aims to review and analyze articles related to sign language recognition based on the sensor- based glove system, in order to identify academic motivations, challenges, and recommendations related to this field. The search for the relevant review materials and articles was performed on four major databases ranging from 2017 to 2022: Science Direct, Web of Science, IEEE Xplore, and Scopus. The articles were chosen based on our inclusion and exclusion criteria. The literature findings of this paper indicate the dataset size to be open issues and challenges for hand gesture recognition. Furthermore, the majority of research on sign language recognition based on data glove was performed on static, single hand, and isolated gestures. Moreover, recognition accuracy typically achieved results higher than 90\%. However, most experiments were carried out with a limited number of gestures. Overall, it is hoped that this study will serve as a roadmap for future research and raise awareness among researchers in the field of sign language recognition.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\4JVIXX4C\Saeed et al. - 2022 - A Systematic Review on Systems-Based Sensory Glove.pdf}
}

@article{saeedSystematicReviewSystemsBased2022a,
  title = {A {{Systematic Review}} on {{Systems-Based Sensory Gloves}} for {{Sign Language Pattern Recognition}}: {{An Update From}} 2017 to 2022},
  shorttitle = {A {{Systematic Review}} on {{Systems-Based Sensory Gloves}} for {{Sign Language Pattern Recognition}}},
  author = {Saeed, Zinah Raad and Zainol, Zurinahni Binti and Zaidan, B. B. and Alamoodi, A. H.},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {123358--123377},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3219430},
  url = {https://ieeexplore.ieee.org/document/9938436},
  urldate = {2023-12-15},
  abstract = {Sign language is the predominant mode of communication for the Hearing impaired community. For the millions of people who suffer from hearing loss around the world, interaction with people who have the ability to hear and do not suffer from hearing impairment or loss is considered as complicated. In line with this issue, technology is perceived as a crucial factor in being an enabler of providing solutions to enhance the quality of life of the hearing impairment by increasing accessibility. This research aims to review and analyze articles related to sign language recognition based on the sensor- based glove system, in order to identify academic motivations, challenges, and recommendations related to this field. The search for the relevant review materials and articles was performed on four major databases ranging from 2017 to 2022: Science Direct, Web of Science, IEEE Xplore, and Scopus. The articles were chosen based on our inclusion and exclusion criteria. The literature findings of this paper indicate the dataset size to be open issues and challenges for hand gesture recognition. Furthermore, the majority of research on sign language recognition based on data glove was performed on static, single hand, and isolated gestures. Moreover, recognition accuracy typically achieved results higher than 90\%. However, most experiments were carried out with a limited number of gestures. Overall, it is hoped that this study will serve as a roadmap for future research and raise awareness among researchers in the field of sign language recognition.},
  eventtitle = {{{IEEE Access}}},
  file = {C\:\\Users\\test\\Zotero\\storage\\MGKXJ7M5\\Saeed et al. - 2022 - A Systematic Review on Systems-Based Sensory Glove.pdf;C\:\\Users\\test\\Zotero\\storage\\QKE99YEN\\9938436.html}
}

@inproceedings{samaniExploringImpactVirtualization2022,
  title = {Exploring the {{Impact}} of {{Virtualization}} on the {{Usability}} of {{Deep Learning Applications}}},
  author = {Samani, Davood G. and Salehi, Mohsen Amini},
  date = {2022-05-01},
  pages = {442--451},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/CCGrid54584.2022.00054},
  url = {https://www.computer.org/csdl/proceedings-article/ccgrid/2022/995600a442/1F8zeX3BAQM},
  urldate = {2023-11-14},
  abstract = {Deep Learning-based (DL) applications are becoming increasingly popular and advancing at an unprecedented pace. While many research works are being undertaken to enhance Deep Neural Networks (DNN)-the centerpiece of DL applications-practical deployment challenges of these applications in the Cloud and Edge systems, and their impact on the usability of the applications have not been sufficiently investigated. In particular, the impact of deploying different virtualization platforms, offered by the Cloud and Edge, on the usability of DL applications (in terms of the End-to-End (E2E) inference time) has remained an open question. Importantly, resource elasticity (by means of scale-up), CPU pinning, and processor type (CPU vs GPU) configurations have shown to be influential on the virtualization overhead. Accordingly, the goal of this research is to study the impact of these potentially decisive deployment options on the E2E performance, thus, usability of the DL applications. To that end, we measure the impact of four popular execution platforms (namely, bare-metal, virtual machine (VM), container, and container in VM) on the E2E inference time of four types of DL applications, upon changing processor configuration (scale-up, CPU pinning) and processor types. This study reveals a set of interesting and sometimes counter-intuitive findings that can be used as best practices by Cloud solution architects to efficiently deploy DL applications in various systems. The notable finding is that the solution architects must be aware of the DL application characteristics, particularly, their pre- and post-processing requirements, to be able to optimally choose and configure an execution platform, determine the use of GPU, and decide the efficient scale-up range.},
  eventtitle = {2022 22nd {{International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet Computing}} ({{CCGrid}})},
  isbn = {978-1-66549-956-9},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\TBHJJEYC\Samani and Salehi - 2022 - Exploring the Impact of Virtualization on the Usab.pdf}
}

@inproceedings{shahriarRealTimeAmericanSign2018,
  title = {Real-{{Time American Sign Language Recognition Using Skin Segmentation}} and {{Image Category Classification}} with {{Convolutional Neural Network}} and {{Deep Learning}}},
  booktitle = {{{TENCON}} 2018 - 2018 {{IEEE Region}} 10 {{Conference}}},
  author = {Shahriar, Shadman and Siddiquee, Ashraf and Islam, Tanveerul and Ghosh, Abesh and Chakraborty, Rajat and Khan, Asir Intisar and Shahnaz, Celia and Fattah, Shaikh Anowarul},
  date = {2018-10},
  pages = {1168--1171},
  publisher = {{IEEE}},
  location = {{Jeju, Korea (South)}},
  doi = {10.1109/TENCON.2018.8650524},
  url = {https://ieeexplore.ieee.org/document/8650524/},
  urldate = {2023-11-29},
  abstract = {A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on skin segmentation and machine learning algorithms. We present an automatic human skin segmentation algorithm based on color information. The YCbCr color space is employed because it is typically used in video coding and provides an effective use of chrominance information for modeling the human skin color. We model the skin-color distribution as a bivariate normal distribution in the CbCr plane. The performance of the algorithm is illustrated by simulations carried out on images depicting people of different ethnicity. Then Convolutional Neural Network (CNN) is used to extract features from the images and Deep Learning Method is used to train a classifier to recognize Sign Language.},
  eventtitle = {{{TENCON}} 2018 - 2018 {{IEEE Region}} 10 {{Conference}}},
  isbn = {978-1-5386-5457-6},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\YXC94JH6\Shahriar et al. - 2018 - Real-Time American Sign Language Recognition Using.pdf}
}

@inproceedings{shiAmericanSignLanguage2018,
  title = {American {{Sign Language Fingerspelling Recognition}} in the {{Wild}}},
  booktitle = {2018 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Shi, Bowen and Del Rio, Aurora Martinez and Keane, Jonathan and Michaux, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  date = {2018-12},
  pages = {145--152},
  doi = {10.1109/SLT.2018.8639639},
  url = {https://ieeexplore.ieee.org/abstract/document/8639639},
  urldate = {2023-11-29},
  abstract = {We address the problem of American Sign Language fingerspelling recognition “in the wild”, using videos collected from websites. We introduce the largest data set available so far for the problem of fingerspelling recognition, and the first using naturally occurring video data. Using this data set, we present the first attempt to recognize fingerspelling sequences in this challenging setting. Unlike prior work, our video data is extremely challenging due to low frame rates and visual variability. To tackle the visual challenges, we train a special-purpose signing hand detector using a small subset of our data. Given the hand detector output, a sequence model decodes the hypothesized fingerspelled letter sequence. For the sequence model, we explore attention-based recurrent encoder-decoders and CTC-based approaches. As the first attempt at fingerspelling recognition in the wild, this work is intended to serve as a baseline for future work on sign language recognition in realistic conditions. We find that, as expected, letter error rates are much higher than in previous work on more controlled data, and we analyze the sources of error and effects of model variants.},
  eventtitle = {2018 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  file = {C\:\\Users\\test\\Zotero\\storage\\M9RBGXG2\\Shi et al. - 2018 - American Sign Language Fingerspelling Recognition .pdf;C\:\\Users\\test\\Zotero\\storage\\CE65XA5S\\8639639.html}
}

@inproceedings{shiFingerspellingDetectionAmerican2021,
  title = {Fingerspelling {{Detection}} in {{American Sign Language}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shi, Bowen and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  date = {2021-06},
  pages = {4164--4173},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00415},
  url = {https://ieeexplore.ieee.org/document/9578297/},
  urldate = {2023-11-29},
  abstract = {Fingerspelling, in which words are signed letter by letter, is an important component of American Sign Language. Most previous work on automatic fingerspelling recognition has assumed that the boundaries of fingerspelling regions in signing videos are known beforehand. In this paper, we consider the task of fingerspelling detection in raw, untrimmed sign language videos. This is an important step towards building real-world fingerspelling recognition systems. We propose a benchmark and a suite of evaluation metrics, some of which reflect the effect of detection on the downstream fingerspelling recognition task. In addition, we propose a new model that learns to detect fingerspelling via multi-task training, incorporating pose estimation and fingerspelling recognition (transcription) along with detection, and compare this model to several alternatives. The model outperforms all alternative approaches across all metrics, establishing a state of the art on the benchmark.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\3DXLW7AJ\Shi et al. - 2021 - Fingerspelling Detection in American Sign Language.pdf}
}

@inproceedings{shiFingerspellingRecognitionWild2019,
  title = {Fingerspelling {{Recognition}} in the {{Wild With Iterative Visual Attention}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Shi, Bowen and Rio, Aurora Martinez Del and Keane, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  date = {2019-10},
  pages = {5399--5408},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00550},
  url = {https://ieeexplore.ieee.org/document/9010036/},
  urldate = {2023-11-29},
  abstract = {Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It outperforms prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\VNP3293B\Shi et al. - 2019 - Fingerspelling Recognition in the Wild With Iterat.pdf}
}

@online{shiSearchingFingerspelledContent2022,
  title = {Searching for Fingerspelled Content in {{American Sign Language}}},
  author = {Shi, Bowen and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  date = {2022-03-24},
  eprint = {2203.13291},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.13291},
  urldate = {2023-12-13},
  abstract = {Natural language processing for sign language video - including tasks like recognition, translation, and search - is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years. In this paper, we address the problem of searching for fingerspelled key-words or key phrases in raw sign language videos. This is an important task since significant content in sign language is often conveyed via fingerspelling, and to our knowledge the task has not been studied before. We propose an end-to-end model for this task, FSS-Net, that jointly detects fingerspelling and matches it to a text sequence. Our experiments, done on a large public dataset of ASL fingerspelling in the wild, show the importance of fingerspelling detection as a component of a search and retrieval model. Our model significantly outperforms baseline methods adapted from prior work on related tasks},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\test\\Zotero\\storage\\7EF3XGGJ\\Shi et al. - 2022 - Searching for fingerspelled content in American Si.pdf;C\:\\Users\\test\\Zotero\\storage\\D4BVWXQD\\2203.html}
}

@online{Sign,
  title = {Sign In},
  url = {https://id.elsevier.com/as/authorization.oauth2?platSite=SD%2Fscience&scope=openid%20email%20profile%20els_auth_info%20els_idp_info%20els_idp_analytics_attrs%20els_sa_discover%20urn%3Acom%3Aelsevier%3Aidp%3Apolicy%3Aproduct%3Ainst_assoc&response_type=code&redirect_uri=https%3A%2F%2Fwww.sciencedirect.com%2Fuser%2Fidentity%2Flanding&authType=SINGLE_SIGN_IN&prompt=login&client_id=SDFE-v4&state=retryCounter%3D0%26csrfToken%3Dd7a80a4a-6ca8-45bc-8303-340959f9df35%26idpPolicy%3Durn%253Acom%253Aelsevier%253Aidp%253Apolicy%253Aproduct%253Ainst_assoc%26returnUrl%3Dhttps%253A%252F%252Fwww.sciencedirect.com%252Fscience%252Farticle%252Fabs%252Fpii%252FS0957417405003040%253Fcasa_token%253D0_g0QboxS4YAAAAA%253AEdrSD8DtFkRI0Dd3o6S_PF8gwIEIyEYC6_Fk4oxupLe5J6rlJDwzgaOpmYdwv0WXHJx7-5WKfw%26prompt%3Dlogin%26cid%3Datp-4f11b9b9-774f-41fa-bb15-80e693d64acf},
  urldate = {2023-12-15},
  file = {C:\Users\test\Zotero\storage\I7ZIHY7P\authorization.html}
}

@inproceedings{skumarTimeSeriesNeural2018,
  title = {Time {{Series Neural Networks}} for {{Real Time Sign Language Translation}}},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {S Kumar, Sujay and Wangyal, Tenzin and Saboo, Varun and Srinath, Ramamoorthy},
  date = {2018-12},
  pages = {243--248},
  publisher = {{IEEE}},
  location = {{Orlando, FL}},
  doi = {10.1109/ICMLA.2018.00043},
  url = {https://ieeexplore.ieee.org/document/8614068/},
  urldate = {2023-11-29},
  abstract = {Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.},
  eventtitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  isbn = {978-1-5386-6805-4},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\JPEHN7HA\S Kumar et al. - 2018 - Time Series Neural Networks for Real Time Sign Lan.pdf}
}

@online{slurmSlurmWorkloadManager2022,
  title = {Slurm {{Workload Manager}} - {{Documentation}}},
  author = {{SLURM}},
  date = {2022-10-05},
  url = {https://slurm.schedmd.com/documentation.html},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\KIRP34RT\documentation.html}
}

@article{starnerRealtimeAmericanSign1998,
  title = {Real-Time {{American}} Sign Language Recognition Using Desk and Wearable Computer Based Video},
  author = {Starner, T. and Weaver, J. and Pentland, A.},
  date = {1998-12},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {12},
  pages = {1371--1375},
  issn = {1939-3539},
  doi = {10.1109/34.735811},
  url = {https://ieeexplore.ieee.org/document/735811},
  urldate = {2023-12-15},
  abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {C:\Users\test\Zotero\storage\SL3X45CK\735811.html}
}

@article{starnerRealtimeAmericanSign1998a,
  title = {Real-Time {{American}} Sign Language Recognition Using Desk and Wearable Computer Based Video},
  author = {Starner, T. and Weaver, J. and Pentland, A.},
  date = {1998-12},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {12},
  pages = {1371--1375},
  issn = {1939-3539},
  doi = {10.1109/34.735811},
  url = {https://ieeexplore.ieee.org/document/735811},
  urldate = {2023-12-15},
  abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {C\:\\Users\\test\\Zotero\\storage\\9ITYQU23\\Starner et al. - 1998 - Real-time American sign language recognition using.pdf;C\:\\Users\\test\\Zotero\\storage\\W6XC44TD\\735811.html}
}

@inreference{TOP5002023,
  title = {{{TOP500}}},
  booktitle = {Wikipedia},
  date = {2023-10-13T21:54:25Z},
  url = {https://en.wikipedia.org/w/index.php?title=TOP500&oldid=1180007333},
  urldate = {2023-11-09},
  abstract = {The TOP500 project ranks and details the 500 most powerful non-distributed computer systems in the world. The project was started in 1993 and publishes an updated list of the supercomputers twice a year. The first of these updates always coincides with the International Supercomputing Conference in June, and the second is presented at the ACM/IEEE Supercomputing Conference in November. The project aims to provide a reliable basis for tracking and detecting trends in high-performance computing and bases rankings on HPL benchmarks, a portable implementation of the high-performance LINPACK benchmark written in Fortran for distributed-memory computers. The 60th TOP500 was published in November 2022. Since June 2022, the United States' Frontier is the most powerful supercomputer on TOP500, reaching 1102 petaFlops (1.102 exaFlops) on the LINPACK benchmarks. The United States has by far the highest share of total computing power on the list (nearly 50\%), while China currently leads the list in number of systems with 173 supercomputers, with the U.S. not far behind in second place.  The TOP500 list is compiled by Jack Dongarra of the University of Tennessee, Knoxville, Erich Strohmaier and Horst Simon of the National Energy Research Scientific Computing Center (NERSC) and Lawrence Berkeley National Laboratory (LBNL), and, until his death in 2014, Hans Meuer of the University of Mannheim, Germany. The TOP500 project also includes lists such as Green500 (measuring energy efficiency) and HPCG (measuring I/O bandwidth).},
  langid = {english},
  annotation = {Page Version ID: 1180007333},
  file = {C:\Users\test\Zotero\storage\P7QJ9WCK\TOP500.html}
}

@online{top500ListStatisticsTOP5002023,
  title = {List {{Statistics}} | {{TOP500}} | {{Interconnect Family}}},
  author = {{TOP500}},
  date = {2023-06},
  url = {https://www.top500.org/statistics/list/},
  urldate = {2023-11-10},
  file = {C:\Users\test\Zotero\storage\GFS97WPE\list.html}
}

@inproceedings{voglerParallelHiddenMarkov1999,
  title = {Parallel Hidden {{Markov}} Models for {{American}} Sign Language Recognition},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Vogler, C. and Metaxas, D.},
  date = {1999-09},
  volume = {1},
  pages = {116-122 vol.1},
  doi = {10.1109/ICCV.1999.791206},
  url = {https://ieeexplore.ieee.org/document/791206},
  urldate = {2023-12-15},
  abstract = {The major challenge that faces American Sign Language (ASL) recognition now is to develop methods that will scale well with increasing vocabulary size. Unlike in spoken languages, phonemes can occur simultaneously in ASL. The number of possible combinations of phonemes after enforcing linguistic constraints is approximately 5.5/spl times/10/sup 8/. Gesture recognition, which is less constrained than ASL recognition, suffers from the same problem. Thus, it is not feasible to train conventional hidden Markov models (HMMs) for large-scab ASL applications. Factorial HMMs and coupled HMMs are two extensions to HMMs that explicitly attempt to model several processes occuring in parallel. Unfortunately, they still require consideration of the combinations at training time. In this paper we present a novel approach to ASL recognition that aspires to being a solution to the scalability problems. It is based on parallel HMMs (PaHMMs), which model the parallel processes independently. Thus, they can also be trained independently, and do not require consideration of the different combinations at training time. We develop the recognition algorithm for PaHMMs and show that it runs in time polynomial in the number of states, and in time linear in the number of parallel processes. We run several experiments with a 22 sign vocabulary and demonstrate that PaHMMs can improve the robustness of HMM-based recognition even on a small scale. Thus, PaHMMs are a very promising general recognition scheme with applications in both gesture and ASL recognition.},
  eventtitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  file = {C:\Users\test\Zotero\storage\8MU6A92M\791206.html}
}

@inproceedings{voglerParallelHiddenMarkov1999a,
  title = {Parallel Hidden {{Markov}} Models for {{American}} Sign Language Recognition},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Vogler, C. and Metaxas, D.},
  date = {1999-09},
  volume = {1},
  pages = {116-122 vol.1},
  doi = {10.1109/ICCV.1999.791206},
  url = {https://ieeexplore.ieee.org/document/791206},
  urldate = {2023-12-15},
  abstract = {The major challenge that faces American Sign Language (ASL) recognition now is to develop methods that will scale well with increasing vocabulary size. Unlike in spoken languages, phonemes can occur simultaneously in ASL. The number of possible combinations of phonemes after enforcing linguistic constraints is approximately 5.5/spl times/10/sup 8/. Gesture recognition, which is less constrained than ASL recognition, suffers from the same problem. Thus, it is not feasible to train conventional hidden Markov models (HMMs) for large-scab ASL applications. Factorial HMMs and coupled HMMs are two extensions to HMMs that explicitly attempt to model several processes occuring in parallel. Unfortunately, they still require consideration of the combinations at training time. In this paper we present a novel approach to ASL recognition that aspires to being a solution to the scalability problems. It is based on parallel HMMs (PaHMMs), which model the parallel processes independently. Thus, they can also be trained independently, and do not require consideration of the different combinations at training time. We develop the recognition algorithm for PaHMMs and show that it runs in time polynomial in the number of states, and in time linear in the number of parallel processes. We run several experiments with a 22 sign vocabulary and demonstrate that PaHMMs can improve the robustness of HMM-based recognition even on a small scale. Thus, PaHMMs are a very promising general recognition scheme with applications in both gesture and ASL recognition.},
  eventtitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  file = {C\:\\Users\\test\\Zotero\\storage\\JBDWMENN\\Vogler and Metaxas - 1999 - Parallel hidden Markov models for American sign la.pdf;C\:\\Users\\test\\Zotero\\storage\\K3R8CHBQ\\791206.html}
}

@article{vonagrisRecentDevelopmentsVisual2008,
  title = {Recent Developments in Visual Sign Language Recognition},
  author = {family=Agris, given=Ulrich, prefix=von, useprefix=true and Zieren, Jörg and Canzler, Ulrich and Bauer, Britta and Kraiss, Karl-Friedrich},
  date = {2008-02-01},
  journaltitle = {Universal Access in the Information Society},
  shortjournal = {Univ Access Inf Soc},
  volume = {6},
  number = {4},
  pages = {323--362},
  issn = {1615-5297},
  doi = {10.1007/s10209-007-0104-x},
  url = {https://doi.org/10.1007/s10209-007-0104-x},
  urldate = {2023-12-15},
  abstract = {Research in the field of sign language recognition has made significant advances in recent years. The present achievements provide the basis for future applications with the objective of supporting the integration of deaf people into the hearing society. Translation systems, for example, could facilitate communication between deaf and hearing people in public situations. Further applications, such as user interfaces and automatic indexing of signed videos, become feasible. The current state in sign language recognition is roughly 30~years behind speech recognition, which corresponds to the gradual transition from isolated to continuous recognition for small vocabulary tasks. Research efforts were mainly focused on robust feature extraction or statistical modeling of signs. However, current recognition systems are still designed for signer-dependent operation under laboratory conditions. This paper describes a comprehensive concept for robust visual sign language recognition, which represents the recent developments in this field. The proposed recognition system aims for signer-independent operation and utilizes a single video camera for data acquisition to ensure user-friendliness. Since sign languages make use of manual and facial means of expression, both channels are employed for recognition. For mobile operation in uncontrolled environments, sophisticated algorithms were developed that robustly extract manual and facial features. The extraction of manual features relies on a multiple hypotheses tracking approach to resolve ambiguities of hand positions. For facial feature extraction, an active appearance model is applied which allows identification of areas of interest such as the eyes and mouth region. In the next processing step, a numerical description of the facial expression, head pose, line of sight, and lip outline is computed. The system employs a resolution strategy for dealing with mutual overlapping of the signer’s hands and face. Classification is based on hidden Markov models which are able to compensate time and amplitude variances in the articulation of a sign. The classification stage is designed for recognition of isolated signs, as well as of continuous sign language. In the latter case, a stochastic language model can be utilized, which considers uni- and bigram probabilities of single and successive signs. For statistical modeling of reference models each sign is represented either as a whole or as a composition of smaller subunits—similar to phonemes in spoken languages. While recognition based on word models is limited to rather small vocabularies, subunit models open the door to large vocabularies. Achieving signer-independence constitutes a challenging problem, as the articulation of a sign is subject to high interpersonal variance. This problem cannot be solved by simple feature normalization and must be addressed at the classification level. Therefore, dedicated adaptation methods known from speech recognition were implemented and modified to consider the specifics of sign languages. For rapid adaptation to unknown signers the proposed recognition system employs a combined approach of maximum likelihood linear regression and maximum a posteriori estimation.},
  langid = {english},
  keywords = {Computer vision,Hidden Markov models,Human–computer interaction,Sign language recognition,Signer adaptation,Statistical pattern recognition}
}

@inproceedings{weerasooriyaSinhalaFingerspellingSign2022,
  title = {Sinhala {{Fingerspelling Sign Language Recognition}} with {{Computer Vision}}},
  booktitle = {2022 {{Moratuwa Engineering Research Conference}} ({{MERCon}})},
  author = {Weerasooriya, Amal A. and Ambegoda, Thanuja D.},
  date = {2022-07-27},
  pages = {1--6},
  publisher = {{IEEE}},
  location = {{Moratuwa, Sri Lanka}},
  doi = {10.1109/MERCon55799.2022.9906281},
  url = {https://ieeexplore.ieee.org/document/9906281/},
  urldate = {2023-11-29},
  abstract = {Computer vision based sign language translation is usually based on using thousands of images or video sequences for model training. This is not an issue in the case of widely used languages such as American Sign Language. However, in case of languages with low resources such as Sinhala Sign Language, it’s challenging to use similar methods for developing translators since there are no known data sets available for such studies.},
  eventtitle = {2022 {{Moratuwa Engineering Research Conference}} ({{MERCon}})},
  isbn = {978-1-66548-786-3},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\J4VM2MWH\Weerasooriya and Ambegoda - 2022 - Sinhala Fingerspelling Sign Language Recognition w.pdf}
}

@inproceedings{yooSLURMSimpleLinux2003,
  title = {{{SLURM}}: {{Simple Linux Utility}} for {{Resource Management}}},
  shorttitle = {{{SLURM}}},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
  editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {44--60},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/10968987_3},
  abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
  isbn = {978-3-540-39727-4},
  langid = {english},
  keywords = {Exit Status,Lawrence Livermore National Laboratory,Message Authentication Code,Remote Execution,Resource Management System},
  file = {C:\Users\test\Zotero\storage\ALL7PQ3K\Yoo et al. - 2003 - SLURM Simple Linux Utility for Resource Managemen.pdf}
}
