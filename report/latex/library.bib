@misc{abadiTensorFlowSystemLargescale2016,
  title = {{{TensorFlow}}: {{A}} System for Large-Scale Machine Learning},
  shorttitle = {{{TensorFlow}}},
  author = {Abadi, Mart{\'i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2016},
  month = may,
  number = {arXiv:1605.08695},
  eprint = {1605.08695},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.08695},
  urldate = {2023-12-18},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {C\:\\Users\\test\\Zotero\\storage\\3ZQEDSI8\\Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf;C\:\\Users\\test\\Zotero\\storage\\CVZGU8KX\\1605.html}
}

@inproceedings{abiyevReconstructionConvolutionalNeural2020,
  title = {Reconstruction of {{Convolutional Neural Network}} for {{Sign Language Recognition}}},
  booktitle = {2020 {{International Conference}} on {{Electrical}}, {{Communication}}, and {{Computer Engineering}} ({{ICECCE}})},
  author = {Abiyev, Rahib and Idoko, John Bush and Arslan, Murat},
  year = {2020},
  month = jun,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Istanbul, Turkey}},
  doi = {10.1109/ICECCE49384.2020.9179356},
  urldate = {2023-11-29},
  abstract = {This paper presents a Sign Language translation model using Convolutional Neural Networks (CNN). A sign language is a language which allows mute and hearingimpaired people to communicate. It is a visually oriented, nonverbal communication which facilitates communication through body/facial postures, expressions and a collection of gestures. To contribute to the wellbeing of the affected population, we are motivated to implement a vision-based system to avert their day to day challenges. Our propose model constitutes object detection and classification phases. The first module is made up of single shot multi-box detector (SSD) used for hand detection. The second module constitutes convolutional neural network plus a fully connected network utilized to constructively translate the detected signs into text. The propose model is implemented using American sign language fingerspelling dataset. The propose system outperformed other published results in the comparative analysis, hence recommended for further exploitation in sign language recognition problems.},
  isbn = {978-1-72817-116-6},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\UFTNKESY\Abiyev et al. - 2020 - Reconstruction of Convolutional Neural Network for.pdf}
}

@article{adeyanjuMachineLearningMethods2021,
  title = {Machine Learning Methods for Sign Language Recognition: {{A}} Critical Review and Analysis},
  shorttitle = {Machine Learning Methods for Sign Language Recognition},
  author = {Adeyanju, I.A. and Bello, O.O. and Adegboye, M.A.},
  year = {2021},
  month = nov,
  journal = {Intelligent Systems with Applications},
  volume = {12},
  pages = {200056},
  issn = {26673053},
  doi = {10.1016/j.iswa.2021.200056},
  urldate = {2023-11-29},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\WDP9WTXW\Adeyanju et al. - 2021 - Machine learning methods for sign language recogni.pdf}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2023-12-15},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\test\\Zotero\\storage\\8AI9Y35A\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;C\:\\Users\\test\\Zotero\\storage\\IX7ZCCJJ\\1409.html}
}

@inproceedings{bantupalliAmericanSignLanguage2018,
  title = {American {{Sign Language Recognition}} Using {{Deep Learning}} and {{Computer Vision}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Bantupalli, Kshitij and Xie, Ying},
  year = {2018},
  month = dec,
  pages = {4896--4899},
  doi = {10.1109/BigData.2018.8622141},
  urldate = {2023-11-29},
  abstract = {Speech impairment is a disability which affects an individuals ability to communicate using speech and hearing. People who are affected by this use other media of communication such as sign language. Although sign language is ubiquitous in recent times, there remains a challenge for non-sign language speakers to communicate with sign language speakers or signers. With recent advances in deep learning and computer vision there has been promising progress in the fields of motion and gesture recognition using deep learning and computer vision based techniques. The focus of this work is to create a visionbased application which offers sign language translation to text thus aiding communication between signers and non-signers. The proposed model takes video sequences and extracts temporal and spatial features from them. We then use Inception, a CNN (Convolutional Neural Network) for recognizing spatial features. We then use a RNN (Recurrent Neural Network) to train on temporal features. The dataset used is the American Sign Language Dataset.},
  file = {C\:\\Users\\test\\Zotero\\storage\\DEKECKH3\\Bantupalli and Xie - 2018 - American Sign Language Recognition using Deep Lear.pdf;C\:\\Users\\test\\Zotero\\storage\\SHY89S3Q\\8622141.html}
}

@article{chongAmericanSignLanguage2018,
  title = {American {{Sign Language Recognition Using Leap Motion Controller}} with {{Machine Learning Approach}}},
  author = {Chong, Teak-Wei and Lee, Boon-Giin},
  year = {2018},
  month = oct,
  journal = {Sensors},
  volume = {18},
  number = {10},
  pages = {3554},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s18103554},
  urldate = {2023-11-29},
  abstract = {Sign language is intentionally designed to allow deaf and dumb communities to convey messages and to connect with society. Unfortunately, learning and practicing sign language is not common among society; hence, this study developed a sign language recognition prototype using the Leap Motion Controller (LMC). Many existing studies have proposed methods for incomplete sign language recognition, whereas this study aimed for full American Sign Language (ASL) recognition, which consists of 26 letters and 10 digits. Most of the ASL letters are static (no movement), but certain ASL letters are dynamic (they require certain movements). Thus, this study also aimed to extract features from finger and hand motions to differentiate between the static and dynamic gestures. The experimental results revealed that the sign language recognition rates for the 26 letters using a support vector machine (SVM) and a deep neural network (DNN) are 80.30\% and 93.81\%, respectively. Meanwhile, the recognition rates for a combination of 26 letters and 10 digits are slightly lower, approximately 72.79\% for the SVM and 88.79\% for the DNN. As a result, the sign language recognition system has great potential for reducing the gap between deaf and dumb communities and others. The proposed prototype could also serve as an interpreter for the deaf and dumb in everyday life in service sectors, such as at the bank or post office.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {American Sign Language,deep neural network,human-computer interaction,Leap Motion Controller,machine learning,multi-class classification,sign language recognition,support vector machine},
  file = {C:\Users\test\Zotero\storage\QGWQXCYL\Chong and Lee - 2018 - American Sign Language Recognition Using Leap Moti.pdf}
}

@inproceedings{cihancamgozSignLanguageTransformers2020,
  title = {Sign {{Language Transformers}}: {{Joint End-to-End Sign Language Recognition}} and {{Translation}}},
  shorttitle = {Sign {{Language Transformers}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cihan Camgoz, Necati and Koller, Oscar and Hadfield, Simon and Bowden, Richard},
  year = {2020},
  month = jun,
  pages = {10020--10030},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01004},
  urldate = {2023-11-29},
  abstract = {Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-theart in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-tosequence learning problems and leads to significant performance gains.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\MGNGYH32\Cihan Camgoz et al. - 2020 - Sign Language Transformers Joint End-to-End Sign .pdf}
}

@article{culjakBriefIntroductionOpenCV,
  title = {A Brief Introduction to {{OpenCV}}},
  author = {Culjak, Ivan and Abram, David and Pribanic, Tomislav and Dzapo, Hrvoje and Cifrek, Mario},
  abstract = {The purpose of this paper is to introduce and quickly make a reader familiar with OpenCV (Open Source Computer Vision) basics without having to go through the lengthy reference manuals and books. OpenCV is an open source library for image and video analysis, originally introduced more than decade ago by Intel. Since then, a number of programmers have contributed to the most recent library developments. The latest major change took place in 2009 (OpenCV 2) which includes main changes to the C++ interface. Nowadays the library has {$>$}2500 optimized algorithms. It is extensively used around the world, having {$>$}2.5M downloads and {$>$}40K people in the user group. Regardless of whether one is a novice C++ programmer or a professional software developer, unaware of OpenCV, the main library content should be interesting for the graduate students and researchers in image processing and computer vision areas. To master every library element it is necessary to consult many books available on the topic of OpenCV. However, reading such more comprehensive material should be easier after comprehending some basics about OpenCV from this paper.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\S8CRTPLE\Culjak et al. - A brief introduction to OpenCV.pdf}
}

@misc{ethnologueAmericanSignLanguage2023,
  title = {American {{Sign Language}} | {{Ethnologue Free}}},
  author = {{Ethnologue}},
  year = {2023},
  month = feb,
  journal = {Ethnologue (Free All)},
  url = {https://www.ethnologue.com/language/ase/},
  urldate = {2023-11-05},
  abstract = {American Sign Language is a stable indigenous language of the United States. It is a Deaf community sign language. The language is used as a first language by all in the ethnic community. It is not known to be taught in schools.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\H57QK6YX\ase.html}
}

@inproceedings{gajurelFineGrainedVisualAttention2021,
  title = {A {{Fine-Grained Visual Attention Approach}} for {{Fingerspelling Recognition}} in the {{Wild}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Gajurel, Kamala and Zhong, Cuncong and Wang, Guanghui},
  year = {2021},
  month = oct,
  pages = {3266--3271},
  publisher = {{IEEE}},
  address = {{Melbourne, Australia}},
  doi = {10.1109/SMC52423.2021.9658982},
  urldate = {2023-12-13},
  abstract = {Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.},
  isbn = {978-1-66544-207-7},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\TSAAPSAB\Gajurel et al. - 2021 - A Fine-Grained Visual Attention Approach for Finge.pdf}
}

@inproceedings{gravesConnectionistTemporalClassification2006a,
  title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  shorttitle = {Connectionist Temporal Classification},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  month = jun,
  series = {{{ICML}} '06},
  pages = {369--376},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143891},
  urldate = {2023-12-16},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  isbn = {978-1-59593-383-6},
  file = {C:\Users\test\Zotero\storage\FS9L227Z\Graves et al. - 2006 - Connectionist temporal classification labelling u.pdf}
}

@misc{gulatiConformerConvolutionaugmentedTransformer2020,
  title = {Conformer: {{Convolution-augmented Transformer}} for {{Speech Recognition}}},
  shorttitle = {Conformer},
  author = {Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and Pang, Ruoming},
  year = {2020},
  month = may,
  number = {arXiv:2005.08100},
  eprint = {2005.08100},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.08100},
  urldate = {2023-12-18},
  abstract = {Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1\%/4.3\% without using a language model and 1.9\%/3.9\% with an external language model on test/testother. We also observe competitive performance of 2.7\%/6.3\% with a small model of only 10M parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\test\\Zotero\\storage\\FU6XEWW8\\Gulati et al. - 2020 - Conformer Convolution-augmented Transformer for S.pdf;C\:\\Users\\test\\Zotero\\storage\\6CR52HYI\\2005.html}
}

@inproceedings{kabadeAmericanSignLanguage2023,
  title = {American {{Sign Language Fingerspelling Recognition}} Using {{Attention Model}}},
  booktitle = {2023 {{IEEE}} 8th {{International Conference}} for {{Convergence}} in {{Technology}} ({{I2CT}})},
  author = {Kabade, Amruta E and Desai, Padmashree and C, Sujatha and G, Shankar},
  year = {2023},
  month = apr,
  pages = {1--6},
  doi = {10.1109/I2CT57861.2023.10126277},
  urldate = {2023-11-29},
  abstract = {Sign Language Recognition(SLR) is a complex gesture recognition problem because of the quick and highly coarticulated motion involved in gestures. This research work focuses on Fingerspelling recognition task, which constitutes 35\% of the American Sign Language (ASL). Fingerspelling identifies the word letter by letter. Fingerspelling is used for signing the words which do not have designated ASL signs such as technical terms, content words and proper nouns. In our proposed work for ASL Fingerspelling recognition, we consider ChicagoFSWild dataset which consists of occlusions and images captured in varying illuminations, lighting conditions (in the wild environments). The optical flow is obtained from Lucas-Kanade algorithm, prior is generated, images are resized and cropped with face-roi technique to get the region of interest (ROI). The visual attention mechanism attends to the ROI iteratively. ResNet, pretrained on Imagenet is used for the extraction of spatial features. The Bi-LSTM network with Connectionist Temporal Classification (CTC) is used to predict the sign. It provides the accuracy of 57\% on ChicagoFSWild dataset for Fingerspelling recognition task.},
  file = {C:\Users\test\Zotero\storage\E8II75TX\Kabade et al. - 2023 - American Sign Language Fingerspelling Recognition .pdf}
}

@misc{kaggleAslFinger,
  title = {Google - American Sign Language Fingerspelling Recognition},
  author = {{Manfred Georg} and {Mark Sherwood} and {Phil Culliton} and {Sam Sepah} and {Sohier Dane} and {Thad Starner} and {Ashley Chow} and {Glenn Cameron}},
  year = {2023},
  publisher = {{Kaggle}},
  url = {https://kaggle.com/competitions/asl-fingerspelling}
}

@misc{kimSqueezeformerEfficientTransformer2022,
  title = {Squeezeformer: {{An Efficient Transformer}} for {{Automatic Speech Recognition}}},
  shorttitle = {Squeezeformer},
  author = {Kim, Sehoon and Gholami, Amir and Shaw, Albert and Lee, Nicholas and Mangalam, Karttikeya and Malik, Jitendra and Mahoney, Michael W. and Keutzer, Kurt},
  year = {2022},
  month = oct,
  number = {arXiv:2206.00888},
  eprint = {2206.00888},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.00888},
  urldate = {2023-12-18},
  abstract = {The recently proposed Conformer model has become the de facto backbone model for various downstream speech tasks based on its hybrid attention-convolution architecture that captures both local and global features. However, through a series of systematic studies, we find that the Conformer architecture's design choices are not optimal. After re-examining the design choices for both the macro and micro-architecture of Conformer, we propose Squeezeformer which consistently outperforms the state-of-the-art ASR models under the same training schemes. In particular, for the macro-architecture, Squeezeformer incorporates (i) the Temporal U-Net structure which reduces the cost of the multi-head attention modules on long sequences, and (ii) a simpler block structure of multi-head attention or convolution modules followed up by feed-forward module instead of the Macaron structure proposed in Conformer. Furthermore, for the micro-architecture, Squeezeformer (i) simplifies the activations in the convolutional block, (ii) removes redundant Layer Normalization operations, and (iii) incorporates an efficient depthwise down-sampling layer to efficiently sub-sample the input signal. Squeezeformer achieves state-of-the-art results of 7.5\%, 6.5\%, and 6.0\% word-error-rate (WER) on LibriSpeech test-other without external language models, which are 3.1\%, 1.4\%, and 0.6\% better than Conformer-CTC with the same number of FLOPs. Our code is open-sourced and available online.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\test\\Zotero\\storage\\UXYTA3FJ\\Kim et al. - 2022 - Squeezeformer An Efficient Transformer for Automa.pdf;C\:\\Users\\test\\Zotero\\storage\\TTAC3JU9\\2206.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper\_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2023-12-15},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {C:\Users\test\Zotero\storage\FKTSXN22\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  urldate = {2023-12-15},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  file = {C\:\\Users\\test\\Zotero\\storage\\UN65WJMB\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;C\:\\Users\\test\\Zotero\\storage\\P7VRXTPH\\726791.html}
}

@misc{lugaresiMediaPipeFrameworkBuilding2019,
  title = {{{MediaPipe}}: {{A Framework}} for {{Building Perception Pipelines}}},
  shorttitle = {{{MediaPipe}}},
  author = {Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and Chang, Wan-Teh and Hua, Wei and Georg, Manfred and Grundmann, Matthias},
  year = {2019},
  month = jun,
  number = {arXiv:1906.08172},
  eprint = {1906.08172},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.08172},
  urldate = {2023-12-18},
  abstract = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and finally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {C\:\\Users\\test\\Zotero\\storage\\TL3SCWJQ\\Lugaresi et al. - 2019 - MediaPipe A Framework for Building Perception Pip.pdf;C\:\\Users\\test\\Zotero\\storage\\PKF8C8CU\\1906.html}
}

@article{mitchellHowManyPeople2006,
  title = {How {{Many People Use ASL}} in the {{United States}}?: {{Why Estimates Need Updating}}},
  shorttitle = {How {{Many People Use ASL}} in the {{United States}}?},
  author = {Mitchell, Ross E. and Young, Travas A. and Bachelda, Bellamie and Karchmer, Michael A.},
  year = {2006},
  journal = {Sign Language Studies},
  volume = {6},
  number = {3},
  eprint = {26190621},
  eprinttype = {jstor},
  pages = {306--335},
  publisher = {{Gallaudet University Press}},
  issn = {0302-1475},
  url = {https://www.jstor.org/stable/26190621},
  urldate = {2023-11-05},
  abstract = {This article traces the sources of the estimates of the number of American Sign Language users in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all of the data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates nor any specific study of ASL use. The article concludes with a call to action to rectify this problem.},
  file = {C:\Users\test\Zotero\storage\RC447MA5\Mitchell et al. - 2006 - How Many People Use ASL in the United States Why.pdf}
}

@article{munibAmericanSignLanguage2007b,
  title = {American Sign Language ({{ASL}}) Recognition Based on {{Hough}} Transform and Neural Networks},
  author = {Munib, Qutaishat and Habeeb, Moussa and Takruri, Bayan and {Al-Malik}, Hiba Abed},
  year = {2007},
  month = jan,
  journal = {Expert Systems with Applications},
  volume = {32},
  number = {1},
  pages = {24--37},
  issn = {09574174},
  doi = {10.1016/j.eswa.2005.11.018},
  urldate = {2023-12-15},
  abstract = {The work presented in this paper aims to develop a system for automatic translation of static gestures of alphabets and signs in American sign language. In doing so, we have used Hough transform and neural networks which is trained to recognize signs. Our system does not rely on using any gloves or visual markings to achieve the recognition task. Instead, it deals with images of bare hands, which allows the user to interact with the system in a natural way. An image is processed and converted to a feature vector that will be compared with the feature vectors of a training set of signs. The extracted features are not affected by the rotation, scaling or translation of the gesture within the image, which makes the system more flexible.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\HQR6UEYU\Munib et al. - 2007 - American sign language (ASL) recognition based on .pdf}
}

@inproceedings{nguyenDeepLearningAmerican2019,
  title = {Deep {{Learning}} for {{American Sign Language Fingerspelling Recognition System}}},
  booktitle = {2019 26th {{International Conference}} on {{Telecommunications}} ({{ICT}})},
  author = {Nguyen, Huy B.D and Do, Hung Ngoc},
  year = {2019},
  month = apr,
  pages = {314--318},
  publisher = {{IEEE}},
  address = {{Hanoi, Vietnam}},
  doi = {10.1109/ICT.2019.8798856},
  urldate = {2023-11-29},
  abstract = {Sign language has always been a major tool for communication among people with disabilities. In this paper, a sign language fingerspelling alphabet identification system would be developed by using image processing technique, supervised machine learning and deep learning. In particular, 24 alphabetical symbols are presented by several combinations of static gestures (excluding 2 motion gestures J and Z). Histogram of Oriented Gradients (HOG) and Local Binary Pattern (LBP) features of each gesture will be extracted from training images. Then Multiclass Support Vector Machines (SVMs) will be applied to train these extracted data. Also, an end-to-end Convolutional Neural Network (CNN) architecture will be applied to the training dataset for comparison. After that, a further combination of CNN as feature descriptor and SVM produces an acceptable result. The Massey Dataset is implemented in the training and testing phases of the whole system.},
  isbn = {978-1-72810-273-3},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\ZCNRKPQK\Nguyen and Do - 2019 - Deep Learning for American Sign Language Fingerspe.pdf}
}

@article{ozAmericanSignLanguage2011,
  title = {American {{Sign Language}} Word Recognition with a Sensory Glove Using Artificial Neural Networks},
  author = {Oz, Cemil and Leu, Ming C.},
  year = {2011},
  month = oct,
  journal = {Engineering Applications of Artificial Intelligence},
  series = {Infrastructures and {{Tools}} for {{Multiagent Systems}}},
  volume = {24},
  number = {7},
  pages = {1204--1213},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2011.06.015},
  urldate = {2023-12-15},
  abstract = {An American Sign Language (ASL) recognition system is being developed using artificial neural networks (ANNs) to translate ASL words into English. The system uses a sensory glove called the Cyberglove{\texttrademark} and a Flock of Birds{\textregistered} 3-D motion tracker to extract the gesture features. The data regarding finger joint angles obtained from strain gauges in the sensory glove define the hand shape, while the data from the tracker describe the trajectory of hand movements. The data from these devices are processed by a velocity network with noise reduction and feature extraction and by a word recognition network. Some global and local features are extracted for each ASL word. A neural network is used as a classifier of this feature vector. Our goal is to continuously recognize ASL signs using these devices in real time. We trained and tested the ANN model for 50 ASL words with a different number of samples for every word. The test results show that our feature vector extraction method and neural networks can be used successfully for isolated word recognition. This system is flexible and open for future extension.},
  keywords = {American Sign Language (ASL),Artificial Neural Networks (ANNs),ASL recognition,Finger spelling recognition,Hand-shape recognition},
  file = {C:\Users\test\Zotero\storage\35EI6IGW\S0952197611001230.html}
}

@misc{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {arXiv:1912.01703},
  eprint = {1912.01703},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.01703},
  urldate = {2023-12-18},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Machine Learning},
  file = {C\:\\Users\\test\\Zotero\\storage\\KTP3SUJU\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf;C\:\\Users\\test\\Zotero\\storage\\94839W9V\\1912.html}
}

@article{rastgooSignLanguageRecognition2021,
  title = {Sign {{Language Recognition}}: {{A Deep Survey}}},
  shorttitle = {Sign {{Language Recognition}}},
  author = {Rastgoo, Razieh and Kiani, Kourosh and Escalera, Sergio},
  year = {2021},
  month = feb,
  journal = {Expert Systems with Applications},
  volume = {164},
  pages = {113794},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.113794},
  urldate = {2023-11-29},
  abstract = {Sign language, as a different form of the communication language, is important to large groups of people in society. There are different signs in each sign language with variability in hand shape, motion profile, and position of the hand, face, and body parts contributing to each sign. So, visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the visionbased proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved. We present a taxonomy to categorize the proposed models for isolated and continuous sign language recognition, discussing applications, datasets, hybrid models, complexity, and future lines of research in the field.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\NXZMWM3W\Rastgoo et al. - 2021 - Sign Language Recognition A Deep Survey.pdf}
}

@article{saeedSystematicReviewSystemsBased2022,
  title = {A {{Systematic Review}} on {{Systems-Based Sensory Gloves}} for {{Sign Language Pattern Recognition}}: {{An Update From}} 2017 to 2022},
  shorttitle = {A {{Systematic Review}} on {{Systems-Based Sensory Gloves}} for {{Sign Language Pattern Recognition}}},
  author = {Saeed, Zinah Raad and Zainol, Zurinahni Binti and Zaidan, B. B. and Alamoodi, A. H.},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {123358--123377},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3219430},
  urldate = {2023-12-15},
  abstract = {Sign language is the predominant mode of communication for the Hearing impaired community. For the millions of people who suffer from hearing loss around the world, interaction with people who have the ability to hear and do not suffer from hearing impairment or loss is considered as complicated. In line with this issue, technology is perceived as a crucial factor in being an enabler of providing solutions to enhance the quality of life of the hearing impairment by increasing accessibility. This research aims to review and analyze articles related to sign language recognition based on the sensor- based glove system, in order to identify academic motivations, challenges, and recommendations related to this field. The search for the relevant review materials and articles was performed on four major databases ranging from 2017 to 2022: Science Direct, Web of Science, IEEE Xplore, and Scopus. The articles were chosen based on our inclusion and exclusion criteria. The literature findings of this paper indicate the dataset size to be open issues and challenges for hand gesture recognition. Furthermore, the majority of research on sign language recognition based on data glove was performed on static, single hand, and isolated gestures. Moreover, recognition accuracy typically achieved results higher than 90\%. However, most experiments were carried out with a limited number of gestures. Overall, it is hoped that this study will serve as a roadmap for future research and raise awareness among researchers in the field of sign language recognition.},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\4JVIXX4C\Saeed et al. - 2022 - A Systematic Review on Systems-Based Sensory Glove.pdf}
}

@article{sherstinskyFundamentalsRecurrentNeural2020,
  title = {Fundamentals of {{Recurrent Neural Network}} ({{RNN}}) and {{Long Short-Term Memory}} ({{LSTM}}) {{Network}}},
  author = {Sherstinsky, Alex},
  year = {2020},
  month = mar,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {404},
  eprint = {1808.03314},
  primaryclass = {cs, stat},
  pages = {132306},
  issn = {01672789},
  doi = {10.1016/j.physd.2019.132306},
  urldate = {2023-12-16},
  abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\test\\Zotero\\storage\\AMLAA4BQ\\Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and.pdf;C\:\\Users\\test\\Zotero\\storage\\DDEIX5SN\\1808.html}
}

@inproceedings{shiAmericanSignLanguage2018,
  title = {American {{Sign Language Fingerspelling Recognition}} in the {{Wild}}},
  booktitle = {2018 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Shi, Bowen and Del Rio, Aurora Martinez and Keane, Jonathan and Michaux, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2018},
  month = dec,
  pages = {145--152},
  doi = {10.1109/SLT.2018.8639639},
  urldate = {2023-11-29},
  abstract = {We address the problem of American Sign Language fingerspelling recognition ``in the wild'', using videos collected from websites. We introduce the largest data set available so far for the problem of fingerspelling recognition, and the first using naturally occurring video data. Using this data set, we present the first attempt to recognize fingerspelling sequences in this challenging setting. Unlike prior work, our video data is extremely challenging due to low frame rates and visual variability. To tackle the visual challenges, we train a special-purpose signing hand detector using a small subset of our data. Given the hand detector output, a sequence model decodes the hypothesized fingerspelled letter sequence. For the sequence model, we explore attention-based recurrent encoder-decoders and CTC-based approaches. As the first attempt at fingerspelling recognition in the wild, this work is intended to serve as a baseline for future work on sign language recognition in realistic conditions. We find that, as expected, letter error rates are much higher than in previous work on more controlled data, and we analyze the sources of error and effects of model variants.},
  file = {C\:\\Users\\test\\Zotero\\storage\\M9RBGXG2\\Shi et al. - 2018 - American Sign Language Fingerspelling Recognition .pdf;C\:\\Users\\test\\Zotero\\storage\\CE65XA5S\\8639639.html}
}

@inproceedings{shiFingerspellingDetectionAmerican2021,
  title = {Fingerspelling {{Detection}} in {{American Sign Language}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shi, Bowen and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2021},
  month = jun,
  pages = {4164--4173},
  publisher = {{IEEE}},
  address = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00415},
  urldate = {2023-11-29},
  abstract = {Fingerspelling, in which words are signed letter by letter, is an important component of American Sign Language. Most previous work on automatic fingerspelling recognition has assumed that the boundaries of fingerspelling regions in signing videos are known beforehand. In this paper, we consider the task of fingerspelling detection in raw, untrimmed sign language videos. This is an important step towards building real-world fingerspelling recognition systems. We propose a benchmark and a suite of evaluation metrics, some of which reflect the effect of detection on the downstream fingerspelling recognition task. In addition, we propose a new model that learns to detect fingerspelling via multi-task training, incorporating pose estimation and fingerspelling recognition (transcription) along with detection, and compare this model to several alternatives. The model outperforms all alternative approaches across all metrics, establishing a state of the art on the benchmark.},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\3DXLW7AJ\Shi et al. - 2021 - Fingerspelling Detection in American Sign Language.pdf}
}

@inproceedings{shiFingerspellingRecognitionWild2019,
  title = {Fingerspelling {{Recognition}} in the {{Wild With Iterative Visual Attention}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Shi, Bowen and Rio, Aurora Martinez Del and Keane, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2019},
  month = oct,
  pages = {5399--5408},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00550},
  urldate = {2023-11-29},
  abstract = {Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It outperforms prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\VNP3293B\Shi et al. - 2019 - Fingerspelling Recognition in the Wild With Iterat.pdf}
}

@misc{shiSearchingFingerspelledContent2022,
  title = {Searching for Fingerspelled Content in {{American Sign Language}}},
  author = {Shi, Bowen and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2022},
  month = mar,
  number = {arXiv:2203.13291},
  eprint = {2203.13291},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.13291},
  urldate = {2023-12-13},
  abstract = {Natural language processing for sign language video - including tasks like recognition, translation, and search - is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years. In this paper, we address the problem of searching for fingerspelled key-words or key phrases in raw sign language videos. This is an important task since significant content in sign language is often conveyed via fingerspelling, and to our knowledge the task has not been studied before. We propose an end-to-end model for this task, FSS-Net, that jointly detects fingerspelling and matches it to a text sequence. Our experiments, done on a large public dataset of ASL fingerspelling in the wild, show the importance of fingerspelling detection as a component of a search and retrieval model. Our model significantly outperforms baseline methods adapted from prior work on related tasks},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\test\\Zotero\\storage\\7EF3XGGJ\\Shi et al. - 2022 - Searching for fingerspelled content in American Si.pdf;C\:\\Users\\test\\Zotero\\storage\\D4BVWXQD\\2203.html}
}

@inproceedings{skumarTimeSeriesNeural2018,
  title = {Time {{Series Neural Networks}} for {{Real Time Sign Language Translation}}},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {S Kumar, Sujay and Wangyal, Tenzin and Saboo, Varun and Srinath, Ramamoorthy},
  year = {2018},
  month = dec,
  pages = {243--248},
  publisher = {{IEEE}},
  address = {{Orlando, FL}},
  doi = {10.1109/ICMLA.2018.00043},
  urldate = {2023-11-29},
  abstract = {Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.},
  isbn = {978-1-5386-6805-4},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\JPEHN7HA\S Kumar et al. - 2018 - Time Series Neural Networks for Real Time Sign Lan.pdf}
}

@inproceedings{srinivasanPythonOpencvSign2023,
  title = {Python {{And Opencv For Sign Language Recognition}}},
  booktitle = {2023 {{International Conference}} on {{Device Intelligence}}, {{Computing}} and {{Communication Technologies}}, ({{DICCT}})},
  author = {Srinivasan, R and Kavita, R and Kavitha, M and Mallikarjuna, Basetty and Bhatia, Sandeep and Agarwal, Bhawna and Ahlawat, Vinay and Goel, Amit},
  year = {2023},
  month = mar,
  pages = {1--5},
  doi = {10.1109/DICCT56244.2023.10110225},
  urldate = {2023-12-18},
  abstract = {Hearing-impaired people cannot communicate with normal people easily. Most people are not aware of sign language recognition. To support this, machine learning and CV can be used to create an impact on the impaired. This can be improved into automatic editors, in which the person can easily understand the sign language of the impaired people by just using hand sign recognition. In non-verbal communication, hand gesture always being an important mode of communication and it plays a vital role to bridge gap between deaf and dumb people. Several sign language recognition systems have been developed but the systems are not flexible and cost-effective. Physically challenged people can express their emotions and feelings through sign language. In this paper, we develop a sign detector that can detect signs and numbers and also other signs that are used in sign language with the help of OpenCV and Keras modules in python. By using this technology, we can understand what they want to convey through sign language which is not a common language to communicate with people. OpenCV and Keras of python are the modules used to achieve our work and the proposed work proved to be a user-friendly approach to communication by using Python language to recognize sign languages for hearing-impaired persons.},
  file = {C\:\\Users\\test\\Zotero\\storage\\XTHMKVND\\Srinivasan et al. - 2023 - Python And Opencv For Sign Language Recognition.pdf;C\:\\Users\\test\\Zotero\\storage\\UKFDHB7G\\10110225.html}
}

@article{starnerRealtimeAmericanSign1998a,
  title = {Real-Time {{American}} Sign Language Recognition Using Desk and Wearable Computer Based Video},
  author = {Starner, T. and Weaver, J. and Pentland, A.},
  year = {1998},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {12},
  pages = {1371--1375},
  issn = {1939-3539},
  doi = {10.1109/34.735811},
  urldate = {2023-12-15},
  abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon.},
  file = {C\:\\Users\\test\\Zotero\\storage\\9ITYQU23\\Starner et al. - 1998 - Real-time American sign language recognition using.pdf;C\:\\Users\\test\\Zotero\\storage\\W6XC44TD\\735811.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2023-12-15},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\test\\Zotero\\storage\\DY65ESM9\\Vaswani et al. - 2023 - Attention Is All You Need.pdf;C\:\\Users\\test\\Zotero\\storage\\4VPZQG2G\\1706.html}
}

@inproceedings{voglerParallelHiddenMarkov1999a,
  title = {Parallel Hidden {{Markov}} Models for {{American}} Sign Language Recognition},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Vogler, C. and Metaxas, D.},
  year = {1999},
  month = sep,
  volume = {1},
  pages = {116-122 vol.1},
  doi = {10.1109/ICCV.1999.791206},
  urldate = {2023-12-15},
  abstract = {The major challenge that faces American Sign Language (ASL) recognition now is to develop methods that will scale well with increasing vocabulary size. Unlike in spoken languages, phonemes can occur simultaneously in ASL. The number of possible combinations of phonemes after enforcing linguistic constraints is approximately 5.5/spl times/10/sup 8/. Gesture recognition, which is less constrained than ASL recognition, suffers from the same problem. Thus, it is not feasible to train conventional hidden Markov models (HMMs) for large-scab ASL applications. Factorial HMMs and coupled HMMs are two extensions to HMMs that explicitly attempt to model several processes occuring in parallel. Unfortunately, they still require consideration of the combinations at training time. In this paper we present a novel approach to ASL recognition that aspires to being a solution to the scalability problems. It is based on parallel HMMs (PaHMMs), which model the parallel processes independently. Thus, they can also be trained independently, and do not require consideration of the different combinations at training time. We develop the recognition algorithm for PaHMMs and show that it runs in time polynomial in the number of states, and in time linear in the number of parallel processes. We run several experiments with a 22 sign vocabulary and demonstrate that PaHMMs can improve the robustness of HMM-based recognition even on a small scale. Thus, PaHMMs are a very promising general recognition scheme with applications in both gesture and ASL recognition.},
  file = {C\:\\Users\\test\\Zotero\\storage\\JBDWMENN\\Vogler and Metaxas - 1999 - Parallel hidden Markov models for American sign la.pdf;C\:\\Users\\test\\Zotero\\storage\\K3R8CHBQ\\791206.html}
}

@article{vonagrisRecentDevelopmentsVisual2008,
  title = {Recent Developments in Visual Sign Language Recognition},
  author = {{von Agris}, Ulrich and Zieren, J{\"o}rg and Canzler, Ulrich and Bauer, Britta and Kraiss, Karl-Friedrich},
  year = {2008},
  month = feb,
  journal = {Universal Access in the Information Society},
  volume = {6},
  number = {4},
  pages = {323--362},
  issn = {1615-5297},
  doi = {10.1007/s10209-007-0104-x},
  urldate = {2023-12-15},
  abstract = {Research in the field of sign language recognition has made significant advances in recent years. The present achievements provide the basis for future applications with the objective of supporting the integration of deaf people into the hearing society. Translation systems, for example, could facilitate communication between deaf and hearing people in public situations. Further applications, such as user interfaces and automatic indexing of signed videos, become feasible. The current state in sign language recognition is roughly 30~years behind speech recognition, which corresponds to the gradual transition from isolated to continuous recognition for small vocabulary tasks. Research efforts were mainly focused on robust feature extraction or statistical modeling of signs. However, current recognition systems are still designed for signer-dependent operation under laboratory conditions. This paper describes a comprehensive concept for robust visual sign language recognition, which represents the recent developments in this field. The proposed recognition system aims for signer-independent operation and utilizes a single video camera for data acquisition to ensure user-friendliness. Since sign languages make use of manual and facial means of expression, both channels are employed for recognition. For mobile operation in uncontrolled environments, sophisticated algorithms were developed that robustly extract manual and facial features. The extraction of manual features relies on a multiple hypotheses tracking approach to resolve ambiguities of hand positions. For facial feature extraction, an active appearance model is applied which allows identification of areas of interest such as the eyes and mouth region. In the next processing step, a numerical description of the facial expression, head pose, line of sight, and lip outline is computed. The system employs a resolution strategy for dealing with mutual overlapping of the signer's hands and face. Classification is based on hidden Markov models which are able to compensate time and amplitude variances in the articulation of a sign. The classification stage is designed for recognition of isolated signs, as well as of continuous sign language. In the latter case, a stochastic language model can be utilized, which considers uni- and bigram probabilities of single and successive signs. For statistical modeling of reference models each sign is represented either as a whole or as a composition of smaller subunits{\textemdash}similar to phonemes in spoken languages. While recognition based on word models is limited to rather small vocabularies, subunit models open the door to large vocabularies. Achieving signer-independence constitutes a challenging problem, as the articulation of a sign is subject to high interpersonal variance. This problem cannot be solved by simple feature normalization and must be addressed at the classification level. Therefore, dedicated adaptation methods known from speech recognition were implemented and modified to consider the specifics of sign languages. For rapid adaptation to unknown signers the proposed recognition system employs a combined approach of maximum likelihood linear regression and maximum a posteriori estimation.},
  langid = {english},
  keywords = {Computer vision,Hidden Markov models,Human{\textendash}computer interaction,Sign language recognition,Signer adaptation,Statistical pattern recognition}
}

@inproceedings{weerasooriyaSinhalaFingerspellingSign2022,
  title = {Sinhala {{Fingerspelling Sign Language Recognition}} with {{Computer Vision}}},
  booktitle = {2022 {{Moratuwa Engineering Research Conference}} ({{MERCon}})},
  author = {Weerasooriya, Amal A. and Ambegoda, Thanuja D.},
  year = {2022},
  month = jul,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Moratuwa, Sri Lanka}},
  doi = {10.1109/MERCon55799.2022.9906281},
  urldate = {2023-11-29},
  abstract = {Computer vision based sign language translation is usually based on using thousands of images or video sequences for model training. This is not an issue in the case of widely used languages such as American Sign Language. However, in case of languages with low resources such as Sinhala Sign Language, it's challenging to use similar methods for developing translators since there are no known data sets available for such studies.},
  isbn = {978-1-66548-786-3},
  langid = {english},
  file = {C:\Users\test\Zotero\storage\J4VM2MWH\Weerasooriya and Ambegoda - 2022 - Sinhala Fingerspelling Sign Language Recognition w.pdf}
}
