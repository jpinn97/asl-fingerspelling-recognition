{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch._dynamo\n",
    "from torchdata.dataloader2 import DataLoader2, MultiProcessingReadingService\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdata.datapipes.iter import (\n",
    "    FileLister,\n",
    "    FileOpener,\n",
    "    TFRecordLoader,\n",
    "    Mapper,\n",
    "    Batcher,\n",
    "    Collator,\n",
    "    Shuffler,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.notebook import (\n",
    "    tqdm_notebook,\n",
    ")  # Assuming you only need one tqdm implementation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    "    RichModelSummary,\n",
    ")\n",
    "from torchmetrics.text import EditDistance\n",
    "\n",
    "\n",
    "# Set the default matmul precision to medium, or high/highest?\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# Read the first CSV file\n",
    "dataset_train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "dataset_supplemental_df = pd.read_csv(\"supplemental_metadata.csv\")\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "dataset_df = pd.concat([dataset_train_df, dataset_supplemental_df], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV file\n",
    "# dataset_df.to_csv(\"train_full.csv\", index=False)\n",
    "dataset_df = dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read the first row of the DataFrame\n",
    "path, sequence_id, file_id, phrase = dataset_df.iloc[0][\n",
    "    [\"path\", \"sequence_id\", \"file_id\", \"phrase\"]\n",
    "]\n",
    "print(f\"path: {path}, sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "sample_sequence_df = pq.read_table(\n",
    "    f\"{str(path)}\",\n",
    "    filters=[\n",
    "        [(\"sequence_id\", \"=\", sequence_id)],\n",
    "    ],\n",
    ").to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))\n",
    "\n",
    "# Calculate the length of each phrase by characters\n",
    "df['phrase_length_chars'] = df['phrase'].apply(len)\n",
    "\n",
    "# Create a histogram\n",
    "ax = df['phrase_length_chars'].plot.hist(bins=30, color='grey', alpha=0.7)\n",
    "plt.title('Distribution of Phrase Lengths in Characters')\n",
    "plt.xlabel('Length of Phrases (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Calculate and display mean and median\n",
    "mean_val = df['phrase_length_chars'].mean()\n",
    "median_val = df['phrase_length_chars'].median()\n",
    "plt.axvline(mean_val, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(median_val, color='c', linestyle='dashed', linewidth=1)\n",
    "plt.legend({'Mean':mean_val, 'Median':median_val})\n",
    "\n",
    "plt.text(mean_val + 10, 1000, f'Mean: {mean_val:.2f}', rotation=0)\n",
    "plt.text(median_val + 10, 45, f'  Median: {median_val:.2f}', rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_directory(directory):\n",
    "  data = []\n",
    "  for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".parquet\"):\n",
    "      filepath = os.path.join(directory, filename)\n",
    "      # Read the Parquet file\n",
    "      df = pd.read_parquet(filepath)\n",
    "      # Group by sequence_id and count the number of frames\n",
    "      grouped_data = df.groupby('sequence_id')['frame'].count().reset_index()\n",
    "      # Append the group data to the main list\n",
    "      data.append(grouped_data)\n",
    "  # Combine all group data into a single DataFrame\n",
    "  result_df = pd.concat(data, ignore_index=True)\n",
    "  return result_df\n",
    "\n",
    "directory = \"/home/jpinn/asl-fingerspelling-recognition/src/train_landmarks\"\n",
    "\n",
    "# Process the directory and get the DataFrame\n",
    "result_df = process_directory(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram\n",
    "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "plt.hist(result_df['frame'], color='grey', bins=100)  # Adjust the number of bins as needed\n",
    "plt.xlabel('Number of Frames')\n",
    "plt.ylabel('Number of Sequences')\n",
    "plt.title('Distribution of Frames per Sequence')\n",
    "plt.grid(True)  # Add gridlines for better readability\n",
    "\n",
    "# Calculate and display mean and median\n",
    "mean_val = result_df['frame'].mean()\n",
    "median_val = result_df['frame'].median()\n",
    "plt.axvline(mean_val, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(median_val, color='c', linestyle='dashed', linewidth=1)\n",
    "plt.legend({'Mean':mean_val, 'Median':median_val})\n",
    "\n",
    "plt.text(mean_val + 150, 1000, f'Mean: {mean_val:.2f}', rotation=0)\n",
    "plt.text(median_val + 200, 500, f'  Median: {median_val:.2f}', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load and concatenate CSV files\n",
    "def load_and_concatenate_csv(file_paths):\n",
    "    dataframes = []\n",
    "    step_offset = 0  # Initialize step offset\n",
    "    for file_path in file_paths:\n",
    "        # Use delimiter='\\t' to specify that the values are separated by tabs\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.columns)  # This will print the column names to check them\n",
    "        \n",
    "        # Ensure 'Step' column exists\n",
    "        if 'Step' not in df.columns:\n",
    "            raise ValueError(f\"Column 'Step' not found in {file_path}. Columns found: {df.columns}\")\n",
    "        \n",
    "        if step_offset > 0:  # Adjust 'Step' if it's not the first file\n",
    "            df['Step'] += step_offset\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        step_offset = df['Step'].iloc[-1]  # Update step offset to the last step of the current df\n",
    "\n",
    "    concatenated_df = pd.concat(dataframes)\n",
    "    return concatenated_df\n",
    "\n",
    "# File paths for train and validation CSV files\n",
    "train_csv_files = ['run-transformer_version_218-tag-train_loss_epoch.csv', 'run-transformer_version_220-tag-train_loss_epoch.csv']\n",
    "val_csv_files = ['run-transformer_version_218-tag-val_loss_epoch.csv', 'run-transformer_version_220-tag-val_loss_epoch.csv']\n",
    "\n",
    "# Load and concatenate data\n",
    "train_data = load_and_concatenate_csv(train_csv_files)\n",
    "val_data = load_and_concatenate_csv(val_csv_files)\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_data['Step'], train_data['Value'], label='Train Loss')\n",
    "plt.plot(val_data['Step'], val_data['Value'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_line(line):\n",
    "    # Split each part of the line\n",
    "    parts = line.split(', ')\n",
    "    predicted = parts[0].split(': ')[1]\n",
    "    target = parts[1].split(': ')[1]\n",
    "    edit_distance = float(parts[2].split(': ')[1])\n",
    "    return {'Predicted': predicted, 'Target': target, 'Edit Distance': edit_distance}\n",
    "\n",
    "def read_data(filepath):\n",
    "    data_list = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():\n",
    "                data_list.append(parse_line(line))\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "# Replace 'data.txt' with the path to your text file\n",
    "df = read_data('edit_dists.txt')\n",
    "\n",
    "total_edit_distance = df['Edit Distance'].mean()\n",
    "print(total_edit_distance)\n",
    "\n",
    "mean_target_length = df['Target'].apply(len).mean()\n",
    "print(mean_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read the total amount unique files\n",
    "unique_paths = dataset_df[\"path\"].unique()\n",
    "\n",
    "sum = unique_paths.shape[0]\n",
    "\n",
    "print(\"Total number of files: {}\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "LIP = [\n",
    "    61,\n",
    "    185,\n",
    "    40,\n",
    "    39,\n",
    "    37,\n",
    "    267,\n",
    "    269,\n",
    "    270,\n",
    "    409,\n",
    "    291,\n",
    "    146,\n",
    "    91,\n",
    "    181,\n",
    "    84,\n",
    "    17,\n",
    "    314,\n",
    "    405,\n",
    "    321,\n",
    "    375,\n",
    "    78,\n",
    "    191,\n",
    "    80,\n",
    "    81,\n",
    "    82,\n",
    "    13,\n",
    "    312,\n",
    "    311,\n",
    "    310,\n",
    "    415,\n",
    "    95,\n",
    "    88,\n",
    "    178,\n",
    "    87,\n",
    "    14,\n",
    "    317,\n",
    "    402,\n",
    "    318,\n",
    "    324,\n",
    "    308,\n",
    "]\n",
    "\n",
    "FACE = (\n",
    "    [f\"x_face_{i}\" for i in LIP]\n",
    "    + [f\"y_face_{i}\" for i in LIP]\n",
    "    + [f\"z_face_{i}\" for i in LIP]\n",
    ")\n",
    "LHAND = (\n",
    "    [f\"x_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_left_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "RHAND = (\n",
    "    [f\"x_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_right_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "POSE = (\n",
    "    [f\"x_pose_{i}\" for i in range(0, 23)]\n",
    "    + [f\"y_pose_{i}\" for i in range(0, 23)]\n",
    "    + [f\"z_pose_{i}\" for i in range(0, 23)]\n",
    ")\n",
    "\n",
    "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
    "FRAME_LEN = 384  # 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read the existing data\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)\n",
    "\n",
    "# Define the new entries\n",
    "new_entries = [\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"P\",\n",
    "]\n",
    "\n",
    "# Add the new entries starting from index 59, only if they don't already exist\n",
    "for i, entry in enumerate(new_entries, start=59):\n",
    "    if entry not in json_chars:\n",
    "        json_chars[entry] = i\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(\"character_to_prediction_index.json\", \"w\") as f:\n",
    "    json.dump(json_chars, f, indent=4)\n",
    "\n",
    "start_token_idx = 59\n",
    "end_token_idx = 60\n",
    "pad_token_idx = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Manager, Pool\n",
    "\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")  # Disable GPU for Tensorflow\n",
    "# Create a Manager object for the progress_queue\n",
    "\n",
    "manager = Manager()\n",
    "progress_queue = manager.Queue()\n",
    "\n",
    "\n",
    "def process_file(file_id):\n",
    "    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "    path = file_df[\"path\"].values[0]\n",
    "    parquet_df = pq.read_table(path, columns=[\"sequence_id\"] + SEL_COLS).to_pandas()\n",
    "    features = [FACE, LHAND, RHAND, POSE]\n",
    "    for feature in features:\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        parquet_df[feature] = scaler.fit_transform(parquet_df[feature])\n",
    "    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "    parquet_numpy = parquet_df.to_numpy(copy=False)\n",
    "    col_to_index = {col: i for i, col in enumerate(parquet_df.columns)}\n",
    "    LHAND_indices = [col_to_index[col] for col in LHAND]\n",
    "    RHAND_indices = [col_to_index[col] for col in RHAND]\n",
    "    buffer_size = 1000  # Adjust as needed\n",
    "    buffer = []\n",
    "    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "        for seq_id, phrase in zip(file_df[\"sequence_id\"], file_df[\"phrase\"]):\n",
    "            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "            progress_queue.put(\n",
    "                f\"Process: {mp.current_process().name}, File: {file_id}, Sequence: {seq_id}\"\n",
    "            )\n",
    "            if frames.shape[0] > FRAME_LEN:\n",
    "                itp = interp1d(\n",
    "                    np.linspace(0, 1, len(frames)),\n",
    "                    frames,\n",
    "                    axis=0,\n",
    "                    kind=\"linear\",\n",
    "                    fill_value=\"extrapolate\",\n",
    "                )\n",
    "                # Generate the new index array and apply interpolation\n",
    "                frames = itp(np.linspace(0, 1, FRAME_LEN))\n",
    "            # Calculate the number of NaN values in each hand landmark\n",
    "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_indices]), axis=1) == 0)\n",
    "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_indices]), axis=1) == 0)\n",
    "            no_nan = max(r_nonan, l_nonan)\n",
    "            frames = np.nan_to_num(frames, nan=0)\n",
    "            num_hand_frames = np.sum(\n",
    "                np.any(frames[:, LHAND_indices + RHAND_indices] != 0, axis=1)\n",
    "            )\n",
    "            if frames.shape[0] < 50 and num_hand_frames < 3:\n",
    "                phrase = \"2 a-e -aroe\"\n",
    "            if 2 * len(phrase) < no_nan:\n",
    "                features = {\n",
    "                    COL: tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(\n",
    "                            value=frames[:, col_to_index[COL]]\n",
    "                        )\n",
    "                    )\n",
    "                    for COL in SEL_COLS\n",
    "                }\n",
    "                features[\"phrase\"] = tf.train.Feature(\n",
    "                    bytes_list=tf.train.BytesList(value=[bytes(phrase, \"utf-8\")])\n",
    "                )\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "                record_bytes = example.SerializeToString()\n",
    "                buffer.append(record_bytes)\n",
    "                if len(buffer) == buffer_size:\n",
    "                    for record in buffer:\n",
    "                        file_writer.write(record)\n",
    "                        buffer = []\n",
    "        if buffer:\n",
    "            for record in buffer:\n",
    "                file_writer.write(record)\n",
    "    # gc.collect()\n",
    "\n",
    "\n",
    "cpu_count = int(mp.cpu_count() / 2)\n",
    "cpu_count = 6  # 8\"\"\"  \"\"\"\n",
    "with Pool(cpu_count) as pool:\n",
    "    progress_bars = [\n",
    "        tqdm_notebook(desc=f\"Process {i + 1}\", unit=\"seq\") for i in range(cpu_count)\n",
    "    ]\n",
    "    for result in pool.imap(\n",
    "        process_file,\n",
    "        dataset_df[\"file_id\"].unique(),\n",
    "    ):\n",
    "        progress_updates = []\n",
    "        while not progress_queue.empty():\n",
    "            progress_updates.append(progress_queue.get())\n",
    "        for update, bar in zip(progress_updates, progress_bars):\n",
    "            bar.set_description(update)\n",
    "            bar.update()\n",
    "print(\"All parquets processed to TFRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "\n",
    "def tokenize_string(text):\n",
    "    # Tokenize the string using the provided vocabulary\n",
    "    token_ids = [vocab[char] for char in text if char in vocab]\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def detokenize_batch(batch, INFERENCE=False):\n",
    "    # Create a reverse vocabulary\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Convert the token IDs back to characters for each sequence in the batch\n",
    "    texts = []\n",
    "    for seq in batch:\n",
    "        text = []\n",
    "        for id in seq:\n",
    "            char = reverse_vocab[id.item()]\n",
    "            if INFERENCE and char == \">\":\n",
    "                break  # Stop adding characters when '>' is found during inference\n",
    "            if char == \"<\":\n",
    "                continue\n",
    "            text.append(char)\n",
    "        texts.append(\"\".join(text))\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Encodes phrase into a tensor of tokens\n",
    "def tokenize_phrase(example):\n",
    "    phrase = example[\"phrase\"][0].decode(\n",
    "        \"utf-8\"\n",
    "    )  # Decode the byte string into a regular string\n",
    "    phrase = \"<\" + phrase + \">\"\n",
    "    token_ids = tokenize_string(phrase)\n",
    "    example[\"phrase\"] = torch.tensor(\n",
    "        token_ids\n",
    "    )  # Replace the byte string with a list of integers\n",
    "    return example\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate phrases and sequence lengths\n",
    "    phrases = [seq.pop(\"phrase\") for seq in batch]\n",
    "    landmarks = [seq for seq in batch]\n",
    "\n",
    "    sequence_lengths = [len(next(iter(landmark.values()))) for landmark in landmarks]\n",
    "    phrase_lengths = [len(phrase) for phrase in phrases]\n",
    "\n",
    "    # Pad sequences and phrases\n",
    "    padded_batch = [\n",
    "        torch.stack(\n",
    "            [\n",
    "                F.pad(\n",
    "                    input=tensor,\n",
    "                    pad=(0, FRAME_LEN - tensor.shape[0]),\n",
    "                    mode=\"constant\",\n",
    "                    value=0,\n",
    "                )\n",
    "                for tensor in seq.values()\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        for seq in batch\n",
    "    ]\n",
    "\n",
    "    stacked_landmarks = torch.stack(padded_batch, dim=0)\n",
    "\n",
    "    padded_phrases = [\n",
    "        F.pad(\n",
    "            input=phrase,\n",
    "            pad=(0, 64 - len(phrase)),\n",
    "            mode=\"constant\",\n",
    "            value=61,\n",
    "        )\n",
    "        for phrase in phrases\n",
    "    ]\n",
    "\n",
    "    stacked_phrases = torch.stack(padded_phrases, dim=0)\n",
    "\n",
    "    return (\n",
    "        stacked_landmarks,\n",
    "        stacked_phrases,\n",
    "        torch.tensor(sequence_lengths),\n",
    "        torch.tensor(phrase_lengths),\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the split index\n",
    "tf_records = dataset_df.file_id.map(\n",
    "    lambda x: f\"/home/jpinn/asl-fingerspelling-recognition/src/preprocessed/{x}.tfrecord\"\n",
    ").unique()\n",
    "\n",
    "# Sample 20% of the TFRecords\n",
    "# sample_size = int(0.2 * len(tf_records))  # Calculate 20% of the total records\n",
    "# tf_records = random.sample(list(tf_records), sample_size)\n",
    "\n",
    "split_index = int(0.8 * len(tf_records))\n",
    "tf_records_len = len(tf_records)\n",
    "\n",
    "print(f\"Split index: {split_index}\" f\"\\nTotal number of TFRecords: {tf_records_len}\")\n",
    "\n",
    "\n",
    "def build_pipe(batch_size, drop_last, start, end, shuffle=True):\n",
    "    datapipe = FileLister(tf_records[start:end])\n",
    "\n",
    "    if shuffle:\n",
    "        datapipe = Shuffler(\n",
    "            datapipe, buffer_size=len(tf_records[start:end])\n",
    "        )  # Shuffle the dataset\n",
    "\n",
    "    datapipe = FileOpener(datapipe, mode=\"b\")\n",
    "    datapipe = TFRecordLoader(datapipe)\n",
    "    datapipe = Mapper(datapipe, tokenize_phrase)\n",
    "    datapipe = Batcher(datapipe, batch_size=batch_size, drop_last=drop_last)\n",
    "    datapipe = Collator(datapipe, collate_fn=collate_fn)\n",
    "    return datapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=0,\n",
    "            end=split_index,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "        return DataLoader2(\n",
    "            datapipe=train_datapipe,\n",
    "            reading_service=MultiProcessingReadingService(num_workers=6),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=split_index+1,\n",
    "            end=tf_records_len-5,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return DataLoader2(\n",
    "            datapipe=val_datapipe,\n",
    "            reading_service=MultiProcessingReadingService(num_workers=6),\n",
    "        )\n",
    "        \n",
    "    def predict_dataloader(self):\n",
    "        predict_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=tf_records_len-4,\n",
    "            end=tf_records_len,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return DataLoader2(\n",
    "            datapipe=predict_datapipe,\n",
    "            reading_service=MultiProcessingReadingService(num_workers=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=62, maxlen=64, d_model=312):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.emb = nn.Embedding(num_vocab, d_model, padding_idx=61)\n",
    "        self.pos_emb = nn.Embedding(maxlen, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.shape[-1]\n",
    "\n",
    "        x = self.emb(x)\n",
    "\n",
    "        # Generate positions\n",
    "        positions = torch.arange(start=0, end=maxlen, dtype=torch.long, device=x.device)\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "\n",
    "        pos_emb = pos_emb.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "\n",
    "        return x + pos_emb\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, d_model=312, maxlen=FRAME_LEN, device=\"cuda\", dropout=0.1):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(maxlen, d_model)\n",
    "\n",
    "        # Define Conv1d layers\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=d_model, out_channels=d_model, kernel_size=11, padding=5\n",
    "        )  # padding to maintain sequence length\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=d_model, out_channels=d_model, kernel_size=11, padding=5\n",
    "        )\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=d_model, out_channels=d_model, kernel_size=11, padding=5\n",
    "        )\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(d_model)\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "        self.bn3 = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Move to the specified device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Permute to fit Conv1d input requirements: [batch_size, channels, seq_len]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply Conv1d layers with ReLU activations and batch normalization\n",
    "        x = self.dropout(F.silu(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout(F.silu(self.bn2(self.conv2(x))))\n",
    "        x = self.dropout(F.silu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Permute back to [batch_size, seq_len, features]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Generate positions\n",
    "        positions = torch.arange(\n",
    "            start=0, end=x.shape[1], dtype=torch.long, device=x.device\n",
    "        )\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "\n",
    "        pos_emb = pos_emb.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "\n",
    "        return x + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "class LightningTransformer(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(config)\n",
    "        self.config = config\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.enc_emb = LandmarkEmbedding(config[\"d_model\"], config[\"src_maxlen\"])\n",
    "        self.dec_emb = TokenEmbedding(\n",
    "            config[\"num_classes\"], config[\"tgt_maxlen\"], config[\"d_model\"]\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config[\"d_model\"],\n",
    "            nhead=config[\"nhead\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            num_encoder_layers=config[\"num_encoder_layers\"],\n",
    "            num_decoder_layers=config[\"num_decoder_layers\"],\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.transformer.d_model, 62)\n",
    "\n",
    "        self.metric = EditDistance()\n",
    "        self.predictions_log = []\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=61)\n",
    "\n",
    "    def create_mask(self, batch_size, max_length, real_length):\n",
    "        \"\"\"Create a boolean mask for sequences based on lengths.\"\"\"\n",
    "        key_padding_mask = torch.ones(\n",
    "            (batch_size, max_length), device=self.device, dtype=torch.bfloat16\n",
    "        )\n",
    "        for i, length in enumerate(real_length):\n",
    "            key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return key_padding_mask\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask):\n",
    "        src = self.enc_emb(src)\n",
    "        tgt = self.dec_emb(tgt)\n",
    "        # Encodes source and decodes target sequences\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(\n",
    "            63, dtype=torch.bfloat16, device=self.device\n",
    "        )\n",
    "        output = self.transformer(\n",
    "            src,\n",
    "            tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True,\n",
    "        )\n",
    "        return self.linear(output)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        tgt_input = target[:, :-1]  # Shifted right for input\n",
    "        tgt_output = target[:, 1:]  # Real target without the first token\n",
    "\n",
    "        src_key_padding_mask = self.create_mask(\n",
    "            source.size(0), source.size(1), src_lengths\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_mask(\n",
    "            target.size(0),\n",
    "            target.size(1),\n",
    "            tgt_lengths,\n",
    "        )\n",
    "\n",
    "        # Get model output\n",
    "        output = self(\n",
    "            source, tgt_input, src_key_padding_mask, tgt_key_padding_mask[:, :-1]\n",
    "        )\n",
    "\n",
    "        # Compute loss; CrossEntropyLoss expects outputs of size (N, C, L) and target of size (N, L)\n",
    "        loss = self.loss(output.transpose(1, 2), tgt_output)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        tgt_input = target[:, :-1]  # Shifted right for input\n",
    "        tgt_output = target[:, 1:]  # Real target without the first token\n",
    "\n",
    "        src_key_padding_mask = self.create_mask(\n",
    "            source.size(0), source.size(1), src_lengths\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_mask(\n",
    "            target.size(0),\n",
    "            target.size(1),\n",
    "            tgt_lengths,\n",
    "        )\n",
    "\n",
    "        # Get model output\n",
    "        output = self(\n",
    "            source, tgt_input, src_key_padding_mask, tgt_key_padding_mask[:, :-1]\n",
    "        )\n",
    "\n",
    "        loss = self.loss(output.transpose(1, 2), tgt_output)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        tgt_input = target[:, :-1]  # Shifted right for input\n",
    "\n",
    "        src_key_padding_mask = self.create_mask(\n",
    "            source.size(0), source.size(1), src_lengths\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_mask(\n",
    "            target.size(0),\n",
    "            target.size(1),\n",
    "            tgt_lengths,\n",
    "        )\n",
    "\n",
    "        # Get model output\n",
    "        output = self(\n",
    "            source, tgt_input, src_key_padding_mask, tgt_key_padding_mask[:, :-1]\n",
    "        )\n",
    "\n",
    "        predicted = torch.argmax(output, dim=2)\n",
    "\n",
    "        # Convert tensors to string lists\n",
    "        predicted_strings = detokenize_batch(predicted, INFERENCE=True)\n",
    "\n",
    "        target_strings = detokenize_batch(tgt_input, INFERENCE=True)\n",
    "\n",
    "        edit_pairs = zip(predicted_strings, target_strings)\n",
    "        for pred, tgt in edit_pairs:\n",
    "            distance = self.metric(pred, tgt)\n",
    "            self.predictions_log.append(\n",
    "                {\"predicted\": pred, \"target\": tgt, \"edit_distance\": distance}\n",
    "            )\n",
    "            print(f\"Predicted: {pred}, Target: {tgt}, Edit Distance: {distance}\")\n",
    "\n",
    "    def on_predict_epoch_end(self):\n",
    "        # Save all logged predictions to a file at the end of the prediction epoch\n",
    "        with open(\"predictions_log.csv\", \"w\", newline=\"\") as file:\n",
    "            writer = csv.DictWriter(\n",
    "                file, fieldnames=[\"predicted\", \"target\", \"edit_distance\"]\n",
    "            )\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.predictions_log)\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.transformer.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.config[\"weight_decay\"],\n",
    "            fused=True,\n",
    "        )\n",
    "        scheduler1 = torch.optim.lr_scheduler.ConstantLR(\n",
    "            optimizer, factor=1, total_iters=10\n",
    "        )\n",
    "\n",
    "        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=190)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer, schedulers=[scheduler1, scheduler2], milestones=[10]\n",
    "        )\n",
    "\n",
    "        # Chain scheduler/s\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 200,\n",
    "    \"learning_rate\": 0.00011587773561551261,\n",
    "    \"num_encoder_layers\": 12,\n",
    "    \"num_decoder_layers\": 4,\n",
    "    \"batch_size\": 128,\n",
    "    \"d_model\": 312,\n",
    "    \"nhead\": 4,\n",
    "    \"weight_decay\": 0.08,\n",
    "    \"dropout\":0.2,\n",
    "    \"src_maxlen\": FRAME_LEN,\n",
    "    \"tgt_maxlen\": 64,\n",
    "    \"num_classes\": 62,\n",
    "}\n",
    "\n",
    "# Initialize callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=30,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "# Set up Logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"transformer\")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=200,\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[checkpoint_callback, lr_monitor, early_stop_callback],\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=True,\n",
    "    precision=\"bf16-mixed\",\n",
    "    accumulate_grad_batches=4,\n",
    "    #gradient_clip_val=4,\n",
    "    num_sanity_val_steps=0,\n",
    "    logger=logger,\n",
    ")\n",
    "# Initialize the model\n",
    "\n",
    "model = LightningTransformer(config)\n",
    "\n",
    "model = torch.compile(model)\n",
    "\n",
    "data_module = LightningDataModule(batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = tuner.lr_find(model, data_module, num_training=500, mode=\"exponential\")\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "if lr_finder:\n",
    "    model.learning_rate = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lightning Training the model...\")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(model, data_module, ckpt_path=\"checkpoints/best-checkpoint-v45.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
