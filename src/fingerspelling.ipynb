{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 20:57:44.610247: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-01 20:57:44.658149: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-01 20:57:44.658193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-01 20:57:44.662832: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-01 20:57:44.681175: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 20:57:45.519398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "from torchdata.dataloader2 import DataLoader2, MultiProcessingReadingService\n",
    "from torchdata.datapipes.iter import (\n",
    "    FileLister,\n",
    "    FileOpener,\n",
    "    TFRecordLoader,\n",
    "    Mapper,\n",
    "    Batcher,\n",
    "    Collator,\n",
    "    Shuffler,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\n",
    "    \"medium\"\n",
    ")  # Set the default matmul precision to medium, or high/highest?\n",
    "\n",
    "\n",
    "# Read the first CSV file\n",
    "dataset_train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "dataset_supplemental_df = pd.read_csv(\"supplemental_metadata.csv\")\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "dataset_df = pd.concat([dataset_train_df, dataset_supplemental_df], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV file\n",
    "# dataset_df.to_csv(\"train_full.csv\", index=False)\n",
    "dataset_df = dataset_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: train_landmarks/5414471.parquet, sequence_id: 1816796431, file_id: 5414471, phrase: 3 creekhouse\n",
      "Full sequence dataset shape is (123, 1630)\n"
     ]
    }
   ],
   "source": [
    "# Read the first row of the DataFrame\n",
    "path, sequence_id, file_id, phrase = dataset_df.iloc[0][\n",
    "    [\"path\", \"sequence_id\", \"file_id\", \"phrase\"]\n",
    "]\n",
    "print(f\"path: {path}, sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "sample_sequence_df = pq.read_table(\n",
    "    f\"{str(path)}\",\n",
    "    filters=[\n",
    "        [(\"sequence_id\", \"=\", sequence_id)],\n",
    "    ],\n",
    ").to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 68\n"
     ]
    }
   ],
   "source": [
    "# Read the total amount unique files\n",
    "unique_paths = dataset_df[\"path\"].unique()\n",
    "\n",
    "sum = unique_paths.shape[0]\n",
    "\n",
    "print(\"Total number of files: {}\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIP = [\n",
    "    61,\n",
    "    185,\n",
    "    40,\n",
    "    39,\n",
    "    37,\n",
    "    267,\n",
    "    269,\n",
    "    270,\n",
    "    409,\n",
    "    291,\n",
    "    146,\n",
    "    91,\n",
    "    181,\n",
    "    84,\n",
    "    17,\n",
    "    314,\n",
    "    405,\n",
    "    321,\n",
    "    375,\n",
    "    78,\n",
    "    191,\n",
    "    80,\n",
    "    81,\n",
    "    82,\n",
    "    13,\n",
    "    312,\n",
    "    311,\n",
    "    310,\n",
    "    415,\n",
    "    95,\n",
    "    88,\n",
    "    178,\n",
    "    87,\n",
    "    14,\n",
    "    317,\n",
    "    402,\n",
    "    318,\n",
    "    324,\n",
    "    308,\n",
    "]\n",
    "\n",
    "FACE = (\n",
    "    [f\"x_face_{i}\" for i in LIP]\n",
    "    + [f\"y_face_{i}\" for i in LIP]\n",
    "    + [f\"z_face_{i}\" for i in LIP]\n",
    ")\n",
    "LHAND = (\n",
    "    [f\"x_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_left_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "RHAND = (\n",
    "    [f\"x_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_right_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "POSE = (\n",
    "    [f\"x_pose_{i}\" for i in range(33)]\n",
    "    + [f\"y_pose_{i}\" for i in range(33)]\n",
    "    + [f\"z_pose_{i}\" for i in range(33)]\n",
    ")\n",
    "\n",
    "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
    "FRAME_LEN = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the existing data\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)\n",
    "\n",
    "# Define the new entries\n",
    "new_entries = [\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"P\",\n",
    "]\n",
    "\n",
    "# Add the new entries starting from index 59, only if they don't already exist\n",
    "for i, entry in enumerate(new_entries, start=59):\n",
    "    if entry not in json_chars:\n",
    "        json_chars[entry] = i\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(\"character_to_prediction_index.json\", \"w\") as f:\n",
    "    json.dump(json_chars, f, indent=4)\n",
    "\n",
    "start_token_idx = 59\n",
    "end_token_idx = 60\n",
    "pad_token_idx = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf.config.set_visible_devices([], \"GPU\")  # Disable GPU for Tensorflow\n",
    "#\n",
    "## Create a Manager object for the progress_queue\n",
    "#manager = Manager()\n",
    "#progress_queue = manager.Queue()\n",
    "#\n",
    "#\n",
    "#def process_file(file_id):\n",
    "#    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "#    path = file_df[\"path\"].values[0]\n",
    "#    parquet_df = pq.read_table(path, columns=[\"sequence_id\"] + SEL_COLS).to_pandas()\n",
    "#\n",
    "#    parquet_df = parquet_df.fillna(0)\n",
    "#\n",
    "#    scalerFACE = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerLHAND = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerRHAND = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerPOSE = StandardScaler(with_mean=True, with_std=True)\n",
    "#\n",
    "#    parquet_df[FACE] = scalerFACE.fit_transform(\n",
    "#        parquet_df[FACE],\n",
    "#    )\n",
    "#    parquet_df[LHAND] = scalerLHAND.fit_transform(\n",
    "#        parquet_df[LHAND],\n",
    "#    )\n",
    "#    parquet_df[RHAND] = scalerRHAND.fit_transform(\n",
    "#        parquet_df[RHAND],\n",
    "#    )\n",
    "#    parquet_df[POSE] = scalerPOSE.fit_transform(\n",
    "#        parquet_df[POSE],\n",
    "#    )\n",
    "#\n",
    "#    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "#    parquet_numpy = parquet_df.to_numpy(copy=False)\n",
    "#    col_to_index = {col: i for i, col in enumerate(parquet_df.columns)}\n",
    "#    LHAND_indices = [col_to_index[col] for col in LHAND]\n",
    "#    RHAND_indices = [col_to_index[col] for col in RHAND]\n",
    "#    buffer_size = 1000  # Adjust as needed\n",
    "#    buffer = []\n",
    "#\n",
    "#    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "#        for seq_id, phrase in zip(file_df[\"sequence_id\"], file_df[\"phrase\"]):\n",
    "#            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "#            progress_queue.put(\n",
    "#                f\"Process: {mp.current_process().name}, File: {file_id}, Sequence: {seq_id}\"\n",
    "#            )\n",
    "#\n",
    "#            current_length, num_features = frames.shape\n",
    "#\n",
    "#            if current_length > FRAME_LEN:\n",
    "#                itp = interp1d(\n",
    "#                    np.linspace(0, 1, current_length),\n",
    "#                    frames,\n",
    "#                    axis=0,\n",
    "#                    kind=\"linear\",\n",
    "#                    fill_value=\"extrapolate\",\n",
    "#                )\n",
    "#                # Generate the new index array and apply interpolation\n",
    "#                new_index = np.linspace(0, 1, FRAME_LEN)\n",
    "#                frames = itp(new_index)\n",
    "#\n",
    "#            # Calculate the number of NaN values in each hand landmark\n",
    "#            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_indices]), axis=1) == 0)\n",
    "#            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_indices]), axis=1) == 0)\n",
    "#            no_nan = max(r_nonan, l_nonan)\n",
    "#\n",
    "#            if 2 * len(phrase) < no_nan:\n",
    "#                features = {\n",
    "#                    COL: tf.train.Feature(\n",
    "#                        float_list=tf.train.FloatList(\n",
    "#                            value=frames[:, col_to_index[COL]]\n",
    "#                        )\n",
    "#                    )\n",
    "#                    for COL in SEL_COLS\n",
    "#                }\n",
    "#                features[\"phrase\"] = tf.train.Feature(\n",
    "#                    bytes_list=tf.train.BytesList(value=[bytes(phrase, \"utf-8\")])\n",
    "#                )\n",
    "#\n",
    "#                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "#                record_bytes = example.SerializeToString()\n",
    "#\n",
    "#                buffer.append(record_bytes)\n",
    "#                if len(buffer) == buffer_size:\n",
    "#                    for record in buffer:\n",
    "#                        file_writer.write(record)\n",
    "#                        buffer = []\n",
    "#        if buffer:\n",
    "#            for record in buffer:\n",
    "#                file_writer.write(record)\n",
    "#\n",
    "#        # gc.collect()\n",
    "#\n",
    "#\n",
    "##cpu_count = int(mp.cpu_count() / 2)\n",
    "#cpu_count = 8  # 8\n",
    "#\n",
    "#\n",
    "#with Pool(cpu_count) as pool:\n",
    "#    progress_bars = [\n",
    "#        tqdm_notebook(desc=f\"Process {i + 1}\", unit=\"seq\") for i in range(cpu_count)\n",
    "#    ]\n",
    "#\n",
    "#    for result in pool.imap(\n",
    "#        process_file,\n",
    "#        dataset_df[\"file_id\"].unique(),\n",
    "#    ):\n",
    "#        progress_updates = []\n",
    "#        while not progress_queue.empty():\n",
    "#            progress_updates.append(progress_queue.get())\n",
    "#        for update, bar in zip(progress_updates, progress_bars):\n",
    "#            bar.set_description(update)\n",
    "#            bar.update()\n",
    "#\n",
    "#print(\"All parquets processed to TFRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)  #\n",
    "\n",
    "\n",
    "# Encodes phrase into a tensor of tokens\n",
    "def tokenize_phrase(example):\n",
    "    phrase = example[\"phrase\"][0].decode(\n",
    "        \"utf-8\"\n",
    "    )  # Decode the byte string into a regular string\n",
    "    phrase = \"<\" + phrase + \">\"\n",
    "    indices = [json_chars.get(char, json_chars.get(\"F\")) for char in phrase]\n",
    "    example[\"phrase\"] = torch.tensor(\n",
    "        indices\n",
    "    )  # Replace the byte string with a list of integers\n",
    "    return example\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate phrases and sequence lengths\n",
    "    phrases = [seq.pop(\"phrase\") for seq in batch]\n",
    "    landmarks = [seq for seq in batch]\n",
    "\n",
    "    sequence_lengths = [len(next(iter(landmark.values()))) for landmark in landmarks]\n",
    "    phrase_lengths = [len(phrase) for phrase in phrases]\n",
    "\n",
    "    # Pad sequences and phrases\n",
    "    padded_batch = [\n",
    "        torch.stack(\n",
    "            [\n",
    "                F.pad(\n",
    "                    input=tensor,\n",
    "                    pad=(0, FRAME_LEN - tensor.shape[0]),\n",
    "                    mode=\"constant\",\n",
    "                    value=0,\n",
    "                )\n",
    "                for tensor in seq.values()\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        for seq in batch\n",
    "    ]\n",
    "\n",
    "    stacked_landmarks = torch.stack(padded_batch, dim=0)\n",
    "\n",
    "    padded_phrases = [\n",
    "        F.pad(\n",
    "            input=phrase,\n",
    "            pad=(0, 64 - len(phrase)),\n",
    "            mode=\"constant\",\n",
    "            value=61,\n",
    "        )\n",
    "        for phrase in phrases\n",
    "    ]\n",
    "\n",
    "    stacked_phrases = torch.stack(padded_phrases, dim=0)\n",
    "\n",
    "    return (\n",
    "        stacked_landmarks,\n",
    "        stacked_phrases,\n",
    "        torch.tensor(sequence_lengths),\n",
    "        torch.tensor(phrase_lengths),\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the split index\n",
    "tf_records = dataset_df.file_id.map(\n",
    "    lambda x: f\"/home/jpinn/asl-fingerspelling-recognition/src/preprocessed/{x}.tfrecord\"\n",
    ").unique()\n",
    "split_index = int(0.8 * len(tf_records))\n",
    "\n",
    "\n",
    "def build_pipe(batch_size, drop_last, start, end, shuffle=True):\n",
    "    datapipe = FileLister(tf_records[start:end])\n",
    "\n",
    "    if shuffle:\n",
    "        datapipe = Shuffler(\n",
    "            datapipe, buffer_size=len(tf_records[start:end])\n",
    "        )  # Shuffle the dataset\n",
    "\n",
    "    datapipe = FileOpener(datapipe, mode=\"b\")\n",
    "    datapipe = TFRecordLoader(datapipe)\n",
    "    datapipe = Mapper(datapipe, tokenize_phrase)\n",
    "    datapipe = Batcher(datapipe, batch_size=batch_size, drop_last=drop_last)\n",
    "    datapipe = Collator(datapipe, collate_fn=collate_fn)\n",
    "    return datapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=None, maxlen=None, num_hid=None):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.emb = nn.Embedding(num_embeddings=num_vocab, embedding_dim=num_hid)\n",
    "        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(1)\n",
    "        x = x.to(torch.int64)\n",
    "        x = self.emb(x)\n",
    "        x = x * torch.sqrt(\n",
    "            torch.tensor(self.num_hid, dtype=torch.float, device=x.device)\n",
    "        )\n",
    "\n",
    "        return x + self.pos_emb[:maxlen, :]\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.to(\"cuda:0\")\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, num_hid=342, maxlen=453):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=342,  # Correctly handle 342 landmark features\n",
    "                out_channels=num_hid,\n",
    "                kernel_size=11,\n",
    "                padding=5,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n",
    "        self.maxlen = maxlen\n",
    "        self.num_hid = num_hid\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Permute the tensor to have channels as the second dimension\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Change from [batch_size, seq_len, features] to [batch_size, features, seq_len]\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Optionally permute back if needed for further processing\n",
    "\n",
    "        x = x * torch.sqrt(torch.tensor(self.num_hid, dtype=torch.float).to(x.device))\n",
    "        return x + self.pos_emb[: x.size(1), :].to(x.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        src2 = self.linear2(self.dropout2(self.linear1(src)))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            d_model, nhead, batch_first=True\n",
    "        )  # Encoder-decoder attention\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def causal_attention_mask(batch_size, seq_len, device):\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
    "        ).bool()\n",
    "        return mask[None, :, :].expand(batch_size * 4, seq_len, seq_len)\n",
    "\n",
    "    def forward(\n",
    "        self, enc_out, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        batch_size, seq_len, _ = target.size()\n",
    "\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, target.device)\n",
    "\n",
    "        tgt_key_padding_mask = tgt_key_padding_mask[:, :seq_len]\n",
    "\n",
    "        # Self-attention and layer norm\n",
    "        target_att, _ = self.self_attn(\n",
    "            target,\n",
    "            target,\n",
    "            target,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        target = self.norm1(target + self.dropout1(target_att))\n",
    "\n",
    "        # Encoder-decoder attention and layer norm\n",
    "        enc_att_output, _ = self.multihead_attn(\n",
    "            target, enc_out, enc_out, key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        target = self.norm2(target + self.dropout2(enc_att_output))\n",
    "\n",
    "        # Feed forward network and layer norm\n",
    "        ffn_output = self.linear2(self.dropout(F.relu(self.linear1(target))))\n",
    "        target = self.norm3(target + self.dropout3(ffn_output))\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid,\n",
    "        num_head,\n",
    "        num_feed_forward,\n",
    "        source_maxlen,\n",
    "        target_maxlen,\n",
    "        num_layers_enc,\n",
    "        num_layers_dec,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_emb = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_emb = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_dec)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(num_hid, num_classes)\n",
    "\n",
    "    def decode(\n",
    "        self, enc_out, target, src_key_padding_mask=None, target_padding_mask=None\n",
    "    ):\n",
    "        y = self.dec_emb(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = self.decoder_layers[i](\n",
    "                enc_out, y, src_key_padding_mask, target_padding_mask\n",
    "            )\n",
    "        return y\n",
    "\n",
    "    def forward(\n",
    "        self, source, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        source = self.enc_emb(source)\n",
    "        memory = source\n",
    "        for layer in self.encoder:\n",
    "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decode(memory, target, src_key_padding_mask, tgt_key_padding_mask)\n",
    "        return self.classifier(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).float()\n",
    "        mask = mask.masked_fill(mask == 0, float(\"-inf\"))  # -inf for future positions\n",
    "        return mask.unsqueeze(0)  # Add batch dimension (1)\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        batch_size = source.size(0)\n",
    "        enc_output = self.encoder(source)\n",
    "\n",
    "        dec_input = torch.ones(batch_size, 1, dtype=torch.long) * target_start_token_idx\n",
    "        for _ in range(self.target_maxlen - 1):\n",
    "            tgt_mask = self.generate_square_subsequent_mask(dec_input.size(1)).to(\n",
    "                source.device\n",
    "            )\n",
    "            dec_out = self.decode(enc_output, dec_input, tgt_mask=tgt_mask)\n",
    "            prediction = self.classifier(dec_out[:, -1])\n",
    "            pred_idx = torch.argmax(prediction, dim=1, keepdim=True)\n",
    "            dec_input = torch.cat([dec_input, pred_idx], dim=1)\n",
    "\n",
    "        return dec_input.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    ")\n",
    "\n",
    "\n",
    "def reading_service_fn(workers):\n",
    "    return MultiProcessingReadingService(num_workers=workers)\n",
    "\n",
    "\n",
    "class TransformerModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(config)\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.learning_rate = config[\"lr\"]\n",
    "        self.transformer = Transformer(\n",
    "            num_hid=200,\n",
    "            num_head=4,\n",
    "            num_feed_forward=400,\n",
    "            source_maxlen=384,\n",
    "            target_maxlen=64,\n",
    "            num_layers_enc=12,\n",
    "            num_layers_dec=2,\n",
    "            num_classes=62,\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=61)\n",
    "\n",
    "    def forward(\n",
    "        self, source, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        return self.transformer(\n",
    "            source,\n",
    "            target,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "\n",
    "    def create_tgt_mask(self, tgt_lengths, batch_size, seq_length):\n",
    "        \"\"\"Create a boolean mask for target sequences based on lengths.\"\"\"\n",
    "        tgt_key_padding_mask = torch.ones(\n",
    "            (batch_size, seq_length), device=self.device, dtype=torch.bool\n",
    "        )\n",
    "        for i, length in enumerate(tgt_lengths):\n",
    "            tgt_key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return tgt_key_padding_mask\n",
    "\n",
    "    def create_src_mask(self, src_lengths, batch_size, seq_length):\n",
    "        \"\"\"Create a boolean mask for source sequences based on lengths.\"\"\"\n",
    "        src_key_padding_mask = torch.ones(\n",
    "            (batch_size, seq_length), device=self.device, dtype=torch.bool\n",
    "        )\n",
    "        for i, length in enumerate(src_lengths):\n",
    "            src_key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return src_key_padding_mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        src_key_padding_mask = self.create_src_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_src_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        output = self.forward(\n",
    "            source,\n",
    "            target[:, :-1],\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        loss = self.criterion(output.transpose(1, 2), target[:, 1:])\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = val_batch\n",
    "\n",
    "        src_key_padding_mask = self.create_src_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_src_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        output = self.forward(\n",
    "            source,\n",
    "            target[:, :-1],\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        loss = self.criterion(output.transpose(1, 2), target[:, 1:])\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        predicted = torch.argmax(output, dim=2)\n",
    "        correct = (predicted == target[:, 1:]).float()\n",
    "        accuracy = correct.sum() / correct.numel()\n",
    "        self.log(\"val_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        training_steps = 1584\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.transformer.parameters(), lr=0.0045, weight_decay=0.08\n",
    "        )\n",
    "\n",
    "        # Create OneCycleLR scheduler using suggested base_lr\n",
    "        # scheduler = {\n",
    "        #    \"scheduler\": torch.optim.lr_scheduler.OneCycleLR(\n",
    "        #        optimizer, max_lr=0.1, steps_per_epoch=792, epochs=100\n",
    "        #    ),\n",
    "        #    \"interval\": \"step\",\n",
    "        # }\n",
    "\n",
    "        # Continue with cosine annealing after warmup\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "        # Chain schedulers\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=0,\n",
    "            end=split_index,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return DataLoader2(train_datapipe, reading_service=reading_service_fn(8))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=split_index,\n",
    "            end=len(tf_records),\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return DataLoader2(datapipe=val_datapipe, reading_service=reading_service_fn(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jpinn/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/jpinn/asl-fingerspelling-recognition/src/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | transformer | Transformer      | 6.5 M \n",
      "1 | criterion   | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "25.964    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4766777a6083499190c44b274a004b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224c7ae50c7b44d6a1f787fadb2e57f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f2fff03f994054a8bc805790afe17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 3.300\n",
      "Epoch 0, global step 792: 'val_loss' reached 3.30011 (best 3.30011), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec4ea9a9f29403e802b2c15c33a25a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.144 >= min_delta = 0.0. New best score: 3.156\n",
      "Epoch 1, global step 1584: 'val_loss' reached 3.15587 (best 3.15587), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f63cbd28bd4f019ab5ef7530101da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.500 >= min_delta = 0.0. New best score: 2.656\n",
      "Epoch 2, global step 2376: 'val_loss' reached 2.65556 (best 2.65556), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16f6d76a49d4a2c93b16e14b9bfeff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 3168: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b8bf81ebe7457eaa1b1b0b5ed754ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.098 >= min_delta = 0.0. New best score: 2.558\n",
      "Epoch 4, global step 3960: 'val_loss' reached 2.55793 (best 2.55793), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ffa09cf6574b1396ab17469dae73c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.067 >= min_delta = 0.0. New best score: 2.491\n",
      "Epoch 5, global step 4752: 'val_loss' reached 2.49082 (best 2.49082), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b3af0854384a5da4c252c1b11f504c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 5544: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb4418e2a09457a89a2448b495f5fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 6336: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ab381e1045482598e1161e6ed138b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 7128: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112381eeb2a74209a9c6ba5c3c0b0aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 7920: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f4421a6b754b7e8fa9d33f66edcd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 8712: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7d10276cce47e5808f1920adc60737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 9504: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d39585756647a4a8749b3f8105db65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 10296: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cb6adce43548ea99c7b70dc0822f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 11088: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b682052d74e342489f7edb3001836307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 11880: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ea43fcbe96431bb06f8bd6b0da7c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 12672: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5fcb40834d409587fda369eabaf53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.048 >= min_delta = 0.0. New best score: 2.443\n",
      "Epoch 16, global step 13464: 'val_loss' reached 2.44255 (best 2.44255), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0023569410643a5bf22ac8d44fcb7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.092 >= min_delta = 0.0. New best score: 2.350\n",
      "Epoch 17, global step 14256: 'val_loss' reached 2.35013 (best 2.35013), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf495ead749f418ea4796c74e682189a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 15048: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94f54287614459899eb2ca9c7e9b03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 2.318\n",
      "Epoch 19, global step 15840: 'val_loss' reached 2.31842 (best 2.31842), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd0c9afa5e14337b7345163186bee25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 16632: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c81366c7cc456d8867cb589faa1636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 17424: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a758e2f1906f4f30aae3802a3e55640d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.093 >= min_delta = 0.0. New best score: 2.225\n",
      "Epoch 22, global step 18216: 'val_loss' reached 2.22543 (best 2.22543), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bae31822c56414b8aa6803a396824f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.029 >= min_delta = 0.0. New best score: 2.196\n",
      "Epoch 23, global step 19008: 'val_loss' reached 2.19620 (best 2.19620), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eac2b010865488a9b20778de34116db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 19800: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baef7e04333b489ba4cd38746c9479da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 2.188\n",
      "Epoch 25, global step 20592: 'val_loss' reached 2.18818 (best 2.18818), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39af341bb98b403c9d599e900a15ba6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 2.173\n",
      "Epoch 26, global step 21384: 'val_loss' reached 2.17288 (best 2.17288), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd0cd1fd4ab46c09d44c62bd92e1b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 2.166\n",
      "Epoch 27, global step 22176: 'val_loss' reached 2.16612 (best 2.16612), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc627c590f44744b9528390fce0eb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 2.160\n",
      "Epoch 28, global step 22968: 'val_loss' reached 2.15965 (best 2.15965), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d296ec8d964247eebaf87533e77e6d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 2.156\n",
      "Epoch 29, global step 23760: 'val_loss' reached 2.15634 (best 2.15634), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf95405814f4eaaaeb5f976de02098d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 2.155\n",
      "Epoch 30, global step 24552: 'val_loss' reached 2.15503 (best 2.15503), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8df22cab323496885b0032537359717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.0. New best score: 2.149\n",
      "Epoch 31, global step 25344: 'val_loss' reached 2.14861 (best 2.14861), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v4.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d13777cf2b24152868c8aa82452f053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 26136: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf644c0afdbf49eea66c9d871cdb8c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 26928: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15602cb866d6419194beb9377b599874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 27720: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdac6fba9b784aa5b056a490ab880341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 28512: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7f3c0873354d5d9176f5980a5604bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 29304: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b230c6d146204dd5b17c0811f4dbe5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 30096: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877571b73c4c4187859d7cbea613bcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 30888: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d1a8b3a88e4a8084348700de400653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39, global step 31680: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b543daa0922f48898bf52d6086c26d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40, global step 32472: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41103d99167c45dd9aab802fb8e209d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, global step 33264: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b16570eed6645f4ad91d6205e1821d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42, global step 34056: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b5276536aa4242a94ae735a6a3cb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43, global step 34848: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fce46a1d5d640a4bc65804bdfa298ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44, global step 35640: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95535a9d037949d19d064200468f7765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45, global step 36432: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554456b3c5c644ed97fc9471860480e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, global step 37224: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7124a4012cb42dda42f28e3e0d83dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47, global step 38016: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ce442c23574e9fb1609dade36905e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48, global step 38808: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0d4dd24a824e8c9458c4f801d2c98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49, global step 39600: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7b6dba68ce41aeb6ef76662d1120d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50, global step 40392: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdb41c92f6844e6b186029acb68a361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51, global step 41184: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aaa8519ea7e40b983e6169681c18377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52, global step 41976: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c449e4a5ff4164a6092b3c0392ac81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53, global step 42768: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361f9368d51d40768da0b602402307d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54, global step 43560: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2cefda804f48d98cc667530c656ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55, global step 44352: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff4f59c8e8d47a6aebc1acf09ac2ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56, global step 45144: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5735ff6f0b2443a6ae4e878f4bde2a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57, global step 45936: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2077655e839e49ee8b2986ced7cd0e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58, global step 46728: 'val_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    config = {\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.0045,\n",
    "        \"batch_size\": 256,\n",
    "    }\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TransformerModule(config)\n",
    "\n",
    "    # Initialize callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=30,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    # Set up Logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"transformer\")\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config[\"epochs\"],\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        callbacks=[checkpoint_callback, lr_monitor, early_stop_callback],\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        enable_checkpointing=True,\n",
    "        precision=\"bf16-mixed\",\n",
    "        accumulate_grad_batches=2,\n",
    "        gradient_clip_val=4,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Create Tuner\n",
    "    # tuner = Tuner(trainer)\n",
    "\n",
    "    ## Tune Learning Rate\n",
    "    # lr_find_result = tuner.lr_find(model, num_training=200, min_lr=1e-6, max_lr=1e-2)\n",
    "    # if lr_find_result:\n",
    "    #    model.learning_rate = lr_find_result.suggestion()\n",
    "\n",
    "    #\n",
    "    ## Start training with tuned parameters\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the idx_to_char mapping from the JSON file\n",
    "with open(\"idx_to_char.json\", \"r\") as f:\n",
    "    idx_to_char = json.load(f)\n",
    "\n",
    "# JSON keys are always strings, so you might need to convert them back to integers\n",
    "idx_to_char = {int(k): v for k, v in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_predictions(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    start_token_idx,\n",
    "    end_token_idx,\n",
    "    idx_to_char,\n",
    "    num_samples=10,\n",
    "):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    preds_list = []\n",
    "    ground_truth_list = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            source, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            # Assuming your model has a generate function similar to model.generate\n",
    "            # Adjust as necessary if your generation process is different\n",
    "            preds = model.generate(source, start_token_idx).cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "\n",
    "            bs = source.size(0)  # Batch size\n",
    "\n",
    "            for i in range(bs):\n",
    "                target_text = \"\".join(\n",
    "                    [idx_to_char[_] for _ in target[i, :] if _ != end_token_idx]\n",
    "                )\n",
    "                ground_truth_list.append(target_text)\n",
    "\n",
    "                prediction = \"\"\n",
    "                for idx in preds[i, :]:\n",
    "                    if idx == end_token_idx:\n",
    "                        break\n",
    "                    prediction += idx_to_char[idx]\n",
    "                preds_list.append(prediction)\n",
    "\n",
    "            if (\n",
    "                batch_idx >= num_samples - 1\n",
    "            ):  # Only sample a few batches for demonstration\n",
    "                break\n",
    "\n",
    "    # Print predictions\n",
    "    for i in range(min(num_samples, len(preds_list))):\n",
    "        print(f\"Ground Truth: {ground_truth_list[i]}\")\n",
    "        print(f\"Prediction: {preds_list[i]}\")\n",
    "        print(\"\\n~~~\\n\")\n",
    "\n",
    "\n",
    "# Call the function with your model, validation dataloader, and other necessary parameters\n",
    "generate_predictions(transformer, val_dataloader, device, 58, 59, idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Note, Reference, Brainstorm\n",
    "\n",
    "https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "\n",
    "## Input Embedding Layer\n",
    "\n",
    "The phrase must be vectorized (our case is chars)\n",
    "We must add Positional Encoding to create Positional Input Embeddings\n",
    "\n",
    "## Encoder Layer\n",
    "\n",
    "Two sub-modules:\n",
    "\n",
    "### Attention\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "3 Distinct fully connected layers\n",
    "\n",
    "- Query, Key, Value\n",
    "- A dot product of the Query and Key matrices is computed to create a score matrix.\n",
    "- The score matrix a table that dictatates how much value each word or char should be given, compared to the other words or chars in the input sequence. Higher score = more important. = more focus.\n",
    "- The score matrix is normalized by dividing by the square root of the dimension of the key vectors.\n",
    "- The score matrix is divided by the square root of the dimension of the key vectors which gives the scaled scores.\n",
    "- The scaled scores are then passed through a softmax function to receive the attention weights.\n",
    "- Multiply attention weights by value matrix to get the output vector of the self-attention layer.\n",
    "- Linear layer to process.\n",
    "\n",
    "#### Multi-headed Attention\n",
    "\n",
    "The query, key, and value is split into N heads.\n",
    "\n",
    "- Each vector goes through the attention layer as normal\n",
    "- The output of all heads is concatenated into a single vector\n",
    "- Each head is given a different representation of the input sequence, allowing the model to simultaneously attend to information from different representation subspaces.\n",
    "- Each head in theory should learn to attend to different parts of the input sequence, and thus overall learn more of the input sequence.\n",
    "\n",
    "### Residual Connection, Layer Normalization, and Feed Forward Layer\n",
    "\n",
    "- The Multi-headed attention output vector is added to the original input vector to the sub-layer, which is called a residual connection.\n",
    "- This output is then normalized by layer normalization.\n",
    "- This enters a feed forward network, which is a simple 2 layer fully connected network with a ReLU activation in between.\n",
    "- The output of that is added again to the original input, to be normalized again. Like before with the multi-headed attention.\n",
    "\n",
    "## Decoder Layer\n",
    "\n",
    "### Output Embedding Layer and Positional Encoding\n",
    "\n",
    "- The output goes through an embedding layer to get the position embeddings.\n",
    "- This enters the first multi-headed attention layer.\n",
    "- The scaled scores are added to a look ahead mask, which prevents the decoder from attending to future tokens.\n",
    "- This happens when the softmax makes future tokens 0, so no attention is given to them.\n",
    "- All the heads are combined to create a masked output layer.\n",
    "\n",
    "The second multi-headed attention layer has the query and key of the encoder output, and the value of the previous multi-headed attention layer output value.\n",
    "\n",
    "A final feed forward network is applied to the output of the second multi-headed attention layer.\n",
    "\n",
    "### Classifier, softmax, and output\n",
    "\n",
    "The feed forward output enters a linear classifier.\n",
    "\n",
    "Classifier output enters softmax to get the probability score of each class (char). The index of the highest probability is the predicted char.\n",
    "\n",
    "Output is added to a list of decoded outputs until the end token is reached.\n",
    "\n",
    "This whole structure can be stacked N layers high. The output of the final encoder, is input into all the decoder layers, who are taking the input from the previous decoder layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "```python\n",
    "import tensorflow_addons as tfa\n",
    "pbar = tfa.callbacks.TQDMProgressBar()\n",
    "model.fit(…,callbacks=[pbar])\n",
    "# TQDMProgressBar() also works with evaluate()\n",
    "model.evaluate(…,callbacks=[pb\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Multiprocessing\n",
    "\n",
    "```python\n",
    "with Pool(workers) as pool:\n",
    "    results = list(tqdm(pool.imap(worker,thread_list, total=len(thread_list))\n",
    "                        ar])\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Padding strategies\n",
    "\n",
    "1. No Padding with <EOS> token: This is the most efficient and elegant approach for Transformers.\n",
    "\n",
    "2. Full Padding: Pad all phrases (and potentially feature sequences) to a fixed maximum length using a constant value or specific technique. This can be simpler to implement but introduces unnecessary computational overhead and potential information distortion due to large padding sections.\n",
    "\n",
    "3. Full Padding with Masking: This combines the simplicity of full padding with the benefits of masking. While you pad all sequences to a fixed length, you apply masking during training to prevent the model from attending to the padded regions. This can be a good compromise if your model struggles with highly variable sequence lengths but you still want to avoid the downsides of excessive padding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
