{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read the first CSV file\n",
    "dataset_train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "dataset_supplemental_df = pd.read_csv(\"supplemental_metadata.csv\")\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "dataset_df = pd.concat([dataset_train_df, dataset_supplemental_df], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV file\n",
    "dataset_df.to_csv(\"train_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first row of the DataFrame\n",
    "path, sequence_id, file_id, phrase = dataset_df.iloc[0][\n",
    "    [\"path\", \"sequence_id\", \"file_id\", \"phrase\"]\n",
    "]\n",
    "print(f\"path: {path}, sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "sample_sequence_df = pq.read_table(\n",
    "    f\"{str(path)}\",\n",
    "    filters=[\n",
    "        [(\"sequence_id\", \"=\", sequence_id)],\n",
    "    ],\n",
    ").to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the total amount unique files\n",
    "unique_paths = dataset_df[\"path\"].unique()\n",
    "\n",
    "sum = unique_paths.shape[0]\n",
    "\n",
    "print(\"Total number of files: {}\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "\n",
    "FACE = [f'x_face_{i}' for i in LIP] + [f'y_face_{i}' for i in LIP] + [f'z_face_{i}' for i in LIP]\n",
    "LHAND = [f'x_left_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)]\n",
    "RHAND = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n",
    "POSE = [f'x_pose_{i}' for i in range(33)] + [f'y_pose_{i}' for i in range(33)] + [f'z_pose_{i}' for i in range(33)]\n",
    "\n",
    "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
    "FRAME_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the existing data\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)\n",
    "\n",
    "# Define the new entries\n",
    "new_entries = [\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"P\",\n",
    "]\n",
    "\n",
    "# Add the new entries starting from index 59, only if they don't already exist\n",
    "for i, entry in enumerate(new_entries, start=59):\n",
    "    if entry not in json_chars:\n",
    "        json_chars[entry] = i\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(\"character_to_prediction_index.json\", \"w\") as f:\n",
    "    json.dump(json_chars, f, indent=4)\n",
    "    \n",
    "pad_token_idx = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")  # Disable GPU for Tensorflow\n",
    "\n",
    "# Create a Manager object for the progress_queue\n",
    "manager = Manager()\n",
    "progress_queue = manager.Queue()\n",
    "\n",
    "\n",
    "def process_file(file_id):\n",
    "    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "    path = file_df[\"path\"].values[0]\n",
    "    parquet_df = pq.read_table(path, columns=[\"sequence_id\"] + SEL_COLS).to_pandas()\n",
    "\n",
    "    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "    parquet_numpy = parquet_df.to_numpy(copy=False)\n",
    "\n",
    "    col_to_index = {col: i for i, col in enumerate(parquet_df.columns)}\n",
    "\n",
    "    LHAND_indices = [col_to_index[col] for col in LHAND]\n",
    "    RHAND_indices = [col_to_index[col] for col in RHAND]\n",
    "\n",
    "    buffer_size = 1000  # Adjust as needed\n",
    "    buffer = []\n",
    "\n",
    "    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "        for seq_id, phrase in zip(file_df[\"sequence_id\"], file_df[\"phrase\"]):\n",
    "            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "            progress_queue.put(\n",
    "                f\"Process: {mp.current_process().name}, File: {file_id}, Sequence: {seq_id}\"\n",
    "            )\n",
    "\n",
    "            # Calculate the number of NaN values in each hand landmark\n",
    "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_indices]), axis=1) == 0)\n",
    "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_indices]), axis=1) == 0)\n",
    "            no_nan = max(r_nonan, l_nonan)\n",
    "\n",
    "            if 2 * len(phrase) < no_nan:\n",
    "                features = {\n",
    "                    COL: tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(\n",
    "                            value=frames[:, col_to_index[COL]]\n",
    "                        )\n",
    "                    )\n",
    "                    for COL in SEL_COLS\n",
    "                }\n",
    "                features[\"phrase\"] = tf.train.Feature(\n",
    "                    bytes_list=tf.train.BytesList(value=[bytes(phrase, \"utf-8\")])\n",
    "                )\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "                record_bytes = example.SerializeToString()\n",
    "\n",
    "                buffer.append(record_bytes)\n",
    "                if len(buffer) == buffer_size:\n",
    "                    for record in buffer:\n",
    "                        file_writer.write(record)\n",
    "                        buffer = []\n",
    "        if buffer:\n",
    "            for record in buffer:\n",
    "                file_writer.write(record)\n",
    "\n",
    "        # gc.collect()\n",
    "\n",
    "\n",
    "# cpu_count = int(mp.cpu_count() / 2)\n",
    "cpu_count = 8  # 8\n",
    "\n",
    "with Pool(cpu_count) as pool:\n",
    "    progress_bars = [\n",
    "        tqdm_notebook(desc=f\"Process {i + 1}\", unit=\"seq\") for i in range(cpu_count)\n",
    "    ]\n",
    "\n",
    "    for result in pool.imap(\n",
    "        process_file,\n",
    "        dataset_df[\"file_id\"].unique(),\n",
    "    ):\n",
    "        progress_updates = []\n",
    "        while not progress_queue.empty():\n",
    "            progress_updates.append(progress_queue.get())\n",
    "        for update, bar in zip(progress_updates, progress_bars):\n",
    "            bar.set_description(update)\n",
    "            bar.update()\n",
    "            \n",
    "print(\"All parquets processed to TFRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torchdata.dataloader2 import DataLoader2\n",
    "from torchdata.datapipes.iter import (\n",
    "    FileLister,\n",
    "    FileOpener,\n",
    "    TFRecordLoader,\n",
    "    Mapper,\n",
    "    Batcher,\n",
    "    Collator,\n",
    "    IterableWrapper,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)  #\n",
    "\n",
    "\n",
    "# Encodes phrase into a tensor of tokens\n",
    "def tokenize_phrase(example):\n",
    "    phrase = example[\"phrase\"][0].decode(\n",
    "        \"utf-8\"\n",
    "    )  # Decode the byte string into a regular string\n",
    "    phrase = \"<\" + phrase + \">\"\n",
    "    indices = [json_chars.get(char, json_chars.get(\"F\")) for char in phrase]\n",
    "    example[\"phrase\"] = torch.tensor(\n",
    "        indices\n",
    "    )  # Replace the byte string with a list of integers\n",
    "    return example\n",
    "\n",
    "\n",
    "# Remove NaN values from tensor\n",
    "def pre_process(example):\n",
    "    for key in example.keys():\n",
    "        example[key] = torch.nan_to_num(example[key])\n",
    "    return example\n",
    "\n",
    "\n",
    "# Normalize landmark vector\n",
    "def normalize(example):\n",
    "    return example\n",
    "\n",
    "\n",
    "# Collate function, this pads https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html\n",
    "# def collate_fn(batch):\n",
    "#    sequence_lengths = [len(tensor) for seq in batch for tensor in seq.values()]\n",
    "#    max_len = max(sequence_lengths)\n",
    "#    padded_batch = [\n",
    "#        {\n",
    "#            key: (\n",
    "#                F.pad(\n",
    "#                    input=tensor,\n",
    "#                    pad=(\n",
    "#                        0,\n",
    "#                        max_len - tensor.shape[0],\n",
    "#                    ),\n",
    "#                    mode=\"constant\",\n",
    "#                    value=0,\n",
    "#                )\n",
    "#                if key != \"phrase\"\n",
    "#                else tensor\n",
    "#            )\n",
    "#            for key, tensor in seq.items()\n",
    "#        }\n",
    "#        for seq in batch\n",
    "#    ]\n",
    "#\n",
    "#    stacked_landmarks = {\n",
    "#        key: torch.stack([seq[key] for seq in padded_batch]) for key in SEL_COLS\n",
    "#    }\n",
    "#    max_phrase_len = max([len(seq[\"phrase\"]) for seq in padded_batch])\n",
    "#    padded_phrases = [\n",
    "#        F.pad(\n",
    "#            input=seq[\"phrase\"],\n",
    "#            pad=(0, max_phrase_len - seq[\"phrase\"].shape[0]),\n",
    "#            mode=\"constant\",\n",
    "#            value=0,\n",
    "#        )\n",
    "#        for seq in batch\n",
    "#    ]\n",
    "#    # Stack the tensors along a new dimension\n",
    "#    stacked_phrases = torch.stack(padded_phrases, dim=0)\n",
    "#\n",
    "#    return batch  # stacked_landmarks, stacked_phrases\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequence_lengths = [len(tensor) for seq in batch for tensor in seq.values()]\n",
    "    max_len = max(sequence_lengths)\n",
    "    padded_batch = [\n",
    "        {\n",
    "            key: (\n",
    "                F.pad(\n",
    "                    input=tensor,\n",
    "                    pad=(\n",
    "                        0,\n",
    "                        max_len - tensor.shape[0],\n",
    "                    ),\n",
    "                    mode=\"constant\",\n",
    "                    value=0,\n",
    "                )\n",
    "                if key != \"phrase\"\n",
    "                else tensor\n",
    "            )\n",
    "            for key, tensor in seq.items()\n",
    "        }\n",
    "        for seq in batch\n",
    "    ]\n",
    "    # Stack the landmark tensors, creating a new dimension\n",
    "    stacked_landmarks = torch.stack(\n",
    "        [torch.stack([seq[key] for seq in padded_batch]) for key in SEL_COLS], dim=-1\n",
    "    )\n",
    "\n",
    "    max_phrase_len = max([len(seq[\"phrase\"]) for seq in padded_batch])\n",
    "    padded_phrases = [\n",
    "        F.pad(\n",
    "            input=seq[\"phrase\"],\n",
    "            pad=(0, max_phrase_len - seq[\"phrase\"].shape[0]),\n",
    "            mode=\"constant\",\n",
    "            value=0,\n",
    "        )\n",
    "        for seq in batch\n",
    "    ]\n",
    "    # Stack the tensors along a new dimension\n",
    "    stacked_phrases = torch.stack(padded_phrases, dim=0)\n",
    "    return stacked_landmarks, stacked_phrases\n",
    "\n",
    "\n",
    "# Build datapipes: TRAIN ONLY\n",
    "def build_train_pipe(batch_size, drop_last):\n",
    "    tf_records = dataset_df.file_id.map(lambda x: f\"preprocessed/{x}.tfrecord\").unique()\n",
    "    train_len = int(0.8 * len(tf_records))\n",
    "    datapipe = FileLister(tf_records[:train_len])\n",
    "    datapipe = FileOpener(datapipe, mode=\"b\")\n",
    "    datapipe = TFRecordLoader(datapipe)\n",
    "    datapipe = Mapper(datapipe, tokenize_phrase)\n",
    "    datapipe = Mapper(datapipe, pre_process)\n",
    "    # datapipe = Mapper(datapipe, normalize)\n",
    "    datapipe = Batcher(datapipe, batch_size=batch_size, drop_last=drop_last)\n",
    "    datapipe = Collator(datapipe, collate_fn=collate_fn)\n",
    "    return datapipe\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "\n",
    "datapipe = build_train_pipe(batch_size=32, drop_last=True)\n",
    "\n",
    "dataloader = DataLoader2(datapipe=datapipe)\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "# batch = next(iter(dataloader))\n",
    "\n",
    "for batch in dataloader:\n",
    "    for i in range(32):\n",
    "        print(batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=342):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.emb = nn.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(1)  # Assuming x.size(1) is sequence length\n",
    "\n",
    "        x = self.emb(x)\n",
    "\n",
    "        x = x * torch.sqrt(torch.tensor(self.num_hid, dtype=torch.float).to(x.device))\n",
    "        return x + self.pos_emb[:maxlen, :].to(x.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, num_hid=342, maxlen=453):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=342,  # Correctly handle 342 landmark features\n",
    "                out_channels=num_hid,\n",
    "                kernel_size=11,\n",
    "                padding=5,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n",
    "        self.maxlen = maxlen\n",
    "        self.num_hid = num_hid\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Permute the tensor to have channels as the second dimension\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Change from [batch_size, seq_len, features] to [batch_size, features, seq_len]\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Optionally permute back if needed for further processing\n",
    "\n",
    "        x = x * torch.sqrt(torch.tensor(self.num_hid, dtype=torch.float).to(x.device))\n",
    "        return x + self.pos_emb[: x.size(1), :].to(x.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask)\n",
    "\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        src2 = self.linear2(self.dropout2(self.linear1(src)))\n",
    "\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            d_model, nhead, batch_first=True\n",
    "        )  # Encoder-decoder attention\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, src_mask=None):\n",
    "        # 1. Masked Self-Attention (Causal)\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # 2. Encoder-Decoder Attention\n",
    "        tgt2, _ = self.multihead_attn(tgt, memory, memory, attn_mask=src_mask)\n",
    "\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # 3. Feed-Forward Network\n",
    "        tgt2 = self.linear2(self.dropout3(F.relu(self.linear1(tgt))))\n",
    "\n",
    "        tgt = tgt + tgt2  # Add residual\n",
    "        tgt = self.norm3(tgt)\n",
    "\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=453,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=62,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.loss = nn.CrossEntropyLoss(\n",
    "            ignore_index=pad_token_idx\n",
    "        )  # Assuming pad_token_idx is defined\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_emb = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen).to(\n",
    "            \"cuda\"\n",
    "        )  # Replace with embedding layer\n",
    "\n",
    "        self.dec_emb = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        ).to(\n",
    "            \"cuda\"\n",
    "        )  # Replace with embedding layer\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_dec)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(num_hid, num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target, tgt_mask=None, src_mask=None):\n",
    "        y = self.dec_emb(target)\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            y = decoder_layer(y, enc_out, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        return y\n",
    "\n",
    "    def forward(self, source, target, src_mask=None, tgt_mask=None):\n",
    "        source = self.enc_emb(source)\n",
    "        memory = self.encoder(source)\n",
    "        output = self.decode(memory, target, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).float()\n",
    "        mask = mask.masked_fill(mask == 0, float(\"-inf\"))  # -inf for future positions\n",
    "        return mask.unsqueeze(0)  # Add batch dimension (1)\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        batch_size = source.size(0)\n",
    "        enc_output = self.encoder(source)\n",
    "\n",
    "        # Initialize decoder input with start token\n",
    "        dec_input = torch.ones(batch_size, 1, dtype=torch.long) * target_start_token_idx\n",
    "\n",
    "        for _ in range(self.target_maxlen - 1):\n",
    "            tgt_mask = self.generate_square_subsequent_mask(dec_input.size(1))\n",
    "            dec_out = self.decode(enc_output, dec_input, tgt_mask=tgt_mask)\n",
    "            prediction = self.classifier(\n",
    "                dec_out[:, -1]\n",
    "            )  # Prediction for the last token\n",
    "            pred_idx = torch.argmax(prediction, dim=1, keepdim=True)\n",
    "            dec_input = torch.cat([dec_input, pred_idx], dim=1)\n",
    "\n",
    "        return dec_input.squeeze(\n",
    "            1\n",
    "        )  # Remove extra dimension for single generated sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=4,\n",
    "    num_feed_forward=400,\n",
    "    source_maxlen=784,\n",
    "    target_maxlen=64,\n",
    "    num_layers_enc=2,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=62,\n",
    ")\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss(ignore_index=pad_token_idx)  # Assuming pad_token_idx is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer.to(device)\n",
    "print(\"CUDA AVAILABLE: \", torch.cuda.is_available(), \", CUDA DEVICE: \", device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        source, target = batch\n",
    "\n",
    "        # Move data to GPU\n",
    "        source = source.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        print(\"Source: \",source.shape)\n",
    "        print(\"Target: \",target.shape)\n",
    "\n",
    "        outputs = transformer(source, target)\n",
    "        loss = criterion(outputs.view(-1, transformer.num_classes), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Code for evaluating the model on validation data (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Note, Reference, Brainstorm\n",
    "\n",
    "https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "\n",
    "## Input Embedding Layer\n",
    "\n",
    "The phrase must be vectorized (our case is chars)\n",
    "We must add Positional Encoding to create Positional Input Embeddings\n",
    "\n",
    "## Encoder Layer\n",
    "\n",
    "Two sub-modules:\n",
    "\n",
    "### Attention\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "3 Distinct fully connected layers\n",
    "\n",
    "- Query, Key, Value\n",
    "- A dot product of the Query and Key matrices is computed to create a score matrix.\n",
    "- The score matrix a table that dictatates how much value each word or char should be given, compared to the other words or chars in the input sequence. Higher score = more important. = more focus.\n",
    "- The score matrix is normalized by dividing by the square root of the dimension of the key vectors.\n",
    "- The score matrix is divided by the square root of the dimension of the key vectors which gives the scaled scores.\n",
    "- The scaled scores are then passed through a softmax function to receive the attention weights.\n",
    "- Multiply attention weights by value matrix to get the output vector of the self-attention layer.\n",
    "- Linear layer to process.\n",
    "\n",
    "#### Multi-headed Attention\n",
    "\n",
    "The query, key, and value is split into N heads.\n",
    "\n",
    "- Each vector goes through the attention layer as normal\n",
    "- The output of all heads is concatenated into a single vector\n",
    "- Each head is given a different representation of the input sequence, allowing the model to simultaneously attend to information from different representation subspaces.\n",
    "- Each head in theory should learn to attend to different parts of the input sequence, and thus overall learn more of the input sequence.\n",
    "\n",
    "### Residual Connection, Layer Normalization, and Feed Forward Layer\n",
    "\n",
    "- The Multi-headed attention output vector is added to the original input vector to the sub-layer, which is called a residual connection.\n",
    "- This output is then normalized by layer normalization.\n",
    "- This enters a feed forward network, which is a simple 2 layer fully connected network with a ReLU activation in between.\n",
    "- The output of that is added again to the original input, to be normalized again. Like before with the multi-headed attention.\n",
    "\n",
    "## Decoder Layer\n",
    "\n",
    "### Output Embedding Layer and Positional Encoding\n",
    "\n",
    "- The output goes through an embedding layer to get the position embeddings.\n",
    "- This enters the first multi-headed attention layer.\n",
    "- The scaled scores are added to a look ahead mask, which prevents the decoder from attending to future tokens. \n",
    "- This happens when the softmax makes future tokens 0, so no attention is given to them.\n",
    "- All the heads are combined to create a masked output layer.\n",
    "\n",
    "The second multi-headed attention layer has the query and key of the encoder output, and the value of the previous multi-headed attention layer output value.\n",
    "\n",
    "A final feed forward network is applied to the output of the second multi-headed attention layer.\n",
    "\n",
    "### Classifier, softmax, and output\n",
    "The feed forward output enters a linear classifier.\n",
    "\n",
    "Classifier output enters softmax to get the probability score of each class (char). The index of the highest probability is the predicted char.\n",
    "\n",
    "Output is added to a list of decoded outputs until the end token is reached.\n",
    "\n",
    "This whole structure can be stacked N layers high. The output of the final encoder, is input into all the decoder layers, who are taking the input from the previous decoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "```python\n",
    "import tensorflow_addons as tfa\n",
    "pbar = tfa.callbacks.TQDMProgressBar()\n",
    "model.fit(…,callbacks=[pbar])\n",
    "# TQDMProgressBar() also works with evaluate()\n",
    "model.evaluate(…,callbacks=[pb\n",
    "```\n",
    "Check!\n",
    "\n",
    "## Multiprocessing\n",
    "\n",
    "```python\n",
    "with Pool(workers) as pool:\n",
    "    results = list(tqdm(pool.imap(worker,thread_list, total=len(thread_list))\n",
    "                        ar])\n",
    "``` \n",
    "\n",
    "Check!\n",
    "\n",
    "## Padding strategies\n",
    "\n",
    "1. No Padding with <EOS> token: This is the most efficient and elegant approach for Transformers.\n",
    "\n",
    "2. Full Padding: Pad all phrases (and potentially feature sequences) to a fixed maximum length using a constant value or specific technique. This can be simpler to implement but introduces unnecessary computational overhead and potential information distortion due to large padding sections.\n",
    "\n",
    "3. Full Padding with Masking: This combines the simplicity of full padding with the benefits of masking. While you pad all sequences to a fixed length, you apply masking during training to prevent the model from attending to the padded regions. This can be a good compromise if your model struggles with highly variable sequence lengths but you still want to avoid the downsides of excessive padding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
