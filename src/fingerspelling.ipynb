{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "from torchdata.dataloader2 import DataLoader2, MultiProcessingReadingService\n",
    "from torchdata.datapipes.iter import (\n",
    "    FileLister,\n",
    "    FileOpener,\n",
    "    TFRecordLoader,\n",
    "    Mapper,\n",
    "    Batcher,\n",
    "    Collator,\n",
    "    Shuffler,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\n",
    "    \"medium\"\n",
    ")  # Set the default matmul precision to medium, or high/highest?\n",
    "\n",
    "\n",
    "# Read the first CSV file\n",
    "dataset_train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "dataset_supplemental_df = pd.read_csv(\"supplemental_metadata.csv\")\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "dataset_df = pd.concat([dataset_train_df, dataset_supplemental_df], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV file\n",
    "# dataset_df.to_csv(\"train_full.csv\", index=False)\n",
    "dataset_df = dataset_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first row of the DataFrame\n",
    "path, sequence_id, file_id, phrase = dataset_df.iloc[0][\n",
    "    [\"path\", \"sequence_id\", \"file_id\", \"phrase\"]\n",
    "]\n",
    "print(f\"path: {path}, sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "sample_sequence_df = pq.read_table(\n",
    "    f\"{str(path)}\",\n",
    "    filters=[\n",
    "        [(\"sequence_id\", \"=\", sequence_id)],\n",
    "    ],\n",
    ").to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the total amount unique files\n",
    "unique_paths = dataset_df[\"path\"].unique()\n",
    "\n",
    "sum = unique_paths.shape[0]\n",
    "\n",
    "print(\"Total number of files: {}\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIP = [\n",
    "    61,\n",
    "    185,\n",
    "    40,\n",
    "    39,\n",
    "    37,\n",
    "    267,\n",
    "    269,\n",
    "    270,\n",
    "    409,\n",
    "    291,\n",
    "    146,\n",
    "    91,\n",
    "    181,\n",
    "    84,\n",
    "    17,\n",
    "    314,\n",
    "    405,\n",
    "    321,\n",
    "    375,\n",
    "    78,\n",
    "    191,\n",
    "    80,\n",
    "    81,\n",
    "    82,\n",
    "    13,\n",
    "    312,\n",
    "    311,\n",
    "    310,\n",
    "    415,\n",
    "    95,\n",
    "    88,\n",
    "    178,\n",
    "    87,\n",
    "    14,\n",
    "    317,\n",
    "    402,\n",
    "    318,\n",
    "    324,\n",
    "    308,\n",
    "]\n",
    "\n",
    "FACE = (\n",
    "    [f\"x_face_{i}\" for i in LIP]\n",
    "    + [f\"y_face_{i}\" for i in LIP]\n",
    "    + [f\"z_face_{i}\" for i in LIP]\n",
    ")\n",
    "LHAND = (\n",
    "    [f\"x_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_left_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "RHAND = (\n",
    "    [f\"x_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_right_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "POSE = (\n",
    "    [f\"x_pose_{i}\" for i in range(33)]\n",
    "    + [f\"y_pose_{i}\" for i in range(33)]\n",
    "    + [f\"z_pose_{i}\" for i in range(33)]\n",
    ")\n",
    "\n",
    "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
    "FRAME_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the existing data\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)\n",
    "\n",
    "# Define the new entries\n",
    "new_entries = [\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"P\",\n",
    "]\n",
    "\n",
    "# Add the new entries starting from index 59, only if they don't already exist\n",
    "for i, entry in enumerate(new_entries, start=59):\n",
    "    if entry not in json_chars:\n",
    "        json_chars[entry] = i\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(\"character_to_prediction_index.json\", \"w\") as f:\n",
    "    json.dump(json_chars, f, indent=4)\n",
    "\n",
    "start_token_idx = 59\n",
    "end_token_idx = 60\n",
    "pad_token_idx = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf.config.set_visible_devices([], \"GPU\")  # Disable GPU for Tensorflow\n",
    "#\n",
    "## Create a Manager object for the progress_queue\n",
    "#manager = Manager()\n",
    "#progress_queue = manager.Queue()\n",
    "#\n",
    "#\n",
    "#def process_file(file_id):\n",
    "#    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "#    path = file_df[\"path\"].values[0]\n",
    "#    parquet_df = pq.read_table(path, columns=[\"sequence_id\"] + SEL_COLS).to_pandas()\n",
    "#\n",
    "#    parquet_df = parquet_df.fillna(0)\n",
    "#\n",
    "#    scalerFACE = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerLHAND = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerRHAND = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerPOSE = StandardScaler(with_mean=True, with_std=True)\n",
    "#\n",
    "#    parquet_df[FACE] = scalerFACE.fit_transform(\n",
    "#        parquet_df[FACE],\n",
    "#    )\n",
    "#    parquet_df[LHAND] = scalerLHAND.fit_transform(\n",
    "#        parquet_df[LHAND],\n",
    "#    )\n",
    "#    parquet_df[RHAND] = scalerRHAND.fit_transform(\n",
    "#        parquet_df[RHAND],\n",
    "#    )\n",
    "#    parquet_df[POSE] = scalerPOSE.fit_transform(\n",
    "#        parquet_df[POSE],\n",
    "#    )\n",
    "#\n",
    "#    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "#    parquet_numpy = parquet_df.to_numpy(copy=False)\n",
    "#    col_to_index = {col: i for i, col in enumerate(parquet_df.columns)}\n",
    "#    LHAND_indices = [col_to_index[col] for col in LHAND]\n",
    "#    RHAND_indices = [col_to_index[col] for col in RHAND]\n",
    "#    buffer_size = 1000  # Adjust as needed\n",
    "#    buffer = []\n",
    "#    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "#        for seq_id, phrase in zip(file_df[\"sequence_id\"], file_df[\"phrase\"]):\n",
    "#            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "#            progress_queue.put(\n",
    "#                f\"Process: {mp.current_process().name}, File: {file_id}, Sequence: {seq_id}\"\n",
    "#            )\n",
    "#\n",
    "#            # Calculate the number of NaN values in each hand landmark\n",
    "#            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_indices]), axis=1) == 0)\n",
    "#            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_indices]), axis=1) == 0)\n",
    "#            no_nan = max(r_nonan, l_nonan)\n",
    "#\n",
    "#            if 2 * len(phrase) < no_nan:\n",
    "#                features = {\n",
    "#                    COL: tf.train.Feature(\n",
    "#                        float_list=tf.train.FloatList(\n",
    "#                            value=frames[:, col_to_index[COL]]\n",
    "#                        )\n",
    "#                    )\n",
    "#                    for COL in SEL_COLS\n",
    "#                }\n",
    "#                features[\"phrase\"] = tf.train.Feature(\n",
    "#                    bytes_list=tf.train.BytesList(value=[bytes(phrase, \"utf-8\")])\n",
    "#                )\n",
    "#\n",
    "#                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "#                record_bytes = example.SerializeToString()\n",
    "#\n",
    "#                buffer.append(record_bytes)\n",
    "#                if len(buffer) == buffer_size:\n",
    "#                    for record in buffer:\n",
    "#                        file_writer.write(record)\n",
    "#                        buffer = []\n",
    "#        if buffer:\n",
    "#            for record in buffer:\n",
    "#                file_writer.write(record)\n",
    "#\n",
    "#        # gc.collect()\n",
    "#\n",
    "#\n",
    "#cpu_count = int(mp.cpu_count() / 2)\n",
    "#cpu_count = 8  # 8\n",
    "#\n",
    "#\n",
    "#with Pool(cpu_count) as pool:\n",
    "#    progress_bars = [\n",
    "#        tqdm_notebook(desc=f\"Process {i + 1}\", unit=\"seq\") for i in range(cpu_count)\n",
    "#    ]\n",
    "#\n",
    "#    for result in pool.imap(\n",
    "#        process_file,\n",
    "#        dataset_df[\"file_id\"].unique(),\n",
    "#    ):\n",
    "#        progress_updates = []\n",
    "#        while not progress_queue.empty():\n",
    "#            progress_updates.append(progress_queue.get())\n",
    "#        for update, bar in zip(progress_updates, progress_bars):\n",
    "#            bar.set_description(update)\n",
    "#            bar.update()\n",
    "#\n",
    "#\n",
    "#print(\"All parquets processed to TFRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)  #\n",
    "\n",
    "\n",
    "# Encodes phrase into a tensor of tokens\n",
    "def tokenize_phrase(example):\n",
    "    phrase = example[\"phrase\"][0].decode(\n",
    "        \"utf-8\"\n",
    "    )  # Decode the byte string into a regular string\n",
    "    phrase = \"<\" + phrase + \">\"\n",
    "    indices = [json_chars.get(char, json_chars.get(\"F\")) for char in phrase]\n",
    "    example[\"phrase\"] = torch.tensor(\n",
    "        indices\n",
    "    )  # Replace the byte string with a list of integers\n",
    "    return example\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate phrases and sequence lengths\n",
    "    phrases = [seq.pop(\"phrase\") for seq in batch]\n",
    "    landmarks = [seq for seq in batch]\n",
    "\n",
    "    sequence_lengths = [len(next(iter(landmark.values()))) for landmark in landmarks]\n",
    "    phrase_lengths = [len(phrase) for phrase in phrases]\n",
    "\n",
    "    # Compute maximum lengths\n",
    "    max_sequence_len = max(sequence_lengths)\n",
    "    max_phrase_len = max(phrase_lengths)\n",
    "\n",
    "    # Pad sequences and phrases\n",
    "    padded_batch = [\n",
    "        torch.stack(\n",
    "            [\n",
    "                F.pad(\n",
    "                    input=tensor,\n",
    "                    pad=(0, max_sequence_len - tensor.shape[0]),\n",
    "                    mode=\"constant\",\n",
    "                    value=0,\n",
    "                )\n",
    "                for tensor in seq.values()\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        for seq in batch\n",
    "    ]\n",
    "\n",
    "    stacked_landmarks = torch.stack(padded_batch, dim=0)\n",
    "\n",
    "    padded_phrases = [\n",
    "        F.pad(\n",
    "            input=phrase,\n",
    "            pad=(0, 64 - len(phrase)),\n",
    "            mode=\"constant\",\n",
    "            value=61,\n",
    "        )\n",
    "        for phrase in phrases\n",
    "    ]\n",
    "\n",
    "    stacked_phrases = torch.stack(padded_phrases, dim=0)\n",
    "\n",
    "    return (\n",
    "        stacked_landmarks,\n",
    "        stacked_phrases,\n",
    "        torch.tensor(sequence_lengths),\n",
    "        torch.tensor(phrase_lengths),\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the split index\n",
    "tf_records = dataset_df.file_id.map(\n",
    "    lambda x: f\"/home/jpinn/asl-fingerspelling-recognition/src/preprocessed/{x}.tfrecord\"\n",
    ").unique()\n",
    "split_index = int(0.8 * len(tf_records))\n",
    "\n",
    "\n",
    "def build_pipe(batch_size, drop_last, start, end, shuffle=True):\n",
    "    datapipe = FileLister(tf_records[start:end])\n",
    "\n",
    "    if shuffle:\n",
    "        datapipe = Shuffler(\n",
    "            datapipe, buffer_size=len(tf_records[start:end])\n",
    "        )  # Shuffle the dataset\n",
    "\n",
    "    datapipe = FileOpener(datapipe, mode=\"b\")\n",
    "    datapipe = TFRecordLoader(datapipe)\n",
    "    datapipe = Mapper(datapipe, tokenize_phrase)\n",
    "    datapipe = Batcher(datapipe, batch_size=batch_size, drop_last=drop_last)\n",
    "    datapipe = Collator(datapipe, collate_fn=collate_fn)\n",
    "    return datapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=None, maxlen=None, num_hid=None):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.emb = nn.Embedding(num_embeddings=num_vocab, embedding_dim=num_hid)\n",
    "        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(1)\n",
    "        x = x.to(torch.int64)\n",
    "        x = self.emb(x)\n",
    "        x = x * torch.sqrt(\n",
    "            torch.tensor(self.num_hid, dtype=torch.float, device=x.device)\n",
    "        )\n",
    "\n",
    "        return x + self.pos_emb[:maxlen, :]\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.to(\"cuda:0\")\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, num_hid=342, maxlen=453):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=342,  # Correctly handle 342 landmark features\n",
    "                out_channels=num_hid,\n",
    "                kernel_size=11,\n",
    "                padding=5,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n",
    "        self.maxlen = maxlen\n",
    "        self.num_hid = num_hid\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Permute the tensor to have channels as the second dimension\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Change from [batch_size, seq_len, features] to [batch_size, features, seq_len]\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Optionally permute back if needed for further processing\n",
    "\n",
    "        x = x * torch.sqrt(torch.tensor(self.num_hid, dtype=torch.float).to(x.device))\n",
    "        return x + self.pos_emb[: x.size(1), :].to(x.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        src2 = self.linear2(self.dropout2(self.linear1(src)))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            d_model, nhead, batch_first=True\n",
    "        )  # Encoder-decoder attention\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def causal_attention_mask(batch_size, seq_len, device):\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
    "        ).bool()\n",
    "        return mask[None, :, :].expand(batch_size * 4, seq_len, seq_len)\n",
    "\n",
    "    def forward(\n",
    "        self, enc_out, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        batch_size, seq_len, _ = target.size()\n",
    "\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, target.device)\n",
    "\n",
    "        tgt_key_padding_mask = tgt_key_padding_mask[:, :seq_len]\n",
    "\n",
    "        # Self-attention and layer norm\n",
    "        target_att, _ = self.self_attn(\n",
    "            target,\n",
    "            target,\n",
    "            target,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        target = self.norm1(target + self.dropout1(target_att))\n",
    "\n",
    "        # Encoder-decoder attention and layer norm\n",
    "        enc_att_output, _ = self.multihead_attn(\n",
    "            target, enc_out, enc_out, key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        target = self.norm2(target + self.dropout2(enc_att_output))\n",
    "\n",
    "        # Feed forward network and layer norm\n",
    "        ffn_output = self.linear2(self.dropout(F.relu(self.linear1(target))))\n",
    "        target = self.norm3(target + self.dropout3(ffn_output))\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid,\n",
    "        num_head,\n",
    "        num_feed_forward,\n",
    "        source_maxlen,\n",
    "        target_maxlen,\n",
    "        num_layers_enc,\n",
    "        num_layers_dec,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_emb = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_emb = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_dec)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(num_hid, num_classes)\n",
    "\n",
    "    def decode(\n",
    "        self, enc_out, target, src_key_padding_mask=None, target_padding_mask=None\n",
    "    ):\n",
    "        y = self.dec_emb(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = self.decoder_layers[i](\n",
    "                enc_out, y, src_key_padding_mask, target_padding_mask\n",
    "            )\n",
    "        return y\n",
    "\n",
    "    def forward(\n",
    "        self, source, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        source = self.enc_emb(source)\n",
    "        memory = source\n",
    "        for layer in self.encoder:\n",
    "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decode(memory, target, src_key_padding_mask, tgt_key_padding_mask)\n",
    "        return self.classifier(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).float()\n",
    "        mask = mask.masked_fill(mask == 0, float(\"-inf\"))  # -inf for future positions\n",
    "        return mask.unsqueeze(0)  # Add batch dimension (1)\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        batch_size = source.size(0)\n",
    "        enc_output = self.encoder(source)\n",
    "\n",
    "        dec_input = torch.ones(batch_size, 1, dtype=torch.long) * target_start_token_idx\n",
    "        for _ in range(self.target_maxlen - 1):\n",
    "            tgt_mask = self.generate_square_subsequent_mask(dec_input.size(1)).to(\n",
    "                source.device\n",
    "            )\n",
    "            dec_out = self.decode(enc_output, dec_input, tgt_mask=tgt_mask)\n",
    "            prediction = self.classifier(dec_out[:, -1])\n",
    "            pred_idx = torch.argmax(prediction, dim=1, keepdim=True)\n",
    "            dec_input = torch.cat([dec_input, pred_idx], dim=1)\n",
    "\n",
    "        return dec_input.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateFinder\n",
    "\n",
    "\n",
    "def reading_service_fn(workers):\n",
    "    return MultiProcessingReadingService(num_workers=workers)\n",
    "\n",
    "\n",
    "class TransformerModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(config)\n",
    "        self.transformer = Transformer(\n",
    "            num_hid=200,\n",
    "            num_head=4,\n",
    "            num_feed_forward=400,\n",
    "            source_maxlen=807,\n",
    "            target_maxlen=64,\n",
    "            num_layers_enc=12,\n",
    "            num_layers_dec=2,\n",
    "            num_classes=62,\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=61, label_smoothing=0.1)\n",
    "\n",
    "    def forward(\n",
    "        self, source, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        return self.transformer(\n",
    "            source,\n",
    "            target,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "\n",
    "    def create_tgt_mask(self, tgt_lengths, batch_size, seq_length):\n",
    "        \"\"\"Create a boolean mask for target sequences based on lengths.\"\"\"\n",
    "        tgt_key_padding_mask = torch.ones(\n",
    "            (batch_size, seq_length), device=self.device, dtype=torch.bool\n",
    "        )\n",
    "        for i, length in enumerate(tgt_lengths):\n",
    "            tgt_key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return tgt_key_padding_mask\n",
    "\n",
    "    def create_src_mask(self, src_lengths, batch_size, seq_length):\n",
    "        \"\"\"Create a boolean mask for source sequences based on lengths.\"\"\"\n",
    "        src_key_padding_mask = torch.ones(\n",
    "            (batch_size, seq_length), device=self.device, dtype=torch.bool\n",
    "        )\n",
    "        for i, length in enumerate(src_lengths):\n",
    "            src_key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return src_key_padding_mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        src_key_padding_mask = self.create_src_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_src_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        output = self.forward(\n",
    "            source,\n",
    "            target[:, :-1],\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        loss = self.criterion(output.transpose(1, 2), target[:, 1:])\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = val_batch\n",
    "\n",
    "        src_key_padding_mask = self.create_src_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_src_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        output = self.forward(\n",
    "            source,\n",
    "            target[:, :-1],\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        loss = self.criterion(output.transpose(1, 2), target[:, 1:])\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        predicted = torch.argmax(output, dim=2)\n",
    "        correct = (predicted == target[:, 1:]).float()\n",
    "        accuracy = correct.sum() / correct.numel()\n",
    "        self.log(\"val_accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": accuracy}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configures the AdamW optimizer with LearningRateFinder.\"\"\"\n",
    "\n",
    "        training_steps = 1588\n",
    "        total_steps = training_steps * self.hparams[\"epochs\"]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.transformer.parameters(), lr=self.hparams[\"lr\"], weight_decay=0.001\n",
    "        )\n",
    "\n",
    "        # Create OneCycleLR scheduler using suggested base_lr\n",
    "        scheduler = torch.optim.OneCycleLR(\n",
    "            optimizer, max_lr=0.01, total_steps=total_steps\n",
    "        )  # Adjust max_lr as needed\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_datapipe = build_pipe(\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            drop_last=True,\n",
    "            start=0,\n",
    "            end=split_index,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return DataLoader2(train_datapipe, reading_service=reading_service_fn(8))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_datapipe = build_pipe(\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            drop_last=True,\n",
    "            start=split_index,\n",
    "            end=len(tf_records),\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return DataLoader2(datapipe=val_datapipe, reading_service=reading_service_fn(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    config = {\n",
    "        \"epochs\": 200,\n",
    "        \"lr\": 0.01,\n",
    "        \"batch_size\": 128,\n",
    "    }\n",
    "    # Initialize the model\n",
    "    model = TransformerModule(config)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=30,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"transformer\")\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        \n",
    "        max_epochs=config[\"epochs\"],\n",
    "        devices=1 if torch.cuda.is_available() else 0,\n",
    "        accelerator=\"auto\",\n",
    "        callbacks=[checkpoint_callback, lr_monitor],\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        enable_checkpointing=True,\n",
    "        precision=\"bf16-mixed\",\n",
    "        accumulate_grad_batches=2,\n",
    "        # profiler=\"advanced\",\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the idx_to_char mapping from the JSON file\n",
    "with open(\"idx_to_char.json\", \"r\") as f:\n",
    "    idx_to_char = json.load(f)\n",
    "\n",
    "# JSON keys are always strings, so you might need to convert them back to integers\n",
    "idx_to_char = {int(k): v for k, v in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_predictions(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    start_token_idx,\n",
    "    end_token_idx,\n",
    "    idx_to_char,\n",
    "    num_samples=10,\n",
    "):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    preds_list = []\n",
    "    ground_truth_list = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            source, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            # Assuming your model has a generate function similar to model.generate\n",
    "            # Adjust as necessary if your generation process is different\n",
    "            preds = model.generate(source, start_token_idx).cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "\n",
    "            bs = source.size(0)  # Batch size\n",
    "\n",
    "            for i in range(bs):\n",
    "                target_text = \"\".join(\n",
    "                    [idx_to_char[_] for _ in target[i, :] if _ != end_token_idx]\n",
    "                )\n",
    "                ground_truth_list.append(target_text)\n",
    "\n",
    "                prediction = \"\"\n",
    "                for idx in preds[i, :]:\n",
    "                    if idx == end_token_idx:\n",
    "                        break\n",
    "                    prediction += idx_to_char[idx]\n",
    "                preds_list.append(prediction)\n",
    "\n",
    "            if (\n",
    "                batch_idx >= num_samples - 1\n",
    "            ):  # Only sample a few batches for demonstration\n",
    "                break\n",
    "\n",
    "    # Print predictions\n",
    "    for i in range(min(num_samples, len(preds_list))):\n",
    "        print(f\"Ground Truth: {ground_truth_list[i]}\")\n",
    "        print(f\"Prediction: {preds_list[i]}\")\n",
    "        print(\"\\n~~~\\n\")\n",
    "\n",
    "\n",
    "# Call the function with your model, validation dataloader, and other necessary parameters\n",
    "generate_predictions(transformer, val_dataloader, device, 58, 59, idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Note, Reference, Brainstorm\n",
    "\n",
    "https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "\n",
    "## Input Embedding Layer\n",
    "\n",
    "The phrase must be vectorized (our case is chars)\n",
    "We must add Positional Encoding to create Positional Input Embeddings\n",
    "\n",
    "## Encoder Layer\n",
    "\n",
    "Two sub-modules:\n",
    "\n",
    "### Attention\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "3 Distinct fully connected layers\n",
    "\n",
    "- Query, Key, Value\n",
    "- A dot product of the Query and Key matrices is computed to create a score matrix.\n",
    "- The score matrix a table that dictatates how much value each word or char should be given, compared to the other words or chars in the input sequence. Higher score = more important. = more focus.\n",
    "- The score matrix is normalized by dividing by the square root of the dimension of the key vectors.\n",
    "- The score matrix is divided by the square root of the dimension of the key vectors which gives the scaled scores.\n",
    "- The scaled scores are then passed through a softmax function to receive the attention weights.\n",
    "- Multiply attention weights by value matrix to get the output vector of the self-attention layer.\n",
    "- Linear layer to process.\n",
    "\n",
    "#### Multi-headed Attention\n",
    "\n",
    "The query, key, and value is split into N heads.\n",
    "\n",
    "- Each vector goes through the attention layer as normal\n",
    "- The output of all heads is concatenated into a single vector\n",
    "- Each head is given a different representation of the input sequence, allowing the model to simultaneously attend to information from different representation subspaces.\n",
    "- Each head in theory should learn to attend to different parts of the input sequence, and thus overall learn more of the input sequence.\n",
    "\n",
    "### Residual Connection, Layer Normalization, and Feed Forward Layer\n",
    "\n",
    "- The Multi-headed attention output vector is added to the original input vector to the sub-layer, which is called a residual connection.\n",
    "- This output is then normalized by layer normalization.\n",
    "- This enters a feed forward network, which is a simple 2 layer fully connected network with a ReLU activation in between.\n",
    "- The output of that is added again to the original input, to be normalized again. Like before with the multi-headed attention.\n",
    "\n",
    "## Decoder Layer\n",
    "\n",
    "### Output Embedding Layer and Positional Encoding\n",
    "\n",
    "- The output goes through an embedding layer to get the position embeddings.\n",
    "- This enters the first multi-headed attention layer.\n",
    "- The scaled scores are added to a look ahead mask, which prevents the decoder from attending to future tokens.\n",
    "- This happens when the softmax makes future tokens 0, so no attention is given to them.\n",
    "- All the heads are combined to create a masked output layer.\n",
    "\n",
    "The second multi-headed attention layer has the query and key of the encoder output, and the value of the previous multi-headed attention layer output value.\n",
    "\n",
    "A final feed forward network is applied to the output of the second multi-headed attention layer.\n",
    "\n",
    "### Classifier, softmax, and output\n",
    "\n",
    "The feed forward output enters a linear classifier.\n",
    "\n",
    "Classifier output enters softmax to get the probability score of each class (char). The index of the highest probability is the predicted char.\n",
    "\n",
    "Output is added to a list of decoded outputs until the end token is reached.\n",
    "\n",
    "This whole structure can be stacked N layers high. The output of the final encoder, is input into all the decoder layers, who are taking the input from the previous decoder layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "```python\n",
    "import tensorflow_addons as tfa\n",
    "pbar = tfa.callbacks.TQDMProgressBar()\n",
    "model.fit(…,callbacks=[pbar])\n",
    "# TQDMProgressBar() also works with evaluate()\n",
    "model.evaluate(…,callbacks=[pb\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Multiprocessing\n",
    "\n",
    "```python\n",
    "with Pool(workers) as pool:\n",
    "    results = list(tqdm(pool.imap(worker,thread_list, total=len(thread_list))\n",
    "                        ar])\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Padding strategies\n",
    "\n",
    "1. No Padding with <EOS> token: This is the most efficient and elegant approach for Transformers.\n",
    "\n",
    "2. Full Padding: Pad all phrases (and potentially feature sequences) to a fixed maximum length using a constant value or specific technique. This can be simpler to implement but introduces unnecessary computational overhead and potential information distortion due to large padding sections.\n",
    "\n",
    "3. Full Padding with Masking: This combines the simplicity of full padding with the benefits of masking. While you pad all sequences to a fixed length, you apply masking during training to prevent the model from attending to the padded regions. This can be a good compromise if your model struggles with highly variable sequence lengths but you still want to avoid the downsides of excessive padding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
