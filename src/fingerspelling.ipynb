{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "import torch._dynamo\n",
    "from torchdata.dataloader2 import DataLoader2, MultiProcessingReadingService\n",
    "from torchdata.datapipes.iter import (\n",
    "    FileLister,\n",
    "    FileOpener,\n",
    "    TFRecordLoader,\n",
    "    Mapper,\n",
    "    Batcher,\n",
    "    Collator,\n",
    "    Shuffler,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    ")\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\n",
    "    \"medium\"\n",
    ")  # Set the default matmul precision to medium, or high/highest?\n",
    "\n",
    "\n",
    "# Read the first CSV file\n",
    "dataset_train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "dataset_supplemental_df = pd.read_csv(\"supplemental_metadata.csv\")\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "dataset_df = pd.concat([dataset_train_df, dataset_supplemental_df], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV file\n",
    "# dataset_df.to_csv(\"train_full.csv\", index=False)\n",
    "dataset_df = dataset_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: train_landmarks/5414471.parquet, sequence_id: 1816796431, file_id: 5414471, phrase: 3 creekhouse\n",
      "Full sequence dataset shape is (123, 1630)\n"
     ]
    }
   ],
   "source": [
    "# Read the first row of the DataFrame\n",
    "path, sequence_id, file_id, phrase = dataset_df.iloc[0][\n",
    "    [\"path\", \"sequence_id\", \"file_id\", \"phrase\"]\n",
    "]\n",
    "print(f\"path: {path}, sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "sample_sequence_df = pq.read_table(\n",
    "    f\"{str(path)}\",\n",
    "    filters=[\n",
    "        [(\"sequence_id\", \"=\", sequence_id)],\n",
    "    ],\n",
    ").to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 68\n"
     ]
    }
   ],
   "source": [
    "# Read the total amount unique files\n",
    "unique_paths = dataset_df[\"path\"].unique()\n",
    "\n",
    "sum = unique_paths.shape[0]\n",
    "\n",
    "print(\"Total number of files: {}\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "LIP = [\n",
    "    61,\n",
    "    185,\n",
    "    40,\n",
    "    39,\n",
    "    37,\n",
    "    267,\n",
    "    269,\n",
    "    270,\n",
    "    409,\n",
    "    291,\n",
    "    146,\n",
    "    91,\n",
    "    181,\n",
    "    84,\n",
    "    17,\n",
    "    314,\n",
    "    405,\n",
    "    321,\n",
    "    375,\n",
    "    78,\n",
    "    191,\n",
    "    80,\n",
    "    81,\n",
    "    82,\n",
    "    13,\n",
    "    312,\n",
    "    311,\n",
    "    310,\n",
    "    415,\n",
    "    95,\n",
    "    88,\n",
    "    178,\n",
    "    87,\n",
    "    14,\n",
    "    317,\n",
    "    402,\n",
    "    318,\n",
    "    324,\n",
    "    308,\n",
    "]\n",
    "\n",
    "FACE = (\n",
    "    [f\"x_face_{i}\" for i in LIP]\n",
    "    + [f\"y_face_{i}\" for i in LIP]\n",
    "    + [f\"z_face_{i}\" for i in LIP]\n",
    ")\n",
    "LHAND = (\n",
    "    [f\"x_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_left_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "RHAND = (\n",
    "    [f\"x_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_right_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "POSE = (\n",
    "    [f\"x_pose_{i}\" for i in range(33)]\n",
    "    + [f\"y_pose_{i}\" for i in range(33)]\n",
    "    + [f\"z_pose_{i}\" for i in range(33)]\n",
    ")\n",
    "\n",
    "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
    "FRAME_LEN = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read the existing data\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)\n",
    "\n",
    "# Define the new entries\n",
    "new_entries = [\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"P\",\n",
    "]\n",
    "\n",
    "# Add the new entries starting from index 59, only if they don't already exist\n",
    "for i, entry in enumerate(new_entries, start=59):\n",
    "    if entry not in json_chars:\n",
    "        json_chars[entry] = i\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(\"character_to_prediction_index.json\", \"w\") as f:\n",
    "    json.dump(json_chars, f, indent=4)\n",
    "\n",
    "start_token_idx = 59\n",
    "end_token_idx = 60\n",
    "pad_token_idx = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf.config.set_visible_devices([], \"GPU\")  # Disable GPU for Tensorflow\n",
    "#\n",
    "## Create a Manager object for the progress_queue\n",
    "#manager = Manager()\n",
    "#progress_queue = manager.Queue()\n",
    "#\n",
    "#\n",
    "#def process_file(file_id):\n",
    "#    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "#    path = file_df[\"path\"].values[0]\n",
    "#    parquet_df = pq.read_table(path, columns=[\"sequence_id\"] + SEL_COLS).to_pandas()\n",
    "#\n",
    "#    parquet_df = parquet_df.fillna(0)\n",
    "#\n",
    "#    scalerFACE = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerLHAND = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerRHAND = StandardScaler(with_mean=True, with_std=True)\n",
    "#    scalerPOSE = StandardScaler(with_mean=True, with_std=True)\n",
    "#\n",
    "#    parquet_df[FACE] = scalerFACE.fit_transform(\n",
    "#        parquet_df[FACE],\n",
    "#    )\n",
    "#    parquet_df[LHAND] = scalerLHAND.fit_transform(\n",
    "#        parquet_df[LHAND],\n",
    "#    )\n",
    "#    parquet_df[RHAND] = scalerRHAND.fit_transform(\n",
    "#        parquet_df[RHAND],\n",
    "#    )\n",
    "#    parquet_df[POSE] = scalerPOSE.fit_transform(\n",
    "#        parquet_df[POSE],\n",
    "#    )\n",
    "#\n",
    "#    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "#    parquet_numpy = parquet_df.to_numpy(copy=False)\n",
    "#    col_to_index = {col: i for i, col in enumerate(parquet_df.columns)}\n",
    "#    LHAND_indices = [col_to_index[col] for col in LHAND]\n",
    "#    RHAND_indices = [col_to_index[col] for col in RHAND]\n",
    "#    buffer_size = 1000  # Adjust as needed\n",
    "#    buffer = []\n",
    "#\n",
    "#    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "#        for seq_id, phrase in zip(file_df[\"sequence_id\"], file_df[\"phrase\"]):\n",
    "#            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "#            progress_queue.put(\n",
    "#                f\"Process: {mp.current_process().name}, File: {file_id}, Sequence: {seq_id}\"\n",
    "#            )\n",
    "#\n",
    "#            current_length, num_features = frames.shape\n",
    "#\n",
    "#            if current_length > FRAME_LEN:\n",
    "#                itp = interp1d(\n",
    "#                    np.linspace(0, 1, current_length),\n",
    "#                    frames,\n",
    "#                    axis=0,\n",
    "#                    kind=\"linear\",\n",
    "#                    fill_value=\"extrapolate\",\n",
    "#                )\n",
    "#                # Generate the new index array and apply interpolation\n",
    "#                new_index = np.linspace(0, 1, FRAME_LEN)\n",
    "#                frames = itp(new_index)\n",
    "#\n",
    "#            # Calculate the number of NaN values in each hand landmark\n",
    "#            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_indices]), axis=1) == 0)\n",
    "#            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_indices]), axis=1) == 0)\n",
    "#            no_nan = max(r_nonan, l_nonan)\n",
    "#\n",
    "#            if 2 * len(phrase) < no_nan:\n",
    "#                features = {\n",
    "#                    COL: tf.train.Feature(\n",
    "#                        float_list=tf.train.FloatList(\n",
    "#                            value=frames[:, col_to_index[COL]]\n",
    "#                        )\n",
    "#                    )\n",
    "#                    for COL in SEL_COLS\n",
    "#                }\n",
    "#                features[\"phrase\"] = tf.train.Feature(\n",
    "#                    bytes_list=tf.train.BytesList(value=[bytes(phrase, \"utf-8\")])\n",
    "#                )\n",
    "#\n",
    "#                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "#                record_bytes = example.SerializeToString()\n",
    "#\n",
    "#                buffer.append(record_bytes)\n",
    "#                if len(buffer) == buffer_size:\n",
    "#                    for record in buffer:\n",
    "#                        file_writer.write(record)\n",
    "#                        buffer = []\n",
    "#        if buffer:\n",
    "#            for record in buffer:\n",
    "#                file_writer.write(record)\n",
    "#\n",
    "#        # gc.collect()\n",
    "#\n",
    "#\n",
    "##cpu_count = int(mp.cpu_count() / 2)\n",
    "#cpu_count = 8  # 8\n",
    "#\n",
    "#\n",
    "#with Pool(cpu_count) as pool:\n",
    "#    progress_bars = [\n",
    "#        tqdm_notebook(desc=f\"Process {i + 1}\", unit=\"seq\") for i in range(cpu_count)\n",
    "#    ]\n",
    "#\n",
    "#    for result in pool.imap(\n",
    "#        process_file,\n",
    "#        dataset_df[\"file_id\"].unique(),\n",
    "#    ):\n",
    "#        progress_updates = []\n",
    "#        while not progress_queue.empty():\n",
    "#            progress_updates.append(progress_queue.get())\n",
    "#        for update, bar in zip(progress_updates, progress_bars):\n",
    "#            bar.set_description(update)\n",
    "#            bar.update()\n",
    "#\n",
    "#print(\"All parquets processed to TFRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split index: 54\n",
      "Total number of TFRecords: 68\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)  #\n",
    "\n",
    "\n",
    "# Encodes phrase into a tensor of tokens\n",
    "def tokenize_phrase(example):\n",
    "    phrase = example[\"phrase\"][0].decode(\n",
    "        \"utf-8\"\n",
    "    )  # Decode the byte string into a regular string\n",
    "    phrase = \"<\" + phrase + \">\"\n",
    "    indices = [json_chars.get(char, json_chars.get(\"F\")) for char in phrase]\n",
    "    example[\"phrase\"] = torch.tensor(\n",
    "        indices\n",
    "    )  # Replace the byte string with a list of integers\n",
    "    return example\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate phrases and sequence lengths\n",
    "    phrases = [seq.pop(\"phrase\") for seq in batch]\n",
    "    landmarks = [seq for seq in batch]\n",
    "\n",
    "    sequence_lengths = [len(next(iter(landmark.values()))) for landmark in landmarks]\n",
    "    phrase_lengths = [len(phrase) for phrase in phrases]\n",
    "\n",
    "    # Pad sequences and phrases\n",
    "    padded_batch = [\n",
    "        torch.stack(\n",
    "            [\n",
    "                F.pad(\n",
    "                    input=tensor,\n",
    "                    pad=(0, FRAME_LEN - tensor.shape[0]),\n",
    "                    mode=\"constant\",\n",
    "                    value=0,\n",
    "                )\n",
    "                for tensor in seq.values()\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        for seq in batch\n",
    "    ]\n",
    "\n",
    "    stacked_landmarks = torch.stack(padded_batch, dim=0)\n",
    "\n",
    "    padded_phrases = [\n",
    "        F.pad(\n",
    "            input=phrase,\n",
    "            pad=(0, 64 - len(phrase)),\n",
    "            mode=\"constant\",\n",
    "            value=61,\n",
    "        )\n",
    "        for phrase in phrases\n",
    "    ]\n",
    "\n",
    "    stacked_phrases = torch.stack(padded_phrases, dim=0)\n",
    "\n",
    "    return (\n",
    "        stacked_landmarks,\n",
    "        stacked_phrases,\n",
    "        torch.tensor(sequence_lengths),\n",
    "        torch.tensor(phrase_lengths),\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the split index\n",
    "tf_records = dataset_df.file_id.map(\n",
    "    lambda x: f\"/home/jpinn/asl-fingerspelling-recognition/src/preprocessed/{x}.tfrecord\"\n",
    ").unique()\n",
    "split_index = int(0.8 * len(tf_records))\n",
    "tf_records_len = len(tf_records)\n",
    "\n",
    "print(f\"Split index: {split_index}\" f\"\\nTotal number of TFRecords: {tf_records_len}\")\n",
    "\n",
    "\n",
    "def build_pipe(batch_size, drop_last, start, end, shuffle=True):\n",
    "    datapipe = FileLister(tf_records[start:end])\n",
    "\n",
    "    if shuffle:\n",
    "        datapipe = Shuffler(\n",
    "            datapipe, buffer_size=len(tf_records[start:end])\n",
    "        )  # Shuffle the dataset\n",
    "\n",
    "    datapipe = FileOpener(datapipe, mode=\"b\")\n",
    "    datapipe = TFRecordLoader(datapipe)\n",
    "    datapipe = Mapper(datapipe, tokenize_phrase)\n",
    "    datapipe = Batcher(datapipe, batch_size=batch_size, drop_last=drop_last)\n",
    "    datapipe = Collator(datapipe, collate_fn=collate_fn)\n",
    "    return datapipe\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LightningDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=256, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=0,\n",
    "            end=split_index,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "        return DataLoader2(\n",
    "            datapipe=train_datapipe,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=split_index,\n",
    "            end=tf_records_len,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return DataLoader2(\n",
    "            datapipe=val_datapipe,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=None, maxlen=None, num_hid=None):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.num_hid = num_hid\n",
    "        self.emb = nn.Embedding(num_embeddings=num_vocab, embedding_dim=num_hid)\n",
    "        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.size(1)\n",
    "        x = x.to(torch.int64)\n",
    "        x = self.emb(x)\n",
    "        x = x * torch.sqrt(torch.tensor(self.num_hid, dtype=torch.float, device=device))\n",
    "\n",
    "        return x + self.pos_emb[:maxlen, :].to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.to(device)\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, num_hid=342, maxlen=453):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=342,  # Correctly handle 342 landmark features\n",
    "                out_channels=num_hid,\n",
    "                kernel_size=11,\n",
    "                padding=5,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=num_hid, out_channels=num_hid, kernel_size=11, padding=5\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n",
    "        self.maxlen = maxlen\n",
    "        self.num_hid = num_hid\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Permute the tensor to have channels as the second dimension\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Change from [batch_size, seq_len, features] to [batch_size, features, seq_len]\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.permute(\n",
    "            0, 2, 1\n",
    "        )  # Optionally permute back if needed for further processing\n",
    "\n",
    "        x = x * torch.sqrt(torch.tensor(self.num_hid, dtype=torch.float, device=device))\n",
    "        return x + self.pos_emb[: x.size(1), :].to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        src2 = self.linear2(self.dropout2(self.linear1(src)))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            d_model, nhead, batch_first=True\n",
    "        )  # Encoder-decoder attention\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def causal_attention_mask(batch_size, seq_len, device):\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=device), diagonal=1\n",
    "        ).bool()\n",
    "        return mask[None, :, :].expand(batch_size * 4, seq_len, seq_len)\n",
    "\n",
    "    def forward(\n",
    "        self, enc_out, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        batch_size, seq_len, _ = target.size()\n",
    "\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, target.device)\n",
    "\n",
    "        tgt_key_padding_mask = tgt_key_padding_mask[:, :seq_len]\n",
    "\n",
    "        # Self-attention and layer norm\n",
    "        target_att_output_only, _ = self.self_attn(\n",
    "            target,\n",
    "            target,\n",
    "            target,\n",
    "            attn_mask=causal_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        target = self.norm1(target + self.dropout1(target_att_output_only))\n",
    "\n",
    "        # Encoder-decoder attention and layer norm\n",
    "        enc_att_output_only, _ = self.multihead_attn(\n",
    "            target,\n",
    "            enc_out,\n",
    "            enc_out,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        target = self.norm2(target + self.dropout2(enc_att_output_only))\n",
    "\n",
    "        # Feed forward network and layer norm\n",
    "        ffn_output = self.linear2(self.dropout(F.relu(self.linear1(target))))\n",
    "        target = self.norm3(target + self.dropout3(ffn_output))\n",
    "\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid,\n",
    "        num_head,\n",
    "        num_feed_forward,\n",
    "        source_maxlen,\n",
    "        target_maxlen,\n",
    "        num_layers_enc,\n",
    "        num_layers_dec,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_emb = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)ween\n",
    "        self.dec_emb = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_dec)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(num_hid, num_classes)\n",
    "\n",
    "    def decode(\n",
    "        self, enc_out, target, src_key_padding_mask=None, target_padding_mask=None\n",
    "    ):\n",
    "        y = self.dec_emb(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = self.decoder_layers[i](\n",
    "                enc_out, y, src_key_padding_mask, target_padding_mask\n",
    "            )\n",
    "        return y\n",
    "\n",
    "    def forward(\n",
    "        self, source, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        source = self.enc_emb(source)\n",
    "        memory = source\n",
    "        for layer in self.encoder:\n",
    "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decode(memory, target, src_key_padding_mask, tgt_key_padding_mask)\n",
    "        return self.classifier(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).float()\n",
    "        mask = mask.masked_fill(mask == 0, float(\"-inf\"))  # -inf for future positions\n",
    "        return mask.unsqueeze(0)  # Add batch dimension (1)\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        batch_size = source.size(0)\n",
    "        enc_output = self.encoder(source)\n",
    "\n",
    "        dec_input = torch.ones(batch_size, 1, dtype=torch.long) * target_start_token_idx\n",
    "        for _ in range(self.target_maxlen - 1):\n",
    "            tgt_mask = self.generate_square_subsequent_mask(dec_input.size(1)).to(\n",
    "                source.device\n",
    "            )\n",
    "            dec_out = self.decode(enc_output, dec_input, tgt_mask=tgt_mask)\n",
    "            prediction = self.classifier(dec_out[:, -1])\n",
    "            pred_idx = torch.argmax(prediction, dim=1, keepdim=True)\n",
    "            dec_input = torch.cat([dec_input, pred_idx], dim=1)\n",
    "\n",
    "        return dec_input.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class TransformerModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(config)\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.learning_rate = config[\"lr\"]\n",
    "        self.transformer = Transformer(\n",
    "            num_hid=200,\n",
    "            num_head=4,\n",
    "            num_feed_forward=400,\n",
    "            source_maxlen=384,\n",
    "            target_maxlen=64,\n",
    "            num_layers_enc=12,\n",
    "            num_layers_dec=2,\n",
    "            num_classes=62,\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=61)\n",
    "\n",
    "    def forward(\n",
    "        self, source, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        return self.transformer(\n",
    "            source,\n",
    "            target,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "\n",
    "    def create_tgt_mask(self, tgt_lengths, batch_size, seq_length):\n",
    "        \"\"\"Create a boolean mask for target sequences based on lengths.\"\"\"\n",
    "        tgt_key_padding_mask = torch.ones(\n",
    "            (batch_size, seq_length), device=self.device, dtype=torch.bool\n",
    "        )\n",
    "        for i, length in enumerate(tgt_lengths):\n",
    "            tgt_key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return tgt_key_padding_mask\n",
    "\n",
    "    def create_src_mask(self, src_lengths, batch_size, seq_length):\n",
    "        \"\"\"Create a boolean mask for source sequences based on lengths.\"\"\"\n",
    "        src_key_padding_mask = torch.ones(\n",
    "            (batch_size, seq_length), device=self.device, dtype=torch.bool\n",
    "        )\n",
    "        for i, length in enumerate(src_lengths):\n",
    "            src_key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return src_key_padding_mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        src_key_padding_mask = self.create_src_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_src_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        output = self.forward(\n",
    "            source,\n",
    "            target[:, :-1],\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        loss = self.criterion(output.transpose(1, 2), target[:, 1:])\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = val_batch\n",
    "\n",
    "        src_key_padding_mask = self.create_src_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        tgt_key_padding_mask = self.create_src_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        output = self.forward(\n",
    "            source,\n",
    "            target[:, :-1],\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "        val_loss = self.criterion(output.transpose(1, 2), target[:, 1:])\n",
    "\n",
    "        predicted = torch.argmax(output, dim=2)\n",
    "        correct = (predicted == target[:, 1:]).float()\n",
    "        val_accuracy = correct.sum() / correct.numel()\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            val_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_accuracy\",\n",
    "            val_accuracy,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.transformer.parameters(), lr=0.0045, weight_decay=0.08\n",
    "        )\n",
    "\n",
    "        # ADD BACK LINEAR WARMUP?\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "        # Chain scheduler/s\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jpinn/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/jpinn/asl-fingerspelling-recognition/src/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing the model...\n",
      "Lightning Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | transformer | Transformer      | 6.5 M \n",
      "1 | criterion   | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "25.964    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1207a793f84f25ae0d8b1f60ad380e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce9f125fe214e7d8c5bb561f3107992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52748ec02434a24a1c45f44a074d3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0502 23:24:17.437000 140109162844160 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0502 23:24:17.437000 140109162844160 torch/_dynamo/convert_frame.py:357]    function: 'torch_dynamo_resume_in_log_at_429' (/home/jpinn/.local/lib/python3.10/site-packages/pytorch_lightning/core/module.py:429)\n",
      "W0502 23:24:17.437000 140109162844160 torch/_dynamo/convert_frame.py:357]    last reason: ___check_type_id(L['self'], 94818531281216)                 \n",
      "W0502 23:24:17.437000 140109162844160 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0502 23:24:17.437000 140109162844160 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "Metric val_loss improved. New best score: 3.010\n",
      "Epoch 0, global step 99: 'val_loss' reached 3.00987 (best 3.00987), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v7.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b7213ed577452ba8b9bd3f6ebc7fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.259 >= min_delta = 0.0. New best score: 2.751\n",
      "Epoch 1, global step 198: 'val_loss' reached 2.75069 (best 2.75069), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v7.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7222e3bf7ba4759803fe8002394e909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.184 >= min_delta = 0.0. New best score: 2.567\n",
      "Epoch 2, global step 297: 'val_loss' reached 2.56682 (best 2.56682), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v7.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c90e4e91774d838efd19c72d5acc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.068 >= min_delta = 0.0. New best score: 2.499\n",
      "Epoch 3, global step 396: 'val_loss' reached 2.49884 (best 2.49884), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v7.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c82cce9d194d4e8c27868ac1cf5148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 2.467\n",
      "Epoch 4, global step 495: 'val_loss' reached 2.46661 (best 2.46661), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v7.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14bebbd121445e6b64da2c000a047e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 594: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8640fce8af4af9a7c6d63cb4b395ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.050 >= min_delta = 0.0. New best score: 2.417\n",
      "Epoch 6, global step 693: 'val_loss' reached 2.41700 (best 2.41700), saving model to '/home/jpinn/asl-fingerspelling-recognition/src/checkpoints/best-checkpoint-v7.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e62db6044d4e8cae58d3905200e584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    config = {\n",
    "        \"epochs\": 200,\n",
    "        \"lr\": 0.0045,\n",
    "        \"batch_size\": 256,\n",
    "    }\n",
    "\n",
    "    # Initialize callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=30,\n",
    "        verbose=True,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    # Set up Logger\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"transformer\")\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=config[\"epochs\"],\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"gpu\",\n",
    "        callbacks=[checkpoint_callback, lr_monitor, early_stop_callback],\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        enable_checkpointing=True,\n",
    "        precision=\"bf16-mixed\",\n",
    "        accumulate_grad_batches=2,\n",
    "        gradient_clip_val=4,\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    # Create Tuner\n",
    "    # tuner = Tuner(trainer)\n",
    "\n",
    "    ## Tune Learning Rate\n",
    "    # lr_find_result = tuner.lr_find(model, num_training=200, min_lr=1e-6, max_lr=1e-2)\n",
    "    # if lr_find_result:\n",
    "    #    model.learning_rate = lr_find_result.suggestion()\n",
    "\n",
    "    #\n",
    "    ## Start training with tuned parameters\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TransformerModule(config)\n",
    "\n",
    "    print(\"Optimizing the model...\")\n",
    "    # Compile the model using torch.compile\n",
    "    optimized_model = torch.compile(model)\n",
    "    \n",
    "    data_module = LightningDataModule(batch_size=256, shuffle=True)\n",
    "\n",
    "    print(\"Lightning Training the model...\")\n",
    "    trainer.fit(optimized_model, data_module)\n",
    "\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the idx_to_char mapping from the JSON file\n",
    "with open(\"idx_to_char.json\", \"r\") as f:\n",
    "    idx_to_char = json.load(f)\n",
    "\n",
    "# JSON keys are always strings, so you might need to convert them back to integers\n",
    "idx_to_char = {int(k): v for k, v in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_predictions(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    start_token_idx,\n",
    "    end_token_idx,\n",
    "    idx_to_char,\n",
    "    num_samples=10,\n",
    "):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    preds_list = []\n",
    "    ground_truth_list = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            source, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            # Assuming your model has a generate function similar to model.generate\n",
    "            # Adjust as necessary if your generation process is different\n",
    "            preds = model.generate(source, start_token_idx).cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "\n",
    "            bs = source.size(0)  # Batch size\n",
    "\n",
    "            for i in range(bs):\n",
    "                target_text = \"\".join(\n",
    "                    [idx_to_char[_] for _ in target[i, :] if _ != end_token_idx]\n",
    "                )\n",
    "                ground_truth_list.append(target_text)\n",
    "\n",
    "                prediction = \"\"\n",
    "                for idx in preds[i, :]:\n",
    "                    if idx == end_token_idx:\n",
    "                        break\n",
    "                    prediction += idx_to_char[idx]\n",
    "                preds_list.append(prediction)\n",
    "\n",
    "            if (\n",
    "                batch_idx >= num_samples - 1\n",
    "            ):  # Only sample a few batches for demonstration\n",
    "                break\n",
    "\n",
    "    # Print predictions\n",
    "    for i in range(min(num_samples, len(preds_list))):\n",
    "        print(f\"Ground Truth: {ground_truth_list[i]}\")\n",
    "        print(f\"Prediction: {preds_list[i]}\")\n",
    "        print(\"\\n~~~\\n\")\n",
    "\n",
    "\n",
    "# Call the function with your model, validation dataloader, and other necessary parameters\n",
    "generate_predictions(transformer, val_dataloader, device, 58, 59, idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Note, Reference, Brainstorm\n",
    "\n",
    "https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "\n",
    "## Input Embedding Layer\n",
    "\n",
    "The phrase must be vectorized (our case is chars)\n",
    "We must add Positional Encoding to create Positional Input Embeddings\n",
    "\n",
    "## Encoder Layer\n",
    "\n",
    "Two sub-modules:\n",
    "\n",
    "### Attention\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "3 Distinct fully connected layers\n",
    "\n",
    "- Query, Key, Value\n",
    "- A dot product of the Query and Key matrices is computed to create a score matrix.\n",
    "- The score matrix a table that dictatates how much value each word or char should be given, compared to the other words or chars in the input sequence. Higher score = more important. = more focus.\n",
    "- The score matrix is normalized by dividing by the square root of the dimension of the key vectors.\n",
    "- The score matrix is divided by the square root of the dimension of the key vectors which gives the scaled scores.\n",
    "- The scaled scores are then passed through a softmax function to receive the attention weights.\n",
    "- Multiply attention weights by value matrix to get the output vector of the self-attention layer.\n",
    "- Linear layer to process.\n",
    "\n",
    "#### Multi-headed Attention\n",
    "\n",
    "The query, key, and value is split into N heads.\n",
    "\n",
    "- Each vector goes through the attention layer as normal\n",
    "- The output of all heads is concatenated into a single vector\n",
    "- Each head is given a different representation of the input sequence, allowing the model to simultaneously attend to information from different representation subspaces.\n",
    "- Each head in theory should learn to attend to different parts of the input sequence, and thus overall learn more of the input sequence.\n",
    "\n",
    "### Residual Connection, Layer Normalization, and Feed Forward Layer\n",
    "\n",
    "- The Multi-headed attention output vector is added to the original input vector to the sub-layer, which is called a residual connection.\n",
    "- This output is then normalized by layer normalization.\n",
    "- This enters a feed forward network, which is a simple 2 layer fully connected network with a ReLU activation in between.\n",
    "- The output of that is added again to the original input, to be normalized again. Like before with the multi-headed attention.\n",
    "\n",
    "## Decoder Layer\n",
    "\n",
    "### Output Embedding Layer and Positional Encoding\n",
    "\n",
    "- The output goes through an embedding layer to get the position embeddings.\n",
    "- This enters the first multi-headed attention layer.\n",
    "- The scaled scores are added to a look ahead mask, which prevents the decoder from attending to future tokens.\n",
    "- This happens when the softmax makes future tokens 0, so no attention is given to them.\n",
    "- All the heads are combined to create a masked output layer.\n",
    "\n",
    "The second multi-headed attention layer has the query and key of the encoder output, and the value of the previous multi-headed attention layer output value.\n",
    "\n",
    "A final feed forward network is applied to the output of the second multi-headed attention layer.\n",
    "\n",
    "### Classifier, softmax, and output\n",
    "\n",
    "The feed forward output enters a linear classifier.\n",
    "\n",
    "Classifier output enters softmax to get the probability score of each class (char). The index of the highest probability is the predicted char.\n",
    "\n",
    "Output is added to a list of decoded outputs until the end token is reached.\n",
    "\n",
    "This whole structure can be stacked N layers high. The output of the final encoder, is input into all the decoder layers, who are taking the input from the previous decoder layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "```python\n",
    "import tensorflow_addons as tfa\n",
    "pbar = tfa.callbacks.TQDMProgressBar()\n",
    "model.fit(…,callbacks=[pbar])\n",
    "# TQDMProgressBar() also works with evaluate()\n",
    "model.evaluate(…,callbacks=[pb\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Multiprocessing\n",
    "\n",
    "```python\n",
    "with Pool(workers) as pool:\n",
    "    results = list(tqdm(pool.imap(worker,thread_list, total=len(thread_list))\n",
    "                        ar])\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Padding strategies\n",
    "\n",
    "1. No Padding with <EOS> token: This is the most efficient and elegant approach for Transformers.\n",
    "\n",
    "2. Full Padding: Pad all phrases (and potentially feature sequences) to a fixed maximum length using a constant value or specific technique. This can be simpler to implement but introduces unnecessary computational overhead and potential information distortion due to large padding sections.\n",
    "\n",
    "3. Full Padding with Masking: This combines the simplicity of full padding with the benefits of masking. While you pad all sequences to a fixed length, you apply masking during training to prevent the model from attending to the padded regions. This can be a good compromise if your model struggles with highly variable sequence lengths but you still want to avoid the downsides of excessive padding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
