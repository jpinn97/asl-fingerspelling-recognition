{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read the first CSV file\n",
    "dataset_train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "dataset_supplemental_df = pd.read_csv(\"supplemental_metadata.csv\")\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "dataset_df = pd.concat([dataset_train_df, dataset_supplemental_df], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV file\n",
    "dataset_df.to_csv(\"train_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: train_landmarks/5414471.parquet, sequence_id: 1816796431, file_id: 5414471, phrase: 3 creekhouse\n",
      "Full sequence dataset shape is (123, 1630)\n"
     ]
    }
   ],
   "source": [
    "# Read the first row of the DataFrame\n",
    "path, sequence_id, file_id, phrase = dataset_df.iloc[0][\n",
    "    [\"path\", \"sequence_id\", \"file_id\", \"phrase\"]\n",
    "]\n",
    "print(f\"path: {path}, sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "sample_sequence_df = pq.read_table(\n",
    "    f\"{str(path)}\",\n",
    "    filters=[\n",
    "        [(\"sequence_id\", \"=\", sequence_id)],\n",
    "    ],\n",
    ").to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 121\n"
     ]
    }
   ],
   "source": [
    "# Read the total amount unique files\n",
    "unique_paths = dataset_df[\"path\"].unique()\n",
    "\n",
    "sum = unique_paths.shape[0]\n",
    "\n",
    "print(\"Total number of files: {}\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "\n",
    "FACE = [f'x_face_{i}' for i in LIP] + [f'y_face_{i}' for i in LIP] + [f'z_face_{i}' for i in LIP]\n",
    "LHAND = [f'x_left_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)]\n",
    "RHAND = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n",
    "POSE = [f'x_pose_{i}' for i in range(33)] + [f'y_pose_{i}' for i in range(33)] + [f'z_pose_{i}' for i in range(33)]\n",
    "\n",
    "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
    "FRAME_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee89b205714044a3b8584e51af81cc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 1: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e087b15ad78a4f638653b245d07912e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 2: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615872ed92774e199060fb3aae4e690e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 3: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b611b5a04344b2284df8bf8b664e9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 4: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5c83b23150439980bb8dd1b03df93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 5: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0671b63a0fb3481fb9ed16d8c09f5b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 6: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6532d098174140aa52872e24cad73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 7: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decbb55d2bb747098c5dd3414de2a447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Thread 8: 0seq [00:00, ?seq/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(cpu_count) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     77\u001b[0m     progress_bars \u001b[38;5;241m=\u001b[39m [tqdm_notebook(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThread \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cpu_count)]\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap(\n\u001b[1;32m     80\u001b[0m         process_file,\n\u001b[1;32m     81\u001b[0m         dataset_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique(),\n\u001b[1;32m     82\u001b[0m     ):\n\u001b[1;32m     83\u001b[0m         progress_updates \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m progress_queue\u001b[38;5;241m.\u001b[39mempty():\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a Manager object for the progress_queue\n",
    "manager = Manager()\n",
    "progress_queue = manager.Queue()\n",
    "\n",
    "def process_file(file_id):\n",
    "    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "    path = file_df[\"path\"].values[0]\n",
    "    parquet_df = pq.read_table(path, columns=[\"sequence_id\"] + SEL_COLS).to_pandas()\n",
    "\n",
    "    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "    parquet_numpy = parquet_df.to_numpy(copy=False)\n",
    "\n",
    "    col_to_index = {col: i for i, col in enumerate(parquet_df.columns)}\n",
    "\n",
    "    LHAND_indices = [col_to_index[col] for col in LHAND]\n",
    "    RHAND_indices = [col_to_index[col] for col in RHAND]\n",
    "\n",
    "    buffer_size = 1000  # Adjust as needed\n",
    "    buffer = []\n",
    "\n",
    "    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "        for seq_id, phrase in zip(file_df[\"sequence_id\"], file_df[\"phrase\"]):\n",
    "            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "            progress_queue.put(\n",
    "                f\"Thread: {mp.current_process().name}, File: {file_id}, Sequence: {seq_id}\"\n",
    "            )\n",
    "\n",
    "            # Calculate the number of NaN values in each hand landmark\n",
    "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_indices]), axis=1) == 0)\n",
    "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_indices]), axis=1) == 0)\n",
    "            no_nan = max(r_nonan, l_nonan)\n",
    "\n",
    "            if 2 * len(phrase) < no_nan:\n",
    "                features = {\n",
    "                    COL: tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(\n",
    "                            value=frames[:, col_to_index[COL]]\n",
    "                        )\n",
    "                    )\n",
    "                    for COL in SEL_COLS\n",
    "                }\n",
    "                features[\"phrase\"] = tf.train.Feature(\n",
    "                    bytes_list=tf.train.BytesList(value=[bytes(phrase, \"utf-8\")])\n",
    "                )\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "                record_bytes = example.SerializeToString()\n",
    "\n",
    "                buffer.append(record_bytes)\n",
    "                if len(buffer) == buffer_size:\n",
    "                    for record in buffer:\n",
    "                        file_writer.write(record)\n",
    "                    buffer = []\n",
    "\n",
    "        if buffer:\n",
    "            for record in buffer:\n",
    "                file_writer.write(record)\n",
    "\n",
    "        # gc.collect()\n",
    "\n",
    "\n",
    "# cpu_count = int(mp.cpu_count() / 2)\n",
    "cpu_count = 8\n",
    "\n",
    "with Pool(cpu_count) as pool:\n",
    "    progress_bars = [tqdm_notebook(desc=f\"Thread {i + 1}\", unit=\"seq\") for i in range(cpu_count)]\n",
    "\n",
    "    for result in pool.imap(\n",
    "        process_file,\n",
    "        dataset_df[\"file_id\"].unique(),\n",
    "    ):\n",
    "        progress_updates = []\n",
    "        while not progress_queue.empty():\n",
    "            progress_updates.append(progress_queue.get())\n",
    "        for update, bar in zip(progress_updates, progress_bars):\n",
    "            bar.set_description(update)\n",
    "            bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "raw_dataset = tf.data.TFRecordDataset(\"preprocessed/5414471.tfrecord\")\n",
    "\n",
    "for raw_record in raw_dataset.take(10):\n",
    "    print(\"im here!\")\n",
    "    print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdata.datapipes.iter import FileLister, FileOpener, TFRecordLoader\n",
    "\n",
    "tf_records_directory = 'preprocessed/'\n",
    "\n",
    "file_lister = FileLister(tf_records_directory, \"*.tfrecord\") # Lists all tfrecords\n",
    "\n",
    "file_opener = FileOpener(file_lister, mode=\"b\") # Opens files, b for binary mode\n",
    "\n",
    "tfrecord_loader = TFRecordLoader(file_opener) # Load and decodes tfrecords\n",
    "\n",
    "# Process an example\n",
    "def process_tf_examples(example):\n",
    "    # Perform even more preprocessing\n",
    "    # Extract things\n",
    "    return example\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(tfrecord_loader, batch_size=32, num_workers=4)\n",
    "\n",
    "# Now you can iterate over the dataloader\n",
    "for batch in dataloader:\n",
    "    # Each batch is a tuple of (data, target)\n",
    "    data, target = batch\n",
    "    # Now you can use the data and target for further steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "pbar = tfa.callbacks.TQDMProgressBar()\n",
    "model.fit(…,callbacks=[pbar])\n",
    "# TQDMProgressBar() also works with evaluate()\n",
    "model.evaluate(…,callbacks=[pb\n",
    "\n",
    "# Multiprocessing\n",
    "\n",
    "with Pool(workers) as pool:\n",
    "    results = list(tqdm(pool.imap(worker,thread_list, total=len(thread_list))\n",
    "                        ar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
