{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 01:44:39.094747: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-07 01:44:39.161760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-07 01:44:39.893734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "import torch._dynamo\n",
    "from torchdata.dataloader2 import DataLoader2, MultiProcessingReadingService\n",
    "from torchdata.datapipes.iter import (\n",
    "    FileLister,\n",
    "    FileOpener,\n",
    "    TFRecordLoader,\n",
    "    Mapper,\n",
    "    Batcher,\n",
    "    Collator,\n",
    "    Shuffler,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    EarlyStopping,\n",
    "    RichModelSummary\n",
    ")\n",
    "\n",
    "\n",
    "# Set the default matmul precision to medium, or high/highest?\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# Read the first CSV file\n",
    "dataset_train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Read the second CSV file\n",
    "dataset_supplemental_df = pd.read_csv(\"supplemental_metadata.csv\")\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "dataset_df = pd.concat([dataset_train_df, dataset_supplemental_df], ignore_index=True)\n",
    "\n",
    "# Save the combined CSV file\n",
    "# dataset_df.to_csv(\"train_full.csv\", index=False)\n",
    "dataset_df = dataset_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read the first row of the DataFrame\n",
    "path, sequence_id, file_id, phrase = dataset_df.iloc[0][\n",
    "    [\"path\", \"sequence_id\", \"file_id\", \"phrase\"]\n",
    "]\n",
    "print(f\"path: {path}, sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "sample_sequence_df = pq.read_table(\n",
    "    f\"{str(path)}\",\n",
    "    filters=[\n",
    "        [(\"sequence_id\", \"=\", sequence_id)],\n",
    "    ],\n",
    ").to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files: 68\n"
     ]
    }
   ],
   "source": [
    "# Read the total amount unique files\n",
    "unique_paths = dataset_df[\"path\"].unique()\n",
    "\n",
    "sum = unique_paths.shape[0]\n",
    "\n",
    "print(\"Total number of files: {}\".format(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "LIP = [\n",
    "    61,\n",
    "    185,\n",
    "    40,\n",
    "    39,\n",
    "    37,\n",
    "    267,\n",
    "    269,\n",
    "    270,\n",
    "    409,\n",
    "    291,\n",
    "    146,\n",
    "    91,\n",
    "    181,\n",
    "    84,\n",
    "    17,\n",
    "    314,\n",
    "    405,\n",
    "    321,\n",
    "    375,\n",
    "    78,\n",
    "    191,\n",
    "    80,\n",
    "    81,\n",
    "    82,\n",
    "    13,\n",
    "    312,\n",
    "    311,\n",
    "    310,\n",
    "    415,\n",
    "    95,\n",
    "    88,\n",
    "    178,\n",
    "    87,\n",
    "    14,\n",
    "    317,\n",
    "    402,\n",
    "    318,\n",
    "    324,\n",
    "    308,\n",
    "]\n",
    "\n",
    "FACE = (\n",
    "    [f\"x_face_{i}\" for i in LIP]\n",
    "    + [f\"y_face_{i}\" for i in LIP]\n",
    "    + [f\"z_face_{i}\" for i in LIP]\n",
    ")\n",
    "LHAND = (\n",
    "    [f\"x_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_left_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_left_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "RHAND = (\n",
    "    [f\"x_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"y_right_hand_{i}\" for i in range(21)]\n",
    "    + [f\"z_right_hand_{i}\" for i in range(21)]\n",
    ")\n",
    "POSE = (\n",
    "    [f\"x_pose_{i}\" for i in range(0, 23)]\n",
    "    + [f\"y_pose_{i}\" for i in range(0, 23)]\n",
    "    + [f\"z_pose_{i}\" for i in range(0, 23)]\n",
    ")\n",
    "\n",
    "SEL_COLS = FACE + LHAND + RHAND + POSE\n",
    "FRAME_LEN = 384 #384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read the existing data\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)\n",
    "\n",
    "# Define the new entries\n",
    "new_entries = [\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"P\",\n",
    "]\n",
    "\n",
    "# Add the new entries starting from index 59, only if they don't already exist\n",
    "for i, entry in enumerate(new_entries, start=59):\n",
    "    if entry not in json_chars:\n",
    "        json_chars[entry] = i\n",
    "\n",
    "# Write the updated data back to the file\n",
    "with open(\"character_to_prediction_index.json\", \"w\") as f:\n",
    "    json.dump(json_chars, f, indent=4)\n",
    "\n",
    "start_token_idx = 59\n",
    "end_token_idx = 60\n",
    "pad_token_idx = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf.config.set_visible_devices([], \"GPU\")  # Disable GPU for Tensorflow\n",
    "#\n",
    "## Create a Manager object for the progress_queue\n",
    "#manager = Manager()\n",
    "#progress_queue = manager.Queue()\n",
    "#\n",
    "#\n",
    "#def process_file(file_id):\n",
    "#    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "#    path = file_df[\"path\"].values[0]\n",
    "#    parquet_df = pq.read_table(path, columns=[\"sequence_id\"] + SEL_COLS).to_pandas()\n",
    "#\n",
    "#    features = [FACE, LHAND, RHAND, POSE]\n",
    "#\n",
    "#    for feature in features:\n",
    "#        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "#        parquet_df[feature] = scaler.fit_transform(parquet_df[feature])\n",
    "#\n",
    "#    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "#    parquet_numpy = parquet_df.to_numpy(copy=False)\n",
    "#    col_to_index = {col: i for i, col in enumerate(parquet_df.columns)}\n",
    "#    LHAND_indices = [col_to_index[col] for col in LHAND]\n",
    "#    RHAND_indices = [col_to_index[col] for col in RHAND]\n",
    "#\n",
    "#    buffer_size = 1000  # Adjust as needed\n",
    "#    buffer = []\n",
    "#\n",
    "#    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "#        for seq_id, phrase in zip(file_df[\"sequence_id\"], file_df[\"phrase\"]):\n",
    "#            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "#            progress_queue.put(\n",
    "#                f\"Process: {mp.current_process().name}, File: {file_id}, Sequence: {seq_id}\"\n",
    "#            )\n",
    "#\n",
    "#            if frames.shape[0] > FRAME_LEN:\n",
    "#                itp = interp1d(\n",
    "#                    np.linspace(0, 1, len(frames)),\n",
    "#                    frames,\n",
    "#                    axis=0,\n",
    "#                    kind=\"linear\",\n",
    "#                    fill_value=\"extrapolate\",\n",
    "#                )\n",
    "#                # Generate the new index array and apply interpolation\n",
    "#                frames = itp(np.linspace(0, 1, FRAME_LEN))\n",
    "#\n",
    "#            # Calculate the number of NaN values in each hand landmark\n",
    "#            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_indices]), axis=1) == 0)\n",
    "#            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_indices]), axis=1) == 0)\n",
    "#            no_nan = max(r_nonan, l_nonan)\n",
    "#\n",
    "#            frames = np.nan_to_num(frames, nan=0)\n",
    "#\n",
    "#            num_hand_frames = np.sum(\n",
    "#                np.any(frames[:, LHAND_indices + RHAND_indices] != 0, axis=1)\n",
    "#            )\n",
    "#\n",
    "#            if frames.shape[0] < 50 and num_hand_frames < 3:\n",
    "#                phrase = \"2 a-e -aroe\"\n",
    "#\n",
    "#            if 2 * len(phrase) < no_nan:\n",
    "#                features = {\n",
    "#                    COL: tf.train.Feature(\n",
    "#                        float_list=tf.train.FloatList(\n",
    "#                            value=frames[:, col_to_index[COL]]\n",
    "#                        )\n",
    "#                    )\n",
    "#                    for COL in SEL_COLS\n",
    "#                }\n",
    "#                features[\"phrase\"] = tf.train.Feature(\n",
    "#                    bytes_list=tf.train.BytesList(value=[bytes(phrase, \"utf-8\")])\n",
    "#                )\n",
    "#                example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "#                record_bytes = example.SerializeToString()\n",
    "#\n",
    "#                buffer.append(record_bytes)\n",
    "#                if len(buffer) == buffer_size:\n",
    "#                    for record in buffer:\n",
    "#                        file_writer.write(record)\n",
    "#                        buffer = []\n",
    "#\n",
    "#        if buffer:\n",
    "#            for record in buffer:\n",
    "#                file_writer.write(record)\n",
    "#\n",
    "#    # gc.collect()\n",
    "#\n",
    "#\n",
    "## cpu_count = int(mp.cpu_count() / 2)\n",
    "#cpu_count = 8  # 8\"\"\"  \"\"\"\n",
    "#\n",
    "#\n",
    "#with Pool(cpu_count) as pool:\n",
    "#    progress_bars = [\n",
    "#        tqdm_notebook(desc=f\"Process {i + 1}\", unit=\"seq\") for i in range(cpu_count)\n",
    "#    ]\n",
    "#\n",
    "#    for result in pool.imap(\n",
    "#        process_file,\n",
    "#        dataset_df[\"file_id\"].unique(),\n",
    "#    ):\n",
    "#        progress_updates = []\n",
    "#        while not progress_queue.empty():\n",
    "#            progress_updates.append(progress_queue.get())\n",
    "#        for update, bar in zip(progress_updates, progress_bars):\n",
    "#            bar.set_description(update)\n",
    "#            bar.update()\n",
    "#\n",
    "#print(\"All parquets processed to TFRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split index: 10\n",
      "Total number of TFRecords: 13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "with open(\"character_to_prediction_index.json\", \"r\") as f:\n",
    "    json_chars = json.load(f)  #\n",
    "\n",
    "\n",
    "# Encodes phrase into a tensor of tokens\n",
    "def tokenize_phrase(example):\n",
    "    phrase = example[\"phrase\"][0].decode(\n",
    "        \"utf-8\"\n",
    "    )  # Decode the byte string into a regular string\n",
    "    phrase = \"<\" + phrase + \">\"\n",
    "    indices = [json_chars.get(char, json_chars.get(\"F\")) for char in phrase]\n",
    "    example[\"phrase\"] = torch.tensor(\n",
    "        indices\n",
    "    )  # Replace the byte string with a list of integers\n",
    "    return example\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate phrases and sequence lengths\n",
    "    phrases = [seq.pop(\"phrase\") for seq in batch]\n",
    "    landmarks = [seq for seq in batch]\n",
    "\n",
    "    sequence_lengths = [len(next(iter(landmark.values()))) for landmark in landmarks]\n",
    "    phrase_lengths = [len(phrase) for phrase in phrases]\n",
    "\n",
    "    # Pad sequences and phrases\n",
    "    padded_batch = [\n",
    "        torch.stack(\n",
    "            [\n",
    "                F.pad(\n",
    "                    input=tensor,\n",
    "                    pad=(0, FRAME_LEN - tensor.shape[0]),\n",
    "                    mode=\"constant\",\n",
    "                    value=0,\n",
    "                )\n",
    "                for tensor in seq.values()\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        for seq in batch\n",
    "    ]\n",
    "\n",
    "    stacked_landmarks = torch.stack(padded_batch, dim=0)\n",
    "\n",
    "    padded_phrases = [\n",
    "        F.pad(\n",
    "            input=phrase,\n",
    "            pad=(0, 64 - len(phrase)),\n",
    "            mode=\"constant\",\n",
    "            value=61,\n",
    "        )\n",
    "        for phrase in phrases\n",
    "    ]\n",
    "\n",
    "    stacked_phrases = torch.stack(padded_phrases, dim=0)\n",
    "\n",
    "    return (\n",
    "        stacked_landmarks,\n",
    "        stacked_phrases,\n",
    "        torch.tensor(sequence_lengths),\n",
    "        torch.tensor(phrase_lengths),\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the split index\n",
    "tf_records = dataset_df.file_id.map(\n",
    "    lambda x: f\"/home/jpinn/asl-fingerspelling-recognition/src/preprocessed/{x}.tfrecord\"\n",
    ").unique()\n",
    "\n",
    "# Sample 20% of the TFRecords\n",
    "sample_size = int(0.2 * len(tf_records))  # Calculate 20% of the total records\n",
    "tf_records = random.sample(list(tf_records), sample_size)\n",
    "\n",
    "split_index = int(0.8 * len(tf_records))\n",
    "tf_records_len = len(tf_records)\n",
    "\n",
    "print(f\"Split index: {split_index}\" f\"\\nTotal number of TFRecords: {tf_records_len}\")\n",
    "\n",
    "\n",
    "def build_pipe(batch_size, drop_last, start, end, shuffle=True):\n",
    "    datapipe = FileLister(tf_records[start:end])\n",
    "\n",
    "    if shuffle:\n",
    "        datapipe = Shuffler(\n",
    "            datapipe, buffer_size=len(tf_records[start:end])\n",
    "        )  # Shuffle the dataset\n",
    "\n",
    "    datapipe = FileOpener(datapipe, mode=\"b\")\n",
    "    datapipe = TFRecordLoader(datapipe)\n",
    "    datapipe = Mapper(datapipe, tokenize_phrase)\n",
    "    datapipe = Batcher(datapipe, batch_size=batch_size, drop_last=drop_last)\n",
    "    datapipe = Collator(datapipe, collate_fn=collate_fn)\n",
    "    return datapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=64, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=0,\n",
    "            end=split_index,\n",
    "            shuffle=self.shuffle,\n",
    "        )\n",
    "        return DataLoader2(\n",
    "            datapipe=train_datapipe,\n",
    "            reading_service=MultiProcessingReadingService(num_workers=8),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_datapipe = build_pipe(\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            start=split_index,\n",
    "            end=tf_records_len,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return DataLoader2(\n",
    "            datapipe=val_datapipe,\n",
    "            reading_service=MultiProcessingReadingService(num_workers=8),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=None, maxlen=None, num_hid=None, device=\"cuda\"):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.num_hid = num_hid\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=num_vocab, embedding_dim=num_hid, padding_idx=61, device=self.device\n",
    "        )\n",
    "        # Compute scaling factor once\n",
    "        self.scale = torch.sqrt(\n",
    "            torch.tensor(num_hid, dtype=torch.float, device=self.device)\n",
    "        )\n",
    "        # Initialize positional encoding and move to device\n",
    "        self.pos_emb = self.positional_encoding(maxlen - 1, num_hid).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.int64)  # Ensure input is of type long for embedding lookup\n",
    "        x = self.emb(x)\n",
    "        x = x * self.scale\n",
    "        return x + self.pos_emb[: x.size(1), :]\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, num_hid=312, output_dim=312, maxlen=384, device=\"cuda\"):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.output_dim = output_dim\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        # Multi-scale Convolutional Setup\n",
    "        self.multi_scale_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=num_hid,\n",
    "                        out_channels=num_hid // 3,\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                        dilation=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(num_hid // 3),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(0.05),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=num_hid,\n",
    "                        out_channels=num_hid // 3,\n",
    "                        kernel_size=5,\n",
    "                        padding=4,\n",
    "                        dilation=2,\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(num_hid // 3),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(0.05),\n",
    "                ),\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=num_hid,\n",
    "                        out_channels=num_hid // 3,\n",
    "                        kernel_size=11,\n",
    "                        padding=15,\n",
    "                        dilation=3,\n",
    "                    ),\n",
    "                    nn.BatchNorm1d(num_hid // 3),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout(0.05),\n",
    "                ),\n",
    "            ]\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Final Convolutional Layer to aggregate multi-scale features and project to output_dim\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_hid, out_channels=output_dim, kernel_size=1),\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_emb = self.positional_encoding(maxlen, output_dim).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Permute to fit Conv1d input requirements\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, features, seq_len]\n",
    "\n",
    "        # Apply each multi-scale convolutional block and concatenate outputs\n",
    "        conv_outputs = [conv(x) for conv in self.multi_scale_conv]\n",
    "        x = torch.cat(conv_outputs, dim=1)  # Concatenate along the channel dimension\n",
    "\n",
    "        # Apply final convolutional layer to aggregate features\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # Permute back to [batch_size, seq_len, features] for positional encoding addition\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, seq_len, features]\n",
    "\n",
    "        # Scale embeddings and add positional encoding\n",
    "        scale = torch.sqrt(\n",
    "            torch.tensor(self.output_dim, dtype=torch.float, device=self.device)\n",
    "        )\n",
    "        x = x * scale\n",
    "        x = (\n",
    "            x + self.pos_emb[: x.size(1), :]\n",
    "        )  # Adjust positional encoding to match sequence length\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(maxlen, d_model):\n",
    "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Transformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid,\n",
    "        num_head,\n",
    "        num_feed_forward,\n",
    "        source_maxlen,\n",
    "        target_maxlen,\n",
    "        num_layers_enc,\n",
    "        num_layers_dec,\n",
    "        num_classes,\n",
    "        batch_first=True,\n",
    "    ):\n",
    "        self.batch_first = batch_first\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_emb = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_emb = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            *[\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    num_hid,\n",
    "                    num_head,\n",
    "                    num_feed_forward,\n",
    "                    dropout=0.1,\n",
    "                    activation=\"relu\",\n",
    "                    layer_norm_eps=1e-5,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                )\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerDecoderLayer(\n",
    "                    num_hid,\n",
    "                    num_head,\n",
    "                    num_feed_forward,\n",
    "                    dropout=0.1,\n",
    "                    activation=\"relu\",\n",
    "                    layer_norm_eps=1e-5,\n",
    "                    batch_first=True,\n",
    "                    bias=True,\n",
    "                )\n",
    "                for _ in range(num_layers_dec)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(num_hid, num_classes)\n",
    "\n",
    "    def generate_look_ahead_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size, dtype=torch.bfloat16), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def encode(self, src, src_key_padding_mask=None):\n",
    "        memory = self.enc_emb(src)\n",
    "        for layer in self.encoder_layers:  # Adjusted to use self.encoder\n",
    "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
    "        return memory\n",
    "\n",
    "    def decode(\n",
    "        self, enc_out, target, src_key_padding_mask=None, tgt_key_padding_mask=None\n",
    "    ):\n",
    "        y = self.dec_emb(target)\n",
    "\n",
    "        tgt_mask = self.generate_look_ahead_mask(target.size(1)).to(y.device)\n",
    "\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = self.decoder_layers[i](\n",
    "                y,\n",
    "                enc_out,\n",
    "                tgt_mask=tgt_mask,  # Look-ahead mask\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,  # Target padding mask\n",
    "                memory_key_padding_mask=src_key_padding_mask,  # Source padding mask for cross-attention\n",
    "            )\n",
    "        return y\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        memory = self.encode(src, src_key_padding_mask)\n",
    "        output = self.decode(memory, tgt, tgt_key_padding_mask)\n",
    "        return self.classifier(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class LightningTransformer(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(config)\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.model = torch.compile(\n",
    "            Transformer(\n",
    "                num_hid=312,\n",
    "                num_head=4,\n",
    "                num_feed_forward=1228,\n",
    "                source_maxlen=384,\n",
    "                target_maxlen=64,\n",
    "                num_layers_enc=2,\n",
    "                num_layers_dec=1,\n",
    "                num_classes=62,\n",
    "            )\n",
    "        )\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=61)\n",
    "\n",
    "    def forward(self, source, target, src_key_padding_mask, tgt_key_padding_mask):\n",
    "        return self.model(source, target, src_key_padding_mask, tgt_key_padding_mask)\n",
    "\n",
    "    def create_mask(self, lengths, batch_size, seq_length):\n",
    "        \"\"\"Create a boolean mask for source sequences based on lengths.\"\"\"\n",
    "        key_padding_mask = torch.ones(\n",
    "            (batch_size, seq_length), device=self.device, dtype=torch.bfloat16\n",
    "        )\n",
    "        for i, length in enumerate(lengths):\n",
    "            key_padding_mask[i, 0:length] = False\n",
    "\n",
    "        return key_padding_mask\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs, target)\n",
    "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        src_key_padding_mask = self.create_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        dec_input = target[:, :-1]  # shift target\n",
    "        dec_target = target[:, 1:]  # shift back for prediction\n",
    "\n",
    "        # create tgt mask\n",
    "        tgt_key_padding_mask = self.create_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        # shift mask to account for shifted target\n",
    "        tgt_key_padding_mask = tgt_key_padding_mask[:, :-1]\n",
    "\n",
    "        output = self(\n",
    "            source,\n",
    "            dec_input,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        loss = self.loss(output.transpose(1, 2), dec_target)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        source, target, src_lengths, tgt_lengths = batch\n",
    "\n",
    "        src_key_padding_mask = self.create_mask(\n",
    "            src_lengths, source.size(0), source.size(1)\n",
    "        )\n",
    "\n",
    "        dec_input = target[:, :-1]  # shift target\n",
    "        dec_target = target[:, 1:]  # shift back for prediction\n",
    "\n",
    "        # create tgt mask\n",
    "        tgt_key_padding_mask = self.create_mask(\n",
    "            tgt_lengths, target.size(0), target.size(1)\n",
    "        )\n",
    "\n",
    "        # shift mask to account for shifted target\n",
    "        tgt_key_padding_mask = tgt_key_padding_mask[:, :-1]\n",
    "\n",
    "        output = self(\n",
    "            source,\n",
    "            dec_input,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        val_loss = self.loss(output.transpose(1, 2), dec_target)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    # def predict_step(self, batch):\n",
    "    #        inputs, target = batch\n",
    "    #        return self.model(inputs, target)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.learning_rate, weight_decay=0.01\n",
    "        )\n",
    "        scheduler1 = torch.optim.lr_scheduler.ConstantLR(\n",
    "            optimizer, factor=1, total_iters=20\n",
    "        )\n",
    "\n",
    "        scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=180)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer, schedulers=[scheduler1, scheduler2], milestones=[20]\n",
    "        )\n",
    "\n",
    "        # Chain scheduler/s\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jpinn/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 200,\n",
    "    \"learning_rate\": 0.0045,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "# Initialize callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=30,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "# Set up Logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"transformer\")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=200,\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[checkpoint_callback, lr_monitor, early_stop_callback, RichModelSummary(max_depth=5)],\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=True,\n",
    "    precision=\"bf16-mixed\",\n",
    "    accumulate_grad_batches=4,\n",
    "    gradient_clip_val=2,\n",
    "    logger=logger,\n",
    ")\n",
    "# Initialize the model\n",
    "\n",
    "model = LightningTransformer(config)\n",
    "\n",
    "data_module = LightningDataModule(batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer)\n",
    "\n",
    "# Run learning rate finder\n",
    "lr_finder = tuner.lr_find(model, data_module, num_training=250, mode = \"exponential\")\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "if lr_finder:\n",
    "    model.learning_rate = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lightning Training the model...\")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the idx_to_char mapping from the JSON file\n",
    "with open(\"idx_to_char.json\", \"r\") as f:\n",
    "    idx_to_char = json.load(f)\n",
    "\n",
    "# JSON keys are always strings, so you might need to convert them back to integers\n",
    "idx_to_char = {int(k): v for k, v in idx_to_char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def generate_predictions(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    start_token_idx,\n",
    "    end_token_idx,\n",
    "    idx_to_char,\n",
    "    num_samples=10,\n",
    "):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    preds_list = []\n",
    "    ground_truth_list = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            source, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            # Assuming your model has a generate function similar to model.generate\n",
    "            # Adjust as necessary if your generation process is different\n",
    "            preds = model.generate(source, start_token_idx).cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "\n",
    "            bs = source.size(0)  # Batch size\n",
    "\n",
    "            for i in range(bs):\n",
    "                target_text = \"\".join(\n",
    "                    [idx_to_char[_] for _ in target[i, :] if _ != end_token_idx]\n",
    "                )\n",
    "                ground_truth_list.append(target_text)\n",
    "\n",
    "                prediction = \"\"\n",
    "                for idx in preds[i, :]:\n",
    "                    if idx == end_token_idx:\n",
    "                        break\n",
    "                    prediction += idx_to_char[idx]\n",
    "                preds_list.append(prediction)\n",
    "\n",
    "            if (\n",
    "                batch_idx >= num_samples - 1\n",
    "            ):  # Only sample a few batches for demonstration\n",
    "                break\n",
    "\n",
    "    # Print predictions\n",
    "    for i in range(min(num_samples, len(preds_list))):\n",
    "        print(f\"Ground Truth: {ground_truth_list[i]}\")\n",
    "        print(f\"Prediction: {preds_list[i]}\")\n",
    "        print(\"\\n~~~\\n\")\n",
    "\n",
    "\n",
    "# Call the function with your model, validation dataloader, and other necessary parameters\n",
    "generate_predictions(transformer, val_dataloader, device, 58, 59, idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Note, Reference, Brainstorm\n",
    "\n",
    "https://www.youtube.com/watch?v=4Bdc55j80l8\n",
    "\n",
    "## Input Embedding Layer\n",
    "\n",
    "The phrase must be vectorized (our case is chars)\n",
    "We must add Positional Encoding to create Positional Input Embeddings\n",
    "\n",
    "## Encoder Layer\n",
    "\n",
    "Two sub-modules:\n",
    "\n",
    "### Attention\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "3 Distinct fully connected layers\n",
    "\n",
    "- Query, Key, Value\n",
    "- A dot product of the Query and Key matrices is computed to create a score matrix.\n",
    "- The score matrix a table that dictatates how much value each word or char should be given, compared to the other words or chars in the input sequence. Higher score = more important. = more focus.\n",
    "- The score matrix is normalized by dividing by the square root of the dimension of the key vectors.\n",
    "- The score matrix is divided by the square root of the dimension of the key vectors which gives the scaled scores.\n",
    "- The scaled scores are then passed through a softmax function to receive the attention weights.\n",
    "- Multiply attention weights by value matrix to get the output vector of the self-attention layer.\n",
    "- Linear layer to process.\n",
    "\n",
    "#### Multi-headed Attention\n",
    "\n",
    "The query, key, and value is split into N heads.\n",
    "\n",
    "- Each vector goes through the attention layer as normal\n",
    "- The output of all heads is concatenated into a single vector\n",
    "- Each head is given a different representation of the input sequence, allowing the model to simultaneously attend to information from different representation subspaces.\n",
    "- Each head in theory should learn to attend to different parts of the input sequence, and thus overall learn more of the input sequence.\n",
    "\n",
    "### Residual Connection, Layer Normalization, and Feed Forward Layer\n",
    "\n",
    "- The Multi-headed attention output vector is added to the original input vector to the sub-layer, which is called a residual connection.\n",
    "- This output is then normalized by layer normalization.\n",
    "- This enters a feed forward network, which is a simple 2 layer fully connected network with a ReLU activation in between.\n",
    "- The output of that is added again to the original input, to be normalized again. Like before with the multi-headed attention.\n",
    "\n",
    "## Decoder Layer\n",
    "\n",
    "### Output Embedding Layer and Positional Encoding\n",
    "\n",
    "- The output goes through an embedding layer to get the position embeddings.\n",
    "- This enters the first multi-headed attention layer.\n",
    "- The scaled scores are added to a look ahead mask, which prevents the decoder from attending to future tokens.\n",
    "- This happens when the softmax makes future tokens 0, so no attention is given to them.\n",
    "- All the heads are combined to create a masked output layer.\n",
    "\n",
    "The second multi-headed attention layer has the query and key of the encoder output, and the value of the previous multi-headed attention layer output value.\n",
    "\n",
    "A final feed forward network is applied to the output of the second multi-headed attention layer.\n",
    "\n",
    "### Classifier, softmax, and output\n",
    "\n",
    "The feed forward output enters a linear classifier.\n",
    "\n",
    "Classifier output enters softmax to get the probability score of each class (char). The index of the highest probability is the predicted char.\n",
    "\n",
    "Output is added to a list of decoded outputs until the end token is reached.\n",
    "\n",
    "This whole structure can be stacked N layers high. The output of the final encoder, is input into all the decoder layers, who are taking the input from the previous decoder layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "```python\n",
    "import tensorflow_addons as tfa\n",
    "pbar = tfa.callbacks.TQDMProgressBar()\n",
    "model.fit(…,callbacks=[pbar])\n",
    "# TQDMProgressBar() also works with evaluate()\n",
    "model.evaluate(…,callbacks=[pb\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Multiprocessing\n",
    "\n",
    "```python\n",
    "with Pool(workers) as pool:\n",
    "    results = list(tqdm(pool.imap(worker,thread_list, total=len(thread_list))\n",
    "                        ar])\n",
    "```\n",
    "\n",
    "Check!\n",
    "\n",
    "## Padding strategies\n",
    "\n",
    "1. No Padding with <EOS> token: This is the most efficient and elegant approach for Transformers.\n",
    "\n",
    "2. Full Padding: Pad all phrases (and potentially feature sequences) to a fixed maximum length using a constant value or specific technique. This can be simpler to implement but introduces unnecessary computational overhead and potential information distortion due to large padding sections.\n",
    "\n",
    "3. Full Padding with Masking: This combines the simplicity of full padding with the benefits of masking. While you pad all sequences to a fixed length, you apply masking during training to prevent the model from attending to the padded regions. This can be a good compromise if your model struggles with highly variable sequence lengths but you still want to avoid the downsides of excessive padding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
